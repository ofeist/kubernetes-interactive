course name:certified kubernetes admin course
tutor: mumshad mannambeth
location: Documents/2_EDU/kubernetes

would be good to do before:
kubernetes for absolute beginners


few links
---
- https://www.cncf.io/certification/cka   # certification
- https://killer.sh/    # the best simulator
  - https://killer.sh/course/preview/e84d0e31-4fff-4c42-8afd-be1bdbc0d994
  - alias k=kubectl
  - export do="-o yaml --dry-run=client"
- the guys experience: https://georgearisty.dev/posts/ckad/
  - https://georgearisty.dev/posts/k8s-cluster-network/
---



2. cluster architecture
---
cargo and control ships analogy


master nodelogin
--
- manage
- plan
- scheduler
- monitor nodes

worker nodes
--
host applications as containers


master node consists of
--
- etcd :: key-value format db
- kube-apiserver :: orchestrator
- kube-scheduler :: how to deploy containers
- kube controller manager
  - node controller
  - replication controller


worker nodes consist of
--
- kubelet :: "the capetan"
- kube-proxy :: communication between services & containers inside of the cluster

container runtime service ( e.g. docker or rkt (rocket) )



3. & 4. etcd
---
- port: 2379
- stores / has the data about the cluster
(nodes, pods, configs, secrets, accounts, roles, bindings, others)


2 types of k8s cluster
--
- installed from scratch
  - you have to install etcd-server by yourself
- kubeadm tool
  - deploys etcd-server as a pod in kube-system namespace
$ kubectl get pods -n kube-system


$ kubectl exec etcs-master -n kube-system
$ etcdctl get / --prefix -keys-only

- data is in the root directory
/registry/
- an then constructs such as
	- minions, pods, replicasets, deployments, roles, secrets


- in HA environment
--
- there will be multiple master nodes in the cluster
- there will be multiple etcd instances spread across the master nodes
- etcd instancs shall know about each other
  - "--initial-cluster" option shall be specified



5. kube-apiserver
---
- kubectl <-reaches-the-> kube-apiserver (authorization/validation) <-retreaves-data-from-> etcd cluster
- kube-apiserver then authentizates and validates request


workflow example: create a pod (not using kubectl, but per APIs)
---
1] apiserver then creates a pod object without assigning it to a node
# curl -X POST /api/v1/namespaces/default/pods ...[other]
- the request is authenticated
- the request is validated
- the data is retrieved
- etcd is updated
- the user gets back the data


2] scheduler
- cont. monitors apiserver and realizes there is a new pod without a new node assigned
- updates kube-apiserver where to put the pod

3] kubeapi-server
- updates etcd-cluster
- passes the info to kubelet on the appropriate worker node

4] kubelet
- creates the pod on the worker node
- instructs the CRE (container runtime engine) to deploy app image
- updates status to kube-apiserver

5] kube-apiserver
updates data back into etcd cluster


kube-apiserver responsibilities are to:
--
1] authentcate user
2] validate request
3] retrieve data
4] update etcd (only component that talks to etcd is kube-apiserver)
5] scheduler
6] kubelet


kube-apiserver options
--
- eg. internesting are
  - certificates :: connectivity between different components (eg. ca.pem, kubernetes.pem, ..)
    - --etcd-catfile=..; --etcd-certfile=..; --etcd-keyfile=..;
  - location of etcd-servers
    - etcd-servers=..


- viewing kube-apiserver options
--
- depends on how you set up your cluster
  - if kubeadmin tool
    - then kube-apiserver-master deployed as a pod in -namespace kube-system @ master
    - the options could be seen in manifest file
      - /etc/kubernetes/manifests/kube-apiserver.yaml
  - if no-kubeadmin setup
    - /etc/systemd/system/kube-apiserver.service
  - or list the process
    - on the master node
      - ps -aux | grep kube-apiserver



6. kube-controller-manager
---
- the role of controller process is to:
  - watch the status
  - remediate (correct, repair) situation

- controllers examples:
  - node-controller
    - node monitor period = 5 s
      - takes the state every 5 seconds)
    - node monitor grace period - 40 s
      - after heartbeat from node notreached, node marked as unreachable
    - pod eviction timeout = 5 min
      - if node deosn't come up, it is removed, another one is provisioned
  - replication-controller
    - monitors the state of replicaSets
    - ensuring the desired nr of pods available all the times within the set
  - deployment-controller
  - namespace-controller
  - endpoint-controller
  - job-controller
  - pv-protection-controller
  - pv-binder-controller
  - replication-controller
  - cronjob
  - stateful-set
  - replicaset


- installing kube-controll-manager
--
$ wget https://storage.googleapis.com/kubernetes-release/v1.13.0/bin/linux/amd64/kube-controller-manager
# run it as a service


- options are @ kube-controller-manager.service
  - abovementioned options are here
    - --node-monitor-period=5s
    - --node-monitor-grace-period=40s
    - --pod-eviction-timeout=5m0s
  - by default all controllers are enabled
  - if some of them are not, here is a good starting point to look at
    - -- controllers stringSlice

- view kube-controller-manager server options
--
1] kubeadmin setup:
- kubeadmin deploys kube controller manager
$ kubectl get pods -n kube-system
$ cat /etc/kubernetes/manifest/kube-controller-manager.yaml

2] no-kubeadmin setup
$ cat /etc/systemd/system/kube-controller-manager.service
- or
$ ps aux | grep kube-controller-manager



7. kube-scheduler
---
- (only) decides which pod goes on which node
- kubelet :: actually places the pod on the node

- how it's done
--
1] filter nodes (e.g. that do not have enough CPUs)
2] rank nodes (ammount of resources after the pod is placed)
3] more requirements come later ..


- installing kube-scheduler
--
wget http://storage.googleapics.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-scheduler
# extract it and run it as a service


- viewing kube-scheduler options
--
$ cat /etc/kubernetes/manifests/kube-scheduler.yaml	# pod definition file
# kubeadm :: pod in kube-system namespace on the master node

$ ps -aux | grep kube-scheduler



8. kubelet
---
- is like captain of the worker node
1] registers the node with the k8s cluster
2] creates the pod (requests the CRE to pull the image and run an instance)
3] monitors node & pods


- installing kubelet
--
# kubeadm does not deploy kubelets
# kubelet has to be always manually installed on worker nodes
wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kubelet

- viewing kubelet options
--
# listing the process on the worker node
ps -aux | grep kubelet



9. kube-proxy
---
- POD Network exists across all nodes

- kube-proxy
  - process that runs on each node
  - looks for new services
    - when service created
      - using iptables
      - creates rules on each node
      - to forward traffic from services to the backend pods


- installing kube-proxy
--
# download, extract it and run it as a service
$ wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-proxy

# kubeadm deploys kube-proxy as pod on each node
# it is deployd as a deamonset
$ kubectl get demonseat -n kube-system



10. pods
---
- the smallest k8s'object
- a single instance of an application
- pod encapsulates the deployed container

- scaling application happens via single container pods

- multi-container pods
--
- having hepler containers (supporting task for e.g. web app)
- all of them share
  - same network space (@ localhost)
  - same storage space


- installing (deploying) pods
--
$ kubectl run nginx --image nginx   # create pod
# the command deploys a docker container by creating a pod

$ kubectl get pods
# list of pods in our cluster




11. & 12. pods with yaml
---
- 4 toplevel properties:
  - apiVersion, kind, metadata, spec

- pod-definition.yaml
--
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx

$ kubectl create -f pod-definition.yaml
$ kubectl get pods
$ kubectl describe pod myapp-pod
$ kubectl delete deployment myapp-pod


13. & 14. & 15. practice test introduction, accessing labs and solution
---
- kodekloud

$ sudo apt install yamllint -y
$ kubectl run redis --image=redis --dry-run=client -o yaml > redis-pod.yaml
$ kubectl apply -f redis-pod.yaml
# now fix the pod definition (the upper definition is correct, nothing to fix there ...)
$ kubectl edit pod redis



16. & 17. recap - replicaSets
---
- reasons for replication:
  - HA
  - load balancing & scaling

- replication controller spans accross multiple nodes in the cluster
- notice the difference:
  - Replication Controller vs. Replica Set
    - they have same purpose, but they are not the same
    - Replication Controller is the older tehnology
    - Replica Set is the recommended way to set up the replication
    - ReplicaSet has to have "selector:" field

1] replication controller
--
rc-definition.yaml
---
apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
    type: frontend
spec:
  template:
    metadata:
      name: nginx
      labels:
        app: myapp
    spec:
      containers:
        - name: nginx-container
          image: nginx
  
  replicas: 3

$ kubectl create -f rs-definitionl.yaml
$ kubectl get replicationcontrollers
$ kubectl get pods

2] replicaset
rs-definition.yaml
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    app: myapp
    type: front-end

spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
  
  replicas: 3
  selector:
    matchLabels:
      type: front-end

$ kubectl get replicasets
$ kubectl get pods

- labels and selectors
  - in generall used to connect dots in kubernetes

- how to scale
1]
# set replicas: 6 in yaml file
$ kubectl replace -f rs-definition.yaml

2]
# either
$ kubectl scale --replicas=6 -f rs-definition.yaml
# this doesn't change the yaml definition file
# or
$ kubectl scale --replicas replicaset myapp-replicaset

- to racap:
$ kubectl get replicaset
$ kubectl get pods
$ kubectl delete replicaset myapp-replicaset    # deletes all underlying pods
$ kubectl replace -f replicaset-definition.yaml
$ kubectl scale --replicas=6 -f rs-definition.yaml
$ kubectl scale replicaset --replicas=5 myapp-replicaset

- replicaset ensures that the desired number of pods always run


18. & 19. deployments
---
- purposes:
  - many instances of server (tech stack)
  - upgrades
  - rolling updates
  - rollback
  - pause & resume changes

- pod < replicaSet < deployment

- deployment-definition.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx
  replicas: 3
  selector:
    matchLabels:
      type: front-end


$ kubectl create -f deployment-definition.yaml
$ kubectl get replicaset
$ kubectl get pods
$ kubectl get all

- to recap the file structure:
--
(aki mesec)
apiVersion:
kind:
metadata:
spec:

--
apiVersion:
kind:
(metanola)
metadata:
  name:
  labels:
(sxpek trese)
spec:
  (temple mesec)
  template:
    metadata:
    spec:
  replicas:
  selector:


- to create deployment imperative way:
$ kubectl create deployment httpd-frontend --replicas=3 --image=httpd:2.4-alpine
$ kubectl create deployment httpd-frontend --replicas=3 --image=httpd:2.4-alpine --dry-run=client -o yaml
# --dry-run=client -o yaml



20. namespaces
---
- isolation

- existing:
  - default
  - kube-system :: services, k8s resources ...
  - kube-public :: 

- to a namespace the following could be assigned:
  - policy
  - resources

- DNS is created
  - e.g. to reach the service:
    -inside of the namespace
      - db-service
    - in another namespace
      - db-service.dev.svc.cluster.local
  - cluster.local = domain
  - svc = subdomain (for a service)
  - dev = namespace
  - db-service = service name
        
$ kubectl get pods --namespace=kube-system

$ kubectl create -f pod-definition.yaml
$ kubectl create -f pod-definition.yaml --namespace=dev

- defined under "metadata" section

- pod-definition.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  namespace: dev
  labels:
    app: myapp
    type: front-end
spec:
  containers:
  - name: nginx-controller
    image: nginx

- how to create a new namespace:

1]
- namespace-dev.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: dev

$ kubectl create -f namespace-dev.yaml

2]
$ kubectl create namespace dev

- switch the working namespace
$ kubectl config set-context $(kubectl config current-context) --namespace=dev

$ kubectl get pods --all-namespaces   # across all namespaces


- to limit resources in a namespace create a resource quota:
compute-quota.yaml
---
apiVersion: v1
kind: ReqourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi

$ kubectl create -f compute-quota.yaml

$ kubectl get namespaces --no-headers | wc -l

# reach db-service in the marketing namespace
# db-service.marketing.svc.cluster.local



22. services & NodePort
---
- k8s object (like replicaSet, deployment, ..)
- it listens to a port on the node and to forward request on that port to a port on the node running the web application
- this is NodePort service

- ServiceTypes
--
- NodePort
  - forwards request node out <> in
- ClusterIP
  - creates virtual ip
- LoadBalancer
  - balances requests

- NodePort service
--
1] targetPort
  - the pod port
  - it has the IP address
2] port
  - the service port
  - it has the IP address (the ClusterIP of the service)
3] nodePort
  - range: 30000-327676

- service-definition.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
  - nodePort: 30008
    port: 80
    targetPort: 80
  selector:
    app: myapp
    type: front-end

- pod-definition.yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: myapp
    type: front-end
  name: myapp-pod
spec:
  containers:
  - image: nginx
    name: nginx-container

$ kubectl create -f service-definition.yaml
$ kubectl create -f pod-definition.yaml

- the service acts as a load balancer if more pods are targeted
  - random algorith
  - with seesionaffinity
- if pods are distributed accross multiple nodes
  - service than spans accross all nodes in the cluster
  - target node is same on all nodes in the cluster
    - curl 192.168.1.2:30008
    - curl 192.168.1.3:30008
    - curl 192.168.1.4:30008


23. service clusterIP
---
- the clusterIP reminds me on the "facade" programming pattern
- each service get the ip which is then used to access the services "accumulated" behind this ip
- this is the default service type!

service-definition.yaml
---
apiVersion: v1
kind: Service

metadata:
  name: back-end

spec:
  type: ClusterIP
  ports:
  - targetPort: 80
    port: 80

  selector:
    app: myapp
    type: back-end



24. service LoadBalancer
---
- this works only with supported platforms (AWS, googleCloud, azure, ...)

- service-definition.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: LoadBalancer
  ports:
  - targetPort: 80
    port: 80
    nodePort: 30008

- if the platform doesn't support the LoadBalancer type, the effect is same as setting the type to NodePort



25. solution - services 
---
$ kubectl get svc                   # get the number, type of services
$ kubectl describe svc kubernetes   # the target port, Labels, Endpoints for kubernetes service -> targetPort
$ kubectl get deployments           # how many deployments
$ kubectl describe deployment nginx | grep -i image         # the image for deployment
$ kubectl create deployment web-app --dry-run=client -o yaml --image=nginx > webapp-deployment.yaml   # creates the deployment
$ kubectl expose deployment webapp --name=webapp-service --target-port=8080 --type=NodePort --port=8080   # create a service out of the deployment
$ kubectl expose deployment webapp --name=webapp-service --target-port=8080 --type=NodePort --port=8080 --dry-run=client -o yaml > webapp-service.yaml  # create a service out of the deployment
  # edit the webapp-service.yaml and add "nodePort: 30080"



26. imperative vs declarative
---
- imperative
  - create and update objects
  - kubectl run/create/expose/edit/scale/set image/create -f/replace -f/ delete -f object
  - use imperative approach for the exam, to save the time
- declarative
  - create a set of files
  - "kubectl apply -f " command
    - apply command takes a look at the existing configuration and figures out what needs to be changed
  - if the exam requires more complicated objects/more containers, this approach is better


- imperative approach
--
- create objects per object configuration files
  $ kubectl create -f nginx.yaml
- update objects:
  $ kubectl edit deployment nginx.yaml
    - this changes will be applied to the live object only, and not to the definition file
  $ kubectl replace -f nginx.yaml
    - better approach is to change the definition file and to track the changes
  $ kubectl replace -f --force nginx.yaml
    - if you want to delete and replace objects

- declarative approach
--
- use the object configuration files to create and update objects
  $ kubectl apply -f nginx.yaml
  $ kubectl apply -f /path/to/config/files

- check kubernetes documentation
  - "manage kubernetes objects"   # documentation



27. imperative vs declarative - solutions
---
$ kubectl run nginx-pod --image=nginx:alpine

$ kubectl run redis --image=redis:alpine --labels="tier=db"
$ kubectl expose pod redis --port=6379 --target-port 6379 --cluster-ip='' --selector=tier=db --name=redis-svc
  # or
  $ kubectl expose pod redis --port=6379 --selector=tier=db --name=redis-svc
    # --cluster-ip= is the default service
$ kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3
  # or
  $ kubectl create deployment webapp --image=kodekloud/webapp-color
  $ kubectl scale deployment webapp --replicas=3

$ kubectl run custom-nginx --image=nginx --port=8080

$ kubectl create ns dev-ns
$ kubectl create deployment redis-deploy --namespace=dev-ns --image=redis --replicas=2

$ kubectl run httpd --image=httpd:alpine
$ kubectl expose pod httpd --name=httpd --cluster-ip='' --port=80 --target-port=80
  # or
  $ kubectl run httpd --image=httpd:alpine --port 80 expose (--dry-run=client -o yaml)



2.28. kubectl apply command
---
- kubectl compares
  - local configuration file
  - last applied configuration
  - a live configuration of the object definition on k8s
  
- kubectl apply -f file
  - yaml -> json
  - json location: k8s cluster in live object configuration
    - annotations:
      - kubectl.kubernetes.io/last-applied-configuration
  - kubectl create/replace commands do not store this configuration!
    - that's why do not mix imperative and declerative ways when managing k8s cluster

- last applied configuration
  - contains fields that were removed from 
    - local file 
    - and from the live k8s configuration

- check k8s documentation
  - "merging changes to primitive fields"   # documentation



3. scheduling
---

3.1. introduction
---

3.2. manual scheduling
---
- nodeName propery in the pod-definition.yaml
  - if it is not there, then this is is candidate for scheduling

- nodeName can be set per binding definition

pod-bind-definition.yaml
--
apiVersion: v1
kind: Binding
metadata:
  name:nginx
target:
  apiVersion: v1
  kind: Node
  name: node02

- then the data in json format is sent to the api
curl --header "Content-Type:application/json" \
  --request POST \
  --data '{"apiVersion":"v1", "kind": "Binding" ...}' \
  http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/



3.3. solution - manual scheduling
--
$ kubectl apply -f nginx.yaml
$ kubectl -n kube-system get pods

- how to disable scheduler: https://github.com/kubernetes/website/issues/21128
- find the part "We originally followed this tutorial: "

- after the scheduler was disabled, the nodeName has to be specified
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx
  name: nginx
spec:
  nodeName: node01
  containers:
  - image: nginx
    name: nginx

- by specifying the "nodeName", the need for scheduler has been circumvented


3.4. labels and selectors
---
- grouping
$ kubectl get pods --selector app=App1  # get pods with labels

- when creating objects
- selector/matchLabels must match template/metadata label

- annotations:
--
- e.g. tool details like name, version ..



3.5. labels and selectors - solution
---
$ kubectl get pods --show-labels
$ k get pods -l env=dev --no-headers
$ k get all --show-labels --no-headers -l env=dev
$ k get pods -l env=prod,bu=finance,tier=frontend



3.6. taints and tolerations
---
- node <-> taints
- pod <->  tolerations

- taints
--
$ kubectl taint nodes node-name color=blue:taint-effect
- taint-effect
  - noSchedule
  - PreferNoSchedule
  - NoExecute
$ kubectl taint nodes node01 app=blue:noSchedule


- tolerations
--
pod-definition.yaml
---
apiVersion: v1
kind: Pod
  name:myapp-pod
spec:
  containers:
  - name: nginx-container
    image: nginx

  toleartions:
  - key:"app"
    operator:"Equal"
    value:"blue"
    effect:"NoSchedule"

$ kubectl describe node controlplane | grep Taint



3.7. solution - taints and tolerations
---
$ kubectl explain pod --recursive | less
$ kubectl explain pod --recursive | grep -A5 tolerations

$ kubectl describe nodes node01 | grep -i taint
$ kubectl taint node master app=blue:NoSchedule-  # the minus removes the taint


controlplane $ k taint node node01 app=blue:NoSchedule
controlplane $ k apply -f nginx.yaml 
controlplane $ k get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP            NODE           NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          8s    192.168.0.8   controlplane   <none>           <none>
controlplane $ k delete pod nginx 
controlplane $ k taint node node01 app=blue:NoSchedule-
controlplane $ k apply -f nginx.yaml
controlplane $ k get pods -o wide
NAME    READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
nginx   0/1     ContainerCreating   0          7s    <none>   node01   <none>           <none>

- now set the toleration in the nginx.yaml definition
nginx.yaml 
---
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx

  tolerations: 
  - effect: "NoSchedule"
    key: "app"
    operator: "Equal"
    value: "blue"
  restartPolicy: Always

$ k delete pod nginx
$ k apply -f nginx.yaml

controlplane $ k get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          3s    192.168.1.4   node01   <none>           <none>

# runs again on the node01

$ taint node node01 app=blue:NoSchedule



3.8. node selectors
---
- node selectors based on the labels assigned to the node

$ kubectl label node node-name key=value  # to label the node
$ kubectl label nodes node01 size=Large

- selectors are exclusive (u can not say e.g. nodeSelector: "Large OR Medium" )
  - for that the nodeAffinity was introduced



3.9. node affinity & solutions
---
- required OR ignored
  - during scheduling
  - during execution
  - eg:
    - requiredDuringSchedulingIgnoredDuringExecution

$ kubectl get nodes node01 --show-labels
$ kubectl label nodes node01 color=blue
$ kubectl create deployment blue --image=nginx --replicas=6
$ kubectl get pods -o wide

- check kubernetes documentation
  - "Assign Pods to Nodes using Node Affinity"   # documentation
    - "Schedule a Pod using required node affinity"   # documentation
      https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/   # link



3.11. node affinity vs taints and tolerations
---
- how to place a combination of 
  - red, green, blue and other pods
  - onto the red, green, blue and other nodes

1] taint RGB nodes
2] tolerate RGB pods
3] affinity of other pods for other nodes



3.12. resource requirements and limits
---
- configure pods and containers   # documentation
  - kubernetes.io
  - https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/



3.13. resource requirements and limits - solutions
---
$ kubectl create -f ns-mem-example.yaml
$ kubectl apply -f pod-mem-example.yaml
$ kubectl get pod memory-demo -n mem-example
$ kubectl get pod memory-demo --output=yaml --namespace=mem-example
$ kubectl top pod memory-demo --namespace=mem-example
$ kubectl top pod memory-demo --namespace=mem-example
  # metrics API not available!!!
$ kubectl delete pod mem-example -n mem-example
$ kubectl config set-context $(kubectl config current-context) --namespace=mem-example   # set (move to) default namespace
$ kubectl apply -f pod-mem-example-2.yaml



3.14. deamon sets
---
- created by kube-api server
- ignored by kube-scheduler
- https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/  # documentation
- deamonSets run one copy of a pod on each node in the cluster
- use cases:
  - install agents
    - monitoring solutions
    - logs viewer
  - kube-proxy
  - weave-net (networking agent)

- similar to ReplicaSet, but with the kind of "DaemonSet"
- to create it programatically
  1] kubectl create deploy nginx --image=nginx --dry-run -o yaml > nginx-ds.yaml
  2] edit the nginx-ds.yaml
    - delete "replicas: 1" line
    - set "kind: DaemonSet" instead of "Deployment" kind

- how does this work?
  - set the nodeName property on each node and bypass the scheduler
    - this approach was used untill k8s v1.12
  - use NodeAffinity and defaultscheduler
    - since v1.12


3.15. daemonset - solutions
---
$ kubect create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 --dry-run=client -oyaml > ds-es.yaml
  # delete replicas, strategy
  # add the namespace
  # change kind Deployment -> DaemonSet



3.16. static pods
---
- created by kubelet
- ignored by kube-scheduler
- kubelet creates a static pod if a pod-definition yaml is placed under
  - /etc/kubernetes/manifests

- this way you cannot create replicasets, daemonsets, ...
- kubelet works on the pod level, so this is possible

- the pod definition path is defined either
  1] as the option to the kubelet.service
  - --pod-manifest-path=/etc/Kubernetes/manifests \\
  - or
  2] as the option of the config file to the kubelet.service
  - --config=kubeconfig.yaml  \\
    - kubeconfig.yaml
    ---
    staticPodPath: /etc/kubernetes/manifests
  - clusters setup using kubeadmin tool use this approach

- to check static pods use the command
  $ docker ps

- kube-apiserver is aware of both
  - static pods and 
  - pods created per kube-apiserver

- kube-apiserver
    - the command "kubectl get pods" returns static pods too
    - but this is only the read property
    - any change on static pods is done per static pod definition yaml file

- the k8s cluster could be set by placing k8s komponent definition files 
  to the manifests folders on the nodes
  - this is how kubeadmin tool does the setup of the k8s cluster
  - this is why you see pods when running
    $ kubectl get pods -n kube-system



3.17. static pods - solution
---
- where are the static pod definition files?
  - $ ps -ef | grep kubelet
  - $ ps aux | grep kubelet   # this worked for me
  - $ systemctl cat kubelet   # can this be used for checking the static pod manifests location?
  - check the "kubelet" setup for the 1.28! # todo
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/  # documentation
  - https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/    # documentation

$ kubectl run static-busybox --image=busybox --dry-run=client -oyaml --command -- sleep 1000 > static-busybox.yaml



3.18. multiple schedulers
---
- how to deploy a kube-scheduler -> installing kube-scheduler

1.a] wget
$ wget http://storage.googleapics.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-scheduler
# extract it and run it as a service
- kube-scheduler.service
  - --scheduler-name=default-scheduler  # if not specified, this is default name option
  - when deploying additional scheduler, set the --scheduler-name e.g. to
    - --scheduler-name=my-custom-scheduler

1.b] kubeadm tool
- make a copy (& rename) the file
  - /etc/kubernetes/manifests/kube-scheduler.yaml
- set the custom name as part of the command
  - --scheduler-name=my-custom-scheduler

2] create pod-definition.yaml
- the next step is to set the scheduler in pod definition
  - schedulerName: my-custom-scheduler
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx

  schedulerName: my-custom-scheduler

- how to know which scheduler picked it up?
$ kubectl get events  # check which scheduler picked up the pod

- how to view the logs
$ kubectl logs my-custom-scheduler --name-space=kube-system   # check logs of the custom scheduler



3.19. multiple schedulers - solution
---
- https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/   # documentation
$ kubectl create -f my-kube-scheduler.yaml
- Failed to pull image "gcr.io/my-gcp-project/my-kube-scheduler:1.0": failed to pull and unpack image "gcr.io/my-gcp-project/my-kube-scheduler:1.0": failed to resolve reference "gcr.io/my-gcp-project/my-kube-scheduler:1.0": failed to authorize: failed to fetch anonymous token: unexpected status: 400 Bad Request

$ watch "kubectl get pods"



3.20. configuring scheduler
---
- manually (wget) vs. kubeadm tool (kubectl)
- ~0.45 shows advanced scheduling options
- https://github.com/kubernetes/community/tree/master/contributors/devel
            - the lower one didnt exist anymore
                - https:github.com/kubernetes/community/blob/master/contributors/devel/scheduler.md
- https://kubernetes.io.blog/2017/03/advanced-scheduling-in-kubernetes/
- https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/
- https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work



4.2. logging and monitoring
---
- heapster is deprecated

- metrics server (heapster's slimmed down version)
  - in-memory performance data
    - no historical performance data

- kubelet (agent) generates the metrics
  - cAdvisor component (containers advisor)
    - retrieve performance metrics from pods
    - exposes them through the kubelet api
      - makes it available to the metrics server

- to deploy metrics-server
---
- minikube environment:
$ minikube addons enable metrics-server   # if you use minikube

- all other environments:
$ git clone https://github.com/kubernetes-incubator/metrics-server.git
$ kubectl create -f deploy/1.8+/

- probably up-to-date info:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml   # install metrics server

- to check performance (cpu / memory)
$ kubectl top node
$ kubectl top pod



4.4. managing application logs
---
$ docker run kodekloud/event-simulator    # event-simulator
$ docker logs -f ecf  # ???

$ k run event-simulator-pod --image=kodekloud/event-simulator --dry-run=client -oyaml > event-simulator.yaml
$ k create -f event-simulator.yaml
$ k logs -f event-simulator-pod

- if more containers inside of pod, then the container from 
  which i'd like to read the logs must be specified

- e.g. i have event-simulator + nginx inside of the pod
  $ kubectl logs -f event-simulator-pod event-simulator   # read the logs from event-simulator



4.5. logging - solution
---
- if the pod has more containers, -c option shows containers
  $ kubectl logs webapp-2 -c simple-webapp  # gives the logs of simple-webapp container in the pod



5.1. application lifecycle management
---
- mentioned already in KCD (kubernetes certified developer course)



5.2. rolling updates and rollbacks
---
- creating a deployment triggers a rolout = deployment revision (function of app/container version)
- a next, new rolout creates new deployment revision (app version)

- deployment status
$ kubectl rollout status deployment/myapp-deployment    # deployment status

- revision and rollout history 
$ kubectl rollout history deployment/myapp-deployment   # revision and rollout history

- deployment strategies
  - recreate strategy
    - destroy & create instances(versions)
    - application downtime
    - not a default strategy
  - rolling update
    - replace instance(version) by instance
    - no downtime
    - default deployment strategy
strategy:    

- how to update deployment
  - k apply command
    $ kubectl apply -f deployment-definition.yaml
      - creates new rollout + new revision
  - k set image
    $ kubectl set image deployment/myapp-depl nginx=nginx:1.9.1
      - whis will result in different config of definition file and deployment

$ kubectl describe
  - gives deployment detailed infomation

- upgrades could be followed watching replicasets
  $ kubectl get replicasets

- rollback of deployment
  $ kubectl rollout undo deployment/myapp-deployment  # rollback deployment
  $ kubectl get replicasets   # before and after deployment

- to summarize deployment commands
$ kubectl create -f deployment-definition.yaml  # create deployment
$ kubectl get deployments
$ kubectl apply -f deployment-definition.yaml   # update deployment
$ kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1
$ kubectl rollout status deployment/myapp-deployment
$ kubectl rollout history deployment/myapp-deployment
$ kubectl rollout undo deployment/myapp

- edit the deployment
$ kubectl edit deployement web-app
  - strategy: recreate  # for example

- FYI - not default option is:
  strategy:    
    type: Recreate



5.4. application commands
---
- not part of exam

- docker image like
---
FROM Ubuntu

ENTRYPOINT ["sleep"]

CMD ["5"]

$ docker run ubuntu-sleeper 10
$ docker run --entrypoint sleep2.0 ubuntu-sleeper 10
  - sleep2.0 is the "new" (another) sleep application 



5.5. application commands and arguments
---
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      command: ["sleep2.0"]
      args: ["10"]

- similarity to docker
  - command ["sleep"] <> ENTRYPOINT ["sleep"]
  - args ["10"] <> CMD ["10"]



5.6. commands and arguments - solution
---
- interesting was flask
  - is this a web server/web app?

1] 
  
Dockerfile (kodekloud/webapp-color)
---
FROM python3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python","app.py"]

CMD ["--color", "red"]


webapp-color-pod.yaml
---
image: kodekloud/webapp-color
command: ["--color","green"]


2] web-app.yaml (from kodekloud)
video @ ~9.00
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: webapp-green
  name: webapp-green
spec:
  containers:
    args: [ "--color=green" ]
    image: kodekloud/webapp-color
    name: webapp-green
    resources: {}
  restartPolicy: Never

gives:
Error from server (BadRequest): error when creating "webapp-green.yaml": Pod in version "v1" cannot be handled as a Pod: json: cannot unmarshal object into Go struct field PodSpec.spec.containers of type []v1.Container



5.7. configure environment variables in applications
---
- use the "env:" property
- env is an array
  -> every property in the array starts with the dash "-"

- key value pair
env:
  - name:
    value:

- ConfigMap (use "valueFrom")
env:
  - name: APP_COLOR
    valueFrom:
      configMapKeyRef:

- Secrets (use "valueFrom")
env:
  - name: APP_COLOR
    valueFrom:
      secretKeyRef:



5.8. configure config map
---
- imperative
  $ kubectl create configmap 

- declerative
  $ kubectl create -f

- imperative
$ kubectl create configmap \
  app-config --from-literal=APP_COLOR=blue  \
              -- from literal=APP_MOD=prod

- from a file
kubectl create configmap \
  app-config  --from-file=app_config.properties


- declarative:
config-map.yaml
---
apiVersion: v1
data:
  APP_COLOR: blue
  APP_MODE: prod
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: app-config

$ kubectl create -f config-map.yaml

- view config map
  $ kubectl get configmaps
  $ kubectl describe configmaps config-map

- now you have to configure this in the pod
spec:
  envFrom:
  - configMapRef:
    name: app-config


- in general, 3 ways of injecting configmap
  1] env
    envFrom:
      - configMapRef:
        name: app-config

  2] single env
    env:
      - name: APP_COLOR
        valueFrom:
          configMapKeyRef:
            name: app-config
            key: APP_COLOR

  3] VOLUME
    volumes:
    - name: app-config-volume
      configMap:
        name: app-config



5.9. environment variables - solution
---
- overall, this part i need to repeat more thoroughly
- https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/

- configure microservices using configMap and secrets
  - https://kubernetes.io/docs/tutorials/configuration/configure-java-microservice/configure-java-microservice/

- katacoda shutter
  - https://kubernetes.io/blog/2023/02/14/kubernetes-katacoda-tutorials-stop-from-2023-03-31/



5.10. configure secrets in applications
---
- create a secret
  - kubectl create secret generic   # imperative
  - kubectl create -f               # declarative

- imperative
  - options
    --from-literal
    --from-file

- declerative
  - secret-data.yaml
  ---
  apiVersion: v1
  kind: Secret
  metadata:
    name: app-secret [A]
  data:
    DB_Host: mysql
    DB_User: root
    DB_Password: passwrd

- but, for the data values use
$ echo -n 'mysql' | base64
$ echo -n 'root' | base64
$ echo -n 'passwrd' | base64

$ kubectl create -f secret-data.yaml
$ kubectl get secrets
$ kubectl describe secrets
$ kubectl get secret app-secret -o yaml

- to decode the values
$ echo -n 'bXlzcWw=' | base64 --decode


- inject into pod

pod-definition
---
apiVersion: v1
kind: Pod
...
...
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
      - containerPort: 8080
    envFrom:
      - secretRef:
            name: app-secret [A]

$ kubectl create -f pod-definition.yaml


- more ways of injecting secrets into pods
1] from secrets (ENV)
2] SINGLE ENV
3] VOLUME

- volume
  -name: app-secret-volume
  - creates a file for each secret
  $ ls opt/app-secret-volumes



5.11. secrets - solutions
---
$ kubectl create secret generic db-secret \
--from-literal=DB_Host=sql01 \
--from-literal=DB_User=root \
--from-literal=DB_Password=password123 \
--dry-run=client -oyaml


db-secret.yaml
---
apiVersion: v1
data:
  DB_Host: c3FsMDE=
  DB_Password: cGFzc3dvcmQxMjM=
  DB_User: cm9vdA==
kind: Secret
metadata:
  creationTimestamp: null
  name: db-secret


$ kubectl explain pods --recursive | grep envFrom -A5
- input
  envFrom:
    - secretRef:
          name: db-secret


 webapp-pod.yaml
 ---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: webapp-pod
  name: webapp-pod
spec:
  containers:
  - image: kodekloud/simplewebapp-mysql
    name: webapp-pod
    envFrom:
    - secretRef:
        name: db-secret


5.12. multi container pods
---
- multicontainer pods share
  - lifecycle
  - network
  - storage

spec:
  containers:
  - name: simple-webapp

- the dash "-" marks that containers are an "array"

- You can use following command to delete the POD forcefully
  $ kubectl delete pod <PODNAME> --grace-period=0 --force



5.13. multi container pods - solutions
---
- sidecar container 
  -> sends logs to elasticsearch



5.14. init containers - solutions
---
- what are init containers?



6.01 cluster maintenance
---

6.02 os updates
---
- if pods part of replicaset
  - pods from a node down are deleted and recreated on another node 
    after the eviction-timeout
  - --pod-eviction-timeout=5m0s   # default

- in theory, some update could be done by taking the node down for < 5min
  - but, you can never tell if you're loose pods (not part of the replicaSet)

- how this shall be done: drain the node
  $ kubectl drain node-1      # "drain" moves pods to another nodes
    - drained node is unschedulable
  # do the maintenance (take down the node), and e.g. reboot
  $ kubectl uncordon node-1   # uncordoned node is now schedulable

- instead of drain you can mark the node unschedulable with "cordon"
  $ kubectl cordon node-1   # marks as unschedulable; doesn't drain



6.03 os updates - solution
---
- daemonSets can not be deleted (drained)
  - --ignore-daemonsets

- play around with taints, i suppose



6.04 kubernetes software versions (releases)
---
$ kubectl get nodes   # shows the version
- all releases available at the releases page of the k8s github
  - "kubernetes.tar.gz"
  - all the control plane components are in the package
    - all control plane components have same version
      - kube-apiserver
      - controller-manager
      - kube-scheduler
      - kubelet
      - kube-proxy
      - kubectl
    - there are other components within the control-plane with different version numbers
      - those are the separate projects:
        - etcd cluster
        - coreDNS

- todo
  - check swagger tutorial
  - check the versioning references
      https://github.com/kubernetes/community/tree/master/contributors/design-proposals
      https://github.com/kubernetes/design-proposals-archive/blob/main/release/versioning.md
      https://github.com/kubernetes/design-proposals-archive/blob/main/api-machinery/api-group.md
      https://blog.risingstack.com/the-history-of-kubernetes/
      https://kubernetes.io/releases/version-skew-policy/



6.05  cluster upgrade process
---
- core control-plane components do not have to be on the same version

- kube-apiserver 
  - is the primary component in the control-plane
  - version of all other components not allowed to be higher of the kube-apiserver version
    - e.g. 1.10

- controller-manager and kube-scheduler 
  - can be on one version lower
  - e.g. 1.9 or 1.10

- kubelet and kube-proxy 
  - can be on two versions below
  - e.g. 1.8, 1.9 or 1.10

- kubectl version 
  - can be higher of kube-apiserver
  - e.g. 1.9, 1.10 or 1.11

- the recommended approach is to upgrade one minor version at the time

- upgrade stratiegies
  - cloud (managed upgrades - few clicks)
  - kubeadm
    - kubectl upgrade plan
    - kubectl upgrade apply
  - the hard way (cluster deployed from scratch)
    - manual deployment

- the kubadm way
---
- steps 1.10 -> 1.11
  1] upgrade the master nodes
  2] upgrade the worker nodes

1] upgrade master
--
- the master node upgraded first
- components go down briefly
  - apiserver
  - scheduler
  - controller-managers

- when master down
  - worker nodes and applications on worker nodes (and running apps) are not affected
  - but all management functions are down, e.g.
    - access via kubectl or api
    - deploy new applications
    - modify existing apps
    - controller managers do not function
      - a failed pod will not be recreated

2] upgrade the worker nodesrecommended
1] master node upgrade
  $ kubeadm upgrade plan
  $ apt-get upgrade -y kubeadm=1.12.0-00
  $ kubeadm upgrade apply v1.12.0
  $ kubectl get nodes   # VERSION column shows kubelet ver
    - gets registered version of kubelets
    - kubeadm sets kubelets on the master node
    - if installed from scratch, kubelets are placed on nodes
      - the master node will not be shown as output of "k get nodes"
  $ apt-get upgrade -y kubelet=1.12.0-00
  $ systemctl restart kubelet
  $ kubectl get nodes   # VERSION
    - the version of master upgraded

2] worker nodes upgrade
  - node-1
    $ kubectl drain node-1
      - moves the workload from the node
      - cordons the node (and marks unschedulable)
    - then upgrade kubeadm and kubelet as done on the master
      $ apt-get upgrade -y kubeadm=1.12.0-00
      $ kubeadm upgrade apply v1.12.0
      $ apt-get upgrade -y kubelet=1.12.0-00
      $ kubeadm upgrade node config --kubelet-version v1.12.0   # different from the master???
      $ systemctl restart kubelet
      $ kubectl uncordon node-1
  - node-2
    $ kubectl drain node-2
      - as per node-1
  - node-N
    $ kubectl drain node-N
      - as per node-1



6.06 k8s upgrade - solution
---
- first i tried to do it by the book (after the theory lessons)
  - s wee little difference on killercoda
    $ kubeadm upgrade node
    $ apt upgrade kubelet
    $ systemctl restart kubelet
    $ kubectl get nodes


- check available versions
  $ apt-cache policy kubeadm
  $ apt-get install kubeadm=1.28.4

- check the cluster version
  $ kubectl get nodes
  $ kubectl version

- check the latest version available for the upgrade
  $ kubeadm upgrade plan


- THE UPGRADE
---
- upgrade master (controlplane)
--
- commands
  $ kubectl drain controlplane
    - status: Ready, SchedulingDisabled
  $ apt-cache policy kubeadm | grep 1.28.
  $ apt install kubeadm=1.28.2-00     # apt-get
  $ kubeadm version
  $ kubeadm upgrade apply v1.28.2     # this upgrades the cluster
    # [upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.28.2". Enjoy!
  $ kubectl version     # server=1.28.1
  $ kubectl get nodes   # server=1.28.1 
    - kubelet needs the manual upgrade
  $ apt install kubelet=1.28.2-00
  $ kubect get nodes
  $ kubectl uncordon controlplane
    - status: Ready


- upgrade worker (node01)
--
- commands
  $ kubectl drain node01
  $ ssh node01
    # apt-cache policy kubeadm | grep 1.28.2
  $ apt install kubeadm=1.28.2-00
  $ kubeadm upgrade node
  $ kubeadm install kubelet=1.28.2-00
  $ logout  # to master
    # kubectl get nodes
  $ kubectl uncordon node01


6.08 backup and restore methods
---
- resources
  - resource configuration
  - etcd cluster
  - persistent volumes

1] resource configuration
--
- declerative way
  - configs saved at source control

- get all resource configs from the cluster
  $ kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml

- for this (get all) you could use tools
  - VELERO  # former ARK by HeptIO


2] etcd
--
- state of the cluster
- nodes and other resources

- either:
  - backup the data directory by a backup tool
    - etcd.service
    ---
    ExecSTart=/usr/local/...
      --name ${ETCD_NAME} \\
      ..
      ..
      --data-dir=/var/lib/etcd

  - or, use the builtin snapshot solution
    - ETCDCTL_API=3 etcdctl \
        snapshot save snapshot.db
    $ etcdctl snapshot save snapshot.db
    $ etcdctl snapshot status snapshot.db
    
    - to restore
      $ service kube-apiserver stop
      $ etccdctl snapshot restore snapshot.db
        - this configures a new members to a new cluster
        - to prevent a new member accidentaly to join an existing cluster
        - new data-dir will be created
          - --data-dir /var/lib/etcs-from-backup
        - the new configuration for etcd.service id needed
          etcd.service
          --name ..
          --data-dir=/var/lib/etcd-from-backup
      $ systemctl daemon-reload
      $ service etcd restart
      $ service kube-apiserver start

    - with all etcdctl commands, you must specify
      --endpoints=https://127.0.0.1:2379  \
      --cacert=/etc/etcd/ca.crt \
      --cert=/etc/etcd/etcd-server.crt \
      --key=/etc/etcd/etcd-server.key

- if managed service, you probably do not have access to the etcd
  - then resource config files option is probably a better way



6.10 backup and restore - solution
---
- check the etcd version
  $ kubect describe -n kube-system pod etcd-controlplane | grep -i image

- what ports to reach etcd cluster from master node
  --listen-client-urls=https://127.0.0.1:2379,https://172.17.0.12:2379
    - 2379

- for other nodes to join the etcd cluster
  --listen-peer-urls=https://172.17.0.12:2380
    - 2380
  
- the location of the server cert file
  --cert-file=/etc/kubernetes/pki/etcd/server.crt

- the location of the etcd ca.cert
  - --truested-ca-file=/etc/kubernetes/pki/etcd/ca.cert
  - or is it the
  - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - same location

- create the backup using snapshoot tool
  - ETCDCTL_API=3 etcdctl version


$ # ETCDCTL_API=3 etcdctl --cert="" --cacert="" snapshot save
  $ kubectl describe -n kube-system pod etcd-controlplane | less
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
$ ETCDCTL_API=3 etcdctl --cert="/etc/kubernetes/pki/etcd/server.crt" --cacert="/etc/kubernetes/pki/etcd/ca.crt" --key="/etc/kubernetes/pki/etcd/server.key" snapshot save /opt/snapshot-pre-boot.db
  - enpoint option not needed -> this is run on the same server where etcd is installed

- snapshot status
  $ ETCDCTL_API=3 etcdctl --cert="/etc/kubernetes/pki/etcd/server.crt" --cacert="/etc/kubernetes/pki/etcd/ca.crt" --key="/etc/kubernetes/pki/etcd/server.key" snapshot status /opt/snapshot-pre-boot.db
  - or simply
  $ ETCDCTL_API=3 etcdctl snapshot status /opt/snapshot-pre-boot.db

- snapshot restore
  - todo: this didnt work, possibly because of the version!
  $ ETCDCTL_API=3 etcdctl snapshot restore /opt/snapshot-pre-boot.db --data-dir=/var/lib/etcd-from-backup
  - then update etcd.yaml
    - /etc/kubernetes/manifests/etcd.yaml
    - mount the "volumes / hostPath"
      - path: /var/lib/etc-from-backup
    - save and exit



7.5 TLS basics
---
~6.30 - most interesting
- PKI = public key infrastructure



7.6 tls in kubernetes
---
- server components
  - kube-apiserver
    - apiserver.crt + apiserver.key
    - possible to create a service specific certificates/keys
      - apiserver-etcd-client.crt + apiserver-etcd-client.key
  - etcd server
    - etcdserver.crt + etcdserver.key
  - kubelet server
    - kubelet.crt + kubelet.key

- clients, from the perspective of k8s, are
  - admin user
    - admin.crt + admin.key
  - scheduler
    - scheduler.crt + scheduler.key
  - controller-manager
    -  controller-manager.crt +  controller-manager.key
  - kube-proxy
    - kube-proxy.crt + kube-proxy.key

- k8s wants u to have CA (certificate authority) for the cluster
  - possible to have more than one CA
    - one for etcd specifically
      -this would sign etcd (crt+key) & api-etcd (crt+key)
    - one for all components in the cluster



7.7 certificates creation
---
- diff tools available
  - easyrsa
  - openssl
  - cfssl

1] create CA key
--
- generate keys
  $ openssl genrsa -out ca.key 2048
    - ca.key
- certificate signing request
  $ openssl req -new -key ca.key -subj 
    "/CN=KUBERNETES-CA" -out ca.csr
      - ca.csr
      - CN=...  # Common Name 
      - the name of the component the certificate is for
- sign certificates
  $ openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt
    - ca.crt
    - this ca.crt will be needed from all components in k8s!

2] client-side certificates 
(admin user)
--
- generate keys
  $ openssl genrsa -out admin.key 2048
    - admin.key
- certificate signing request
  $ openssl req -new -key admin.key -subj \
    "/CN=kube-admin/O=system:masters" -out admin.csr
      - admin.csr
      - O=system:masters  # group:SYSTEM:MASTERS
- sign certificate
  $ openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt
    - admin.crt

- same procedure for all other components
  - admin user
  - kube-scheduler
  - kube-controller-manager
  - kube-proxy

- kube scheduler and kube controller manager 
  - they are in control plane
  - their name must be prefixed with "SYSTEM:"
    - "SYSTEM:KUBE-SCHEDULER"
    - "SYSTEM:KUBE-CONTROLLER-MANAGER"

- what you do with certificates?
  - e.g. admin => use instead uname/pwd
    - $ curl https://kube-apiserver:6443/api/v1/pods  \
      --key admin.key --cert admin.crt  \
      --cacert ca.crt
    
    - or put details in kube-config.yaml
      apiVersion: v1
      clusters:
      - cluster
          certificate-authority: ca.crt
          server: https://kube-apiserver:6443
        name: kubernetes
      kind: Config
      users:
      - name: kubernetes-admin
        user:
          client-certificate: admin.crt
          client-key: admin.key

3] server-side certificates
--

- etcd servers
  - the common name CN=ETCD_SERVER
- in HA cluster more etcd servers possible
  - etcdpeer1.crt & etcdpeer1.key
  - CN=ETCD-PEER
  - all have to be specified in etcd.yaml
    - --peer-cert-file
    - ca.crt has to be specified!

- kube-apiserver
  - the most popular component
  - KUBE-API SERVER
  - all different names have to be present
    - kubernetes
    - kubernetes.default
    - kubernetes.default.svc
    - kubernetes.default.svc.cluster.local
    - 10.96.0.1
    - 172.17.0.87
  
  - generate certificates
    $ openssl genrsa -out apiserver.key 2048
      - apiserver.key
    $ openssl req -new -key apiserver.key -subj \
    "/CN=kube-apiserver" -out apiserver.csr
      - apiserver.csr
      - create openssl.cnf for alternative DNS entries
        [alt_names]
        DNS.1 = kubernetes
        DNS.2 = kubernetes.default
        DNS.3 = kubernetes.default.svc
        DNS.4 ...
    $ openssl x509 -req -in apiserver.csr \
    -CA ca.crt -CAkey ca.key -out apiserver.crt
    # sign the certificate
      - apiserver.crt

- kubelet
--
- kubectl nodes (server cert)
  - all nodes need key-cert pair
  - CN names will be: node01, node02, node03
  - use created certificates in the kubelet-config.yaml file
      kind: KubeletConfiguration
      ...
      tlsCertFile: "/var/lib/kubelet/kubelet-node01.crt"
      tlsPrivateKeyFile: "/var/lib/kubelet/kubelet-node01.key"
  - do this for all certificates

- kubectl (client cert)
--
- used for kubeapi-server authentication
- names
  - CN=system:node:node01
  - CN=system:node:node02
  - CN=system:node:node03
- group: system:nodes



7.8 view certificates
---
- it depends on the setup of cluster 
  - the hard way, or
    - /etc/systems/system/kube-apiserver.service
  - kubeadm
    - /etc/kubernetes/manifests/kube-apiserver.yaml

- create a spreadsheet for all certificates: kubeadm.certificates.csv

kubeadm.certificates.csv
---
- first row:
  - component, type, certificate path, cn name, alt names, organization, issuer, expiration

- component column, type, ... (columns):
  - kube-apiserver, server, ...
    kube-apiserver, server, ...
    kube-apiserver, server, ...
    kube-apiserver, client (kubelet), ...
    kube-apiserver, client (kubelet), ...
    kube-apiserver, client (etcd), ...
    kube-apiserver, client (etcd), ...
    kube-apiserver, client (etcd), ...

- kubeadm setup: look in the kube-apiserver definition file
  - /etc/kubernetes/manifests/kube-apiserver.yaml
   - --client-ca-file
   - --etcd-cafile
   - --etcd-certfile
   - --etcd-keyfile
   - --kubelet-client
   - --kubelet-client-key
   - --tls-cert-file
   - --tls-private-key-file

  - check inside of each certificate for the details
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
  
  - decode the details
    $ openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout
      - Subject: CN=kube-apiserver
      - X509v3 Subject Alternative Name:
      - validity: Not After : Feb 11 ... 2020
      - Issuer: CN=kubernetes

- certificate requirements listed in k8s docu
  - https://kubernetes.io/docs/setup/best-practices/certificates/

- if you run into issues, check logs
  - if you setup the cluster yourself, with native services
    - then use the OS logging functionality
      $ journalctl -u etcd.service -l
      # ClientTLS: cert
      #
  - if you setup the cluster using kubeadm
    - then use k8s logging functionality
      $ kubectl logs etcd-master
      # ClientTLS: cert
      #
    - if the k8s functionality doesn't work (e.g. pod issues)
      $ docker ps -a    # get the CONTAINER ID (e.g. 87fc6991)
      $ docker logs 87fc
      # ClientTLS: cert



7.9 manage certificates (certificate api)
---
- kubeadm creates and stores certificates on master node
  - CA Server = master node

- certificates api
  - manages certificates
  - sign requests
  - rotate certificates

- cert sign request is sent to kubernetes through api call
- how this works
  - the user creates a key
  - an admin receives a sign request
  - an admin creates CertificateSigningRequest object
  - these requests can be reviewed and approved (kubectl commands)
  - certificate can be shared with the user (after it is extracted)

- step-by-step workflow
  - the user creates a key
    $ openssl genrsa -out jane.key 2048
  - the user sends the request to the administrator
    $ openssl req -new -key jane.key -subj "/CN=jane" -out jane.csr
  - out of the key, the admin creates CertificateSigningRequest object
    - cat jane.csr | base64
    - jane-csr.yaml
    ---
    apiVersion: certificates.k8s.io/v1beta1
    kind: CertificateSigningRequest
    metadata:
      name: jane
    spec:
      groups:
      - system: authenticated
      usages:
      - digital signature
      - key encipherment
      - server auth
      request:
        LS0tLS1CRU ...
        ...
        ... base64 encoded jane.csr ... ...
        ...
        ... ... ... ... ... ... 41TXVxOTlOZbnJ
  - all cert signing requests can be seen by administrators
    $ kubectl get csr
      - NAME
      - CONDITION = Pending
  - admin can approve the request
    $ kubectl certificate approve jane
  - k8s signs the certificates & generates a certificate for the user
    - using ca key-pair
  - certificate extracted and shared with user
    $ kubectl get csr jane -o yaml
      apiVersion: certificates.k8s.io/v1beta1
      status:
        certificate:
      LS0tL ...
      ..
      .. ... ... LQo=
    $ echo "LS0 ... LQo=" | base64 --decode
      -----BEGIN CERTIFICATE -----
      MIICWDCC
      ...
      -----END CERTIFICATE -----
  - the decoded certificate can than be shared with the end user

- this everything is cone by controller-manager
  - csr-approving
  - csr-signing

- /etc/kubernetes/manifests/kube-controller-manager.yaml
 - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
 - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key



7.10 KubeConfig
---
$ curl https://my-kube:6443/api/v1/pods \
  --key admin.key
  --cert admin.crt
  --cacert ca.crt

- kubectl config file location
  - $HOME/.kube/config

- sections
    clusters:
    - name: kubernetes
    - name: development
    - name: production

    contexts:
    - name: kubernetes-admin@kubernetes
    - name: prod-user@production

    users:
    - name: kubernetes-admin
    - name: prod-user

- to change context is give the user right for the cluster

- to change the context
  $ kubectl config use-context prod-user@production
    - changes the $HOME/.kube/config file
    - "current-context: prod-user@production"
    
- $ kubectl config -h   # check

- namespaces could be set for context
  contexts:
  - name: admin@production
    context:
      cluster: production
      user: admin
      namespace: finance

- certificates in KubeConfig could be specified using
  - certificate-authority: /etc/kubernetes/pki/ca.crt
    # or
  - certificate-authority-data: LS0t .. BASE64 encoded .. PbnJ


- play around jack
  - prepare needed certs
  $ cat ~/.kube/config
  $ echo "LS0 ... LS0tCg==" | base64 --decode > admin.crt
  $ echo "LS0 ... LS0tCg==" | base64 --decode > admin.key

  $ kubectl get pods
    - $ curl https://172.30.1.2:6443/api/v1/pods \
        --key admin.key \
        --cert admin.crt  \
        --cacert /etc/kubernetes/pki/ca.crt

- rewind it and play it again
  $ kubectl version
    - $ curl https://172.30.1.2:644310.96.0.1i/ca.crt

- check the master node DNS entry
  $ openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout
    - gives:
      DNS:controlplane
      DNS:kubernetes
      DNS ...
  $ kubectl version
    - $ curl https://controlplane:6443/version \
        --key admin.key \
        --cert admin.crt  \
        --cacert /etc/kubernetes/pki/ca.crt



7.11 API groups
---
- existing
  - api:
    /metrics
    /healthz
    /version
    /api
    /apis
    /logs

- for the cluster functionality responsible are
  - /api    # core
  - /apis   # named
    $ 

- named
  /apis
    - API Groups:
      /apps
        /v1
          - Resources:  # aka nouns
            /deployments
              - Verbs:  # aka actions
                /list
                /get
                /create
                /delete
            /replicasets
            /statefulsets
      /networking.k8s.io
        /v1
          /networkpolicies

- REST looks like
  - named/API Groups/version/Resources/Verbs
    - resources = nouns
    - verbs = actions

  - Resources are the nouns of the Web 
    - they describe any object, document, or thing that 
      you may need to store or send to other services

- documentation
  - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#pod-v1-core

- infos could be got by exec curl localhost:6433
  - prepare needed certs
    $ cat ~/.kube/config
    $ echo "LS0 ... LS0tCg==" | base64 --decode > admin.crt
    $ echo "LS0 ... LS0tCg==" | base64 --decode > admin.key

  $ curl https://controlplane:6443 \
      --key admin.key \
      --cert admin.crt  \
      --cacert /etc/kubernetes/pki/ca.crt
    #
    - returns "paths"
  
  $ curl https://controlplane:6443/api \
      --key admin.key \
      --cert admin.crt  \
      --cacert /etc/kubernetes/pki/ca.crt

- instead of authentication with keys
  - curl https:://... --key ... 
  - you can start
    $ kubectl proxy
      # Starting to serve on 127.0.0.1:8001
    $ curl http://localhost:8001

- kube-proxy != kubectl proxy
  !!!



7.12 authorization
---
!= authentication
  - authentication = access

- authorization
  - what can you do

- authorization mechanisms
  - node
  - ABAC  # attribute based authorization
  - RBAC  # role based authorization
  - Webhook
  - AlwaysAllow
  - AlwaysDeny

- node based autorization
--
- kubelet + kube API
  - system:node:node01

- node authorizer
  - authorizes any request comming from a user with 
      - a name system:node
      - and a part of the system:node group

- ABAC
--
- external access
- a user has the set of permissions
  - can view pod
  - can create pod
  - can delete pod
- this is done per a policy file with a set of policies
  - this file is passed to the API server
  - after change, the kubeAPI-server has to be restarted
    - difficult to manage

- RBAC
--
- role with a set of permissions
  - a user is then asocciated to the role
- (more) standard approach of maanging k8s

- Webhook
--
- outsourcing of all authorization mechanisms
- e.g. Open Policy Agent

- where is this setup
  - kube-apiserver.yaml
    -- authorization-mode=AlwaysAllow   # per default
  - multiple modes possible
    -- authorization-mode=Node,RBAC,Webhook
      - this is the order or authorization (1.Node, then 2.RBAC, then 3.Webhook)
        - e.g. deny, deny, allow = allow



7.13 RBAC
---

1] create the role
  developer-role.yaml
  ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      name: developer
    rules:
    - apiGroups: [""]
      resources: ["pods"]
      verbs: ["list","get","create","update","delete"]
    - apiGroups: [""]
      resources: ["ConfigMap"]
      verbs: ["create]

  - for the "core" group the "apiGroups: [""]" can stay empty
  - multiple rules (upper example) could be given for single role

  - $ kubectl create -f developer-role.yaml

2] bind the user to the role 
  devuser-developer-binding.yaml
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    name: devuser-developer-binding
  subjects:
  - kind: user
    name: dev-user
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: Role
    name: developer
    apiGroup: rbac.authorization.k8s.io

  $ kubectl create -f devuser-developer-binding.yaml

- this role + role binding is inside of the default namespace

- get the roles and bindings info
  $ kubectl get roles
  $ kubectl get rolebindings
  $ kubectl describe role developer
  $ kubectl describe rolebinding devuser-developer-binding

- get the user rights
  $ kubectl auth can-i create deployments
    - yes 
  $ kubectl auth can-i delete nodes
- check for other users (as admin)
  $ kubectl auth can-i create deployments --as dev-user
  $ kubectl auth can-i create pods --as dev-user
  $ kubectl auth can-i create pods --as dev-user --namespace test

- resource names could also be restricted
  - resourceNames: ["blue", "orange"]   # e.g. pods are resources



7.14 cluster roles and roles bindings
---
- nodes can not be grouped in the namespace
  - nodes are cluster-wide resources

- resources are either
  - namespaced
  - cluster scoped

- namespaced resources
  $ kubectl api-resources --namespaced=true 
    - pods
    - replicasets
    - jobs
    - deployments
    - ...

- cluster scoped
  $ kubectl api-resources --namespaced=false
    - nodes
    - PV
    - clusterroles
    - clusterrolebindings
    - certificatesigningrequests
    - namespaces

- cluster roles have to be created, e.g.
  - cluster-admin (create, view, delete nodes)
  - storage-admin (view PVs, create PVs, delete PVs)

- to create a cluster role
  - cluster-admin-role.yaml
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  matadata:
    name: cluster-administrator
  rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["list", "get", "create", "delete"]

  $ kubectl create -f cluster-admin-role.yaml

  - cluster-admin-role-binding.yaml
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: cluster-admin-role-binding
  subjects:
  - kind: user
    name: cluster-admin
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: ClusterRole
    name: cluster-administrator
    apiGroup: rbac.authorization.k8s.io
  
  $ kubectl create -f cluster-admin-role-binding.yaml

- cluster role could be created for a namespaced resource
  - the user in that case has the access across all namespaces
  - e.g. authorize the user with ClusterRole to access all pods accross the whole cluster

- k8s creates many cluster roles by default when it's set-up

# !!!
$ kubectl api-resources -o wide # get the roles and verbs



7.15 service accounts
---
$ kubectl create serviceaccount dashboard-sa
$ kubectl get serviceaccount
$ kubectl describe serviceaccount dashboard-sa
  - Tokens: dashboard-sa-token-kbbdm
    - Name (dashboard-sa) -> creates object ->token in secret -> connects to the service account
$ kubectl describe secret dashboard-sa-token-kbbdm
  - token:
    eyJhb...iwia3
- this can be used with curl to make a REST call
  $ curl https://192.168.56.70:6443/api -insecure \
    --header "Authorization: Bearer eyJhb...iwia"
      # "eyJhb ... iwia" <--TOKEN

- but what if the application is on the k8s itself?
  - then mount the secret token as a volume inside the pod where the app runs

- each namespace has the "default" service account already created and mounted
  $ kubectl get serviceaccount
    - default sa and its token are mounted as a volume to the pod
  $ kubectl describe pod my-kubernetes-dashboard
    Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-j4hkv
    SecretName: default-token-j4hkv
  $ kubectl exec -it my-kubernetes-dashboard ls /var/run/secrets/kubernetes.io/serviceaccount
    # ca.crt namespace token
  $ kubectl exec -it my-kubernetes-dashboard cat /var/run/secrets/kubernetes.io/serviceaccount

- the default sa is very restricted
  - todo: check permissions!

- to use additional service account (dashboard-sa), specify in pod-definition.yaml
    ...
    spec:
      containers:
        - name: my-kubernetes-dashboard
          image: my-kubernetes-dashboard
      serviceAccount: dashboard-sa

- pod has to be deleted and recreated (if you change the service account)
- if deployment, a new rollout will be triggered

- not to mount default service account automatically, specify:
  automountServiceAccountToken: false



7.16 image security
---
- image:    docker.io/nginx/nginx
          ( registry/user_account/image_repository )

- popular registries:
  - docker.io   # docker registry
    gcr.io      # google registry
      - gcr.io/kubernetes-e2e-test-images/dnsutils

- private repository (the docker way)
  $ docker login private-registry.io  # login first
  $ docker run private-regitry.io/apps/internal-app   # then run

- private repo (the k8s way)
  - create a secret object )(of type "docker-registry")
    $ kubectl create secret docker-registry regcred \
      --docker-server= private-registry.io  \
      --docker-username= registry-user  \
      --docker-password=  registry-password \
      --docker-eamil= user@org.com

  - use the "regcred" secret inside of pod-definition.yaml
    imagePullSecrets:
    - name: regcred


7.17 security contexts
---
- can be defined on pod or on container level
- if defined on container level, this overrides the pod level

- security-context-pod-level.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: web-pod
spec:
  securityContext:
    runAsUser: 1000

  containers:
    - name: ubuntu
      image: ubuntu
      command: ["sleep","3600"]


- security-context-container-level.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: web-pod
spec:
  containers:
    - name: ubuntu
      image: ubuntu
      command: ["sleep","3600"]
      securityContext:
        runAsUser: 1000


- to add capabilities (supported on container level only!)
---
apiVersion: v1
kind: Pod
metadata:
  name: web-pod
spec:
  containers:
    - name: ubuntu
      image: ubuntu
      command: ["sleep","3600"]
      securityContext:
        runAsUser: 1000
        capabilities:
          add:  ["MAC_ADMIN"]



7.18 network policy
---
- ingress vs. egress
  - the direction of the traffic origin is relevant
    - user -> web app -> backend app -> db

- network security in k8s network
  - per default all pods can comunicate with each other

- network policy
  - if i want to prevent communication for some pods
  - network policy is applied to a pod

- the rule:
  allow
  Ingress
  traffic
  from
  API Pod
  on
  Port 3306
---
- network-policy.yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          name: api-pod
    ports:
    - protocol: TCP
      port: 3306

$ kubectl create -f network-policy.yaml

- Flannel doesn't support network policies!

- solutions that support network policies:
  - kube-router
  - calico
  - romana
  - weave-net

- if cluster doesn't support newtwork policy, they are still createable
  - they won't work
  - no error message will be thrown

- There is no imperative command to extract yaml instead of that you can copy template from official k8s docs.
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/#networkpolicy-resource  # docu
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/   # docu

- Installing Romana with kubeadm 
  - https://github.com/romana/romana/tree/master/containerize   # docu



7.19 developing network policies
---
- 3 availableselectors for (ingress) traffic
  - from:
    - podSelector (matchLabels)
    - namespaceSelector (matchLabels)
    - ipBlock (cidr: 192.168.5.10/32)

- if allow ingress from API pod to DB pod
  - then no need to specify egress on DB
    - this will work automatically
      - but, if DB Pod needs to make calls to API pod then egress needs to be defined

- policy example AND / OR
---
policyTypes:
- Ingress
ingress:
- from:
  - podSelector:
      matchLabels:
        name: api-pod
    namespaceSelector:  # AND !!!
      matchLabels:
        name: prod
  - ipBlock:            # OR  !!!
      cidr: 192.168.5.10/32

- policy example OR / OR
---
ingress:
- from:
  - podSelector:        # OR !!!
      matchLabels:
        name: api-pod
  - namespaceSelector:  # OR !!!
      matchLabels:
        name: prod
  - ipBlock:            # OR  !!!
      cidr: 192.168.5.10/32


- egress:
  - when DB wnats to reach outside service
    - e.g. backup server

- policy example:
---
policyTypes:
- Ingress
- Egress
ingress:
- from:
  - podSelector:
      matchLabels:
        name: api-pod
  ports:
  - protoclo: TCP
    port: 3306
egress:
- to
  - ipBlock:
      cidr: 192.168.5.10/32



7.20 network policies
---
$ kubectl get netpol    # get network policies
$ kubectl get pods -l name=payroll
$ kubectl describe netpol payroll-policy


~5.45
  - create network policy



8.02 docker storage intro
---
- 2 types
  - storage drivers
  - volume drivers



8.03 container storage interface
---
- k8s was working on docker at the beginning
- later on, important toopen doors for other container solutions like
  - rkt
  - cri-o 
- so CRI (container runtime interface) was built

- similarly CNI (container networking interface) introduced
  - networking vendors like
    - weaveworks
    - flannel
    - cilium

- CSI = container storage interface
  - CSI is not a k8s standard
    - currently onboard are
      - k8s
      - cloud foundry
      - mesos
  - vendors like
    - portworkx
    - amazon ebs
    - dell emc
    - glusterFS


- more on the subject
  - https://github.com/container-storage-interface/spec???



8.6 volumes
---
- different storage solutions
  - should be used for multinode
  - dir mounts will assume all nodes have the same mounted dir (with the same content)
    - which is not true



8.7 persistent volumes
---

- large portion of storage defined
  - that is, persistent volume
  - this defined by administrator

- users then carve out and take what they need
  - using persisten volumes

- https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes
- Types of Persistent Volumes
  - PersistentVolume types are implemented as plugins. 
    Kubernetes currently supports the following plugins:
      - csi - Container Storage Interface (CSI)
      - fc - Fibre Channel (FC) storage
      - hostPath - HostPath volume (for single node testing only; WILL NOT WORK in a multi-node cluster; consider using local volume instead)
      - iscsi - iSCSI (SCSI over IP) storage
      - local - local storage devices mounted on nodes.
      - nfs - Network File System (NFS) storage


pv-definition.yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol1
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 1Gi 
  hostPath:
    path: /tmp/data

$ kubectl create -f pv-definition.yaml
$ kubectl get PersistentVolume

- you can replace "hostPath" with a vendor/storage type


pv-definition.yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol1
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 1Gi 
  awsElasticBlockStore:
    volumeID: <volume-id>
    fsType: ext4



8.7 persistent volume claim
---
- this defined by users
- it uses persistent volume
- persistent volume and persistent volume claims are different k8s objects
  - pv
  - pvc

- pvc are bounded to pv's based on
  - properties
    - sufficient capacity
    - access modes
    - volume modes
    - storage class
  - or labels & selectors
    selector:
      matchLabels:
        name: my-pv

- 1:1 relationship of claims & volumes
  - if pvc < pv capacity, then no other claim can take the rest of pv's capa

- pvc pending if no pv's are available


pvc-definition
---
apiVersino: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    ReadWriteOnce

  resources:
    requests:
      storage: 500Mi

$ k create -f pvc-definition.yaml
$ k get Persistentvolumeclaim
$ k delete persistenvolumeclaim myclaim   # delete persistent volume claim
  - after pvc delition, the pv stays per default
    - persistentVolumeReclaimPolicy: Retain
      - manual delition needed
    - or
      - persistentVolumeReclaimPolicy: Delete
        - automatically deleted
    - or
      - persistentVolumeReclaimPolicy: Recycle
        - the data in PV will be scrubbed after pvc delition


8.9 persistent volumes and persistent volume claims solution
---
- check the logs in the pod "webapp"
  $ kubectl exec webapp -- cat /log/app.log

- get the pod definiton file
  - delete all unnecessary lines 
    - d100d in vim editor

- nginx
  - logs per default in /var/log/nginx directory

- hostPath configuration example
  - https://kubernetes.io/docs/concepts/storage/volumes/#hostpath-configuration-example
  [
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: Directory
  ]

- container definition:
spec:
  containers:
  - env: 
    - name: LOG_HANDLERS
      value: file
    image: kodekloud/event-simulator
    name: event-simulator
    volumeMounts:
    - mountPath: /log
      name: log-volume
  volumes:
  - name: log-volume
    hostPath:
      path: /var/log/webapp

$ kubectl explain persistentvolume --recursive
  - /hostpath


- cat pv.yaml 
--
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 10Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  storageClassName: slow
  hostPath:
    path: /pv/log


- cat pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Mi

$ k get pv,pvc
$ k describe pvc
  -> waiting for first consumer to be created before binding

https://stackoverflow.com/questions/55044486/waitforfirstconsumer-persistentvolumeclaim-waiting-for-first-consumer-to-be-crea


kubectl get storageclasses.storage.k8s.io
kubectl get storageclasses.storage.k8s.io gp2 -o yaml > gp2.yaml

- todo
  - work more on the examples



8.10 storage classes
---
- storage class creates automatically the pv
  - dynamic provisioning

- examples for pv, pvc and pod yaml files
  ~0.10

- kind: storageClassName



9.2 networking
---
- 2 nodes which i want to connect
- i do that via switch
  - switch defines the network (meh)

    - works on th elevel of one network
- i need an interface on the host - eth0
  $ ip link   # check the interface

- then i assign the systems the addresses on the same network
  - system A: $ ip addr add 192.168.1.10/24 dev eth0
  - system 8: $ ip addr add 192.168.1.11/24 dev eth0
   - comm works
    $ ping 192.168.1.11   # !

- to connect more networks, a router is needed
- a router has addresses of both networks assigned
  - 192.168.1.1 & 129.168.2.1

- how does the system B knows the router's address (192.168.1.1)?
  - through the system gateway  # gw = door to other networks
  - gw = 192.168.1.1

- to see the existing routing configuration on a system
  $ route

- to configure a system on 192.168.1.x nw to reach x.x.2.0 network
  $ ip route add 192.168.2.0/24 via 192.168.1.1
                  DESTINATION           GW

- analogue configuration has to be done on all the systems

- default gateway (= internet)
$ ip route add default via 192.168.2.1
  - default = 0.0.0.0   # both can be used
  - it means "any ip destination"

- 0.0.0.0 in the Gateway field ("route" command) means "no gateway needed"
  - because the system is it's own network


- linux host as a router
--
- how to setup a linux host as a router
  - linux host = B 
  - B has eth0 and eth1

- per default a linux host doesn't route between eth0 <> eth1
  - security reasons
    $ cat /proc/sys/net/ipv4/ip_forward
      0
    - set to 1 to enable forwarding
    $ echo 1 > /proc/sys/net/ipv4/ip_forward

  - to have this setting saved across reboots
    $ /etc/sysctl.conf
      net.ipv4.ip_forward = 1

- important commands
$ ip link
$ ip addr
$ ip addr add
$ etc/nw/interfaces to persist
$ ip route
$ ip route add
$ cat /etc/sys/net/ipv4/ip_forward



9.3 dns 
---
- linux host

- cat /etc/hosts
  192.168.1.11  db

$ ping db # works

- name resolution 
  = translation of ip's into names

- such maintenance of name resolution (host file) is possible on small systems
- on k8s this /etc/hosts file is moved to DNS server
  - let's say ours is on 192.168.1.100

- we have to point all hosts to the DNS server
  $ cat /etc/resolv.conf
  nameserver  192.168.1.100

- what of entries to a same hostname show different ip addresses in
  - / etc/hosts   # local
  - and DNS server
    - in that case the server looks first in /etc/hosts
      - this order can be changed
        cat /etc/nsswitch.conf
        # hosts:   files mdns4_minimal [NOTFOUND=return] dns myhostname
          - "files" refers to /etc/hosts
          - "dns" refers to the dns-server
            - this order can be modified

- ping hostname not in listens
  # temporary failure in name resolution

- subdomains are: www, maps, abc
  www.google.com
  maps.google.com
  abc.google.com

- inside of an organization, how are subdomains found?
  - you add "search" in resolv.conf
    $ cat /etc/resolv.conf
      nameserver  192.168.1.100
      search      mycompany.com   prod.mycompany.com

  $ ping web    # searches:
    - web.mycompany.com
    - and web.prod.mycompany.com

- record types
--
- A record  = storing ipv4 to hostnames
- AAAA (quad A) record  = storing ipv6 to hostnames
- CNAME = mapping one name to another name (food.web = eat.web)

- tools to test dns resolution
  - ping
  - nslookup  # nslookup www.google.com
    - nslookup does not consider entries in the local etc/hosts file!!!
    - only considers dns
  - dig     # dig www.google.com



9.4 network namespaces
---
- used for isolation

- on the container
  $ ps aux  # gives only the container process(es)

- on the host
  $ ps aux  # gives all processes: host + containers

- ARP table
  - ip to MAC addr link

- networking part
  - host has its own
    - nw interfaces (eth0, ..)
    - routing tables
    - arp tables
  - the created container has it's own namespace (isolation)
    - container has it's own
      - nw interface  (veth0 = virtual)
      - routing table
      - arp table

- create a new network namespace on a linux host (red, blue)
  $ ip netns add red    # create a new network namespace
  $ ip netns add blue   # create a new network namespace

- list the network namespaces
  $ ip netns  # list the network namespaces

- list the nw interfaces on my host
  $ ip link     # list the nw interfaces

- list the nw interfaces within the "red" namespace (the container)
  $ ip net ns exec red ip link      # list the nw interfaces within the "red" namespace
  $ ip -n red link  # same as above # list the nw interfaces within the "red" namespace

- similar with arp tables
  $ arp                       # inside host gives all arp entries
  $ ip net ns exec red arp    # inside the namespace gives no entry!

- the same for routing table
  $ route                       # on host gives a list
  $ ip net ns exec red route   # inside the namespace no list

- to have the connectivity between red and blue namespace, needed is
  1] virtual ethernet pair (aka cable or pipe)
    - this pipe connects veth red with blue
      $ ip link add veth-red type veth peer name veth-blue
  2] attach veth to a namespace
    $ ip link set veth-red netns red
    $ ip link set veth-blue netns blue
  3] assign the ip addresses to each ns
    $ ip -n red addr add 192.168.15.1 dev veth-red
    $ ip -n blue addr add 192.168.15.2 dev veth-blue
  4] bring up the interface
    $ ip -n red link set veth-red up
    $ ip -n blue link set veth-blue up
  5] try a ping 
    $ ip netns exec red ping 192.168.15.2
  6] check arp
    $ ip netns exec red arp     # now identifies the blue one
    $ ip netns exec blue arp    # now identifies the red one
    $ arp   # arp on the host has no idea of the new namespaces

- this worked between 2 namespaces, but what if there are plenty
  - a virtual network inside the host is needed
  - for that a virtual switch is needed

- there are diferrent virtual switch solutions
  - linux bridge  # the native solution
  - open vSwitch ( OvS )

- create virtual switch (virtual network) using linux bridge option
  1] add virtual interface to the host
    $ ip link add v-net-0 type bridge   # add virtual interface to the host
    $ ip link                           # shows all interfaces, including the new one
    $ ip link set dev v-net-0 up        # bring the interface up
    - for the namespaces, this is like a switch to connect to
    - to the host, it's like an interface
  2] connect the namespaces to this virtual switch
      (first, the old pipe needs to be deleted
        $ ip -n red link del veth-red )
    $ ip link add veth-red type veth peer name veth-red-br
      - creates a cable/pipe from red to the bridge
      - "-br" means it connects to the bridge connection
    $ ip link add veth-blue type veth peer name veth-blue-br
      - pipe from blue to the bridge nw
  3] attach namespace interfaces to the virtual mutual network
    $ ip link set veth-red netns red     # attach veth-red to the red namespace
    $ ip link set veth-red-br master v-net-0  # attach veth-red-br to the virtual network (master)
    $ ip link set veth-blue netns blue
    $ ip link set veth-blue-br master v-net-0
  4] set the ip addresses
    $ ip -n red addr add 192.168.15.1 dev veth-red
    $ ip -n blue addr add 192.168.15.2 dev veth-blue
  5] turn the devices up
    $ ip -n red link set veth-red up
    $ ip -n blue link set veth-blue up
  - etc for all network namespaces

- to ping from host the namespace doesn't work
  $ ping 192.168.15.1   # Not Reachable!  # err

- this to work, i need to assign the ip address to the interface
  $ ip addr add 192.168.15.5/24 dev v-net-0
  $ ping 192.168.15.1   # ping works now!

- this entire network is private/restricted inside the host
  - namespace -> to the outside world = doesn't work
  - outside world -> to the namespace = doesn't work

- e.g. if i ping some outside lan network/host from my blue namespace
  $ ip netns exec blue ping 192.168.1.3   # x.x.x.3 is outside host
    - the blue ns checks route
      $ ip netns exec blue route  # no route found
    - this gives error: "Connect: Netork is unreachable"
  - i need to provide the GW to the outside world
    $ ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5
      - gw = 192.168.15.5
  - if i try to ping now no "unreachable", but still no response back
    - we need a NAT here
      $ iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -j MASQUERADE
      $ ip netns exec blue ping 192.168.1.3   # works now

- e.g. if i want to ping ineternet from the blue namespace
  $ ip netns exec blue ping 8.8.8.8
    # Connect: Network is unreachable -> routing table fix needed
  $ ip netns exec blue route
    - our host can reach any network, so host shaould be the GW
  $ ip netns exec blue ip route add default via 192.168.15.5
  $ ip netns exec blue ping 8.8.8.8
    - works now

- what about reaching from outside LAN to the blue namespace web app @ port 80?
  - ping 192.168.15.2   # from the outside host on LAN -> Connect: Network is unreachable
  - two solutions to that
    1] set the route on the x.x.x.3 host  # not wanted
    2] set the port forwarding
      $ iptables -t nat -A PREROUTING --dport 80 --to-destination 192.168.15.2.:80 -j DNAT
      - any traffic to localhost:80 forwarded to 192.168.15.2:80

- todo: practice on the vm



9.5 docker networking
---
- three types of docker networking
  - none    # docker run --network none nginx (no connectivity to the host)
  - host    # docker run --network host nginx (only one container can connect to :80)
  - bridge  # cocker run nginx  (private network for all containers -> more nginx to :80, different ip for each container)
    - bridge is what we need
    - this bridge network is like a switch for the containers
    - to check the nw address
      $ ip addr

$ docker network ls
  - name=bridge

$ ip link   # gives docker0
  - on the host, this bridge is created with the name docker0
  - docker uses this to add the bridge nw type
    $ ip link add docker0 type bridge
  - note that the iterface (network) is currently down
    $ ip addr   # gives the nw address

- docker creates a nw namespace   # 2e41deb9ef...3541b4
  - whenever a container created
  - container = nw namespace   # will be a synonym from now on

$ ip netns  # list the namespace
  - gives b3165c10a92b
    - checkut the resource section for a hack here
  $ docker inspect b3165c10a92b

- docker attaches nw namespace (container) to the bridge via virtual cable (pipe)
  $ ip link
    - one part is attached to the local bridge   # vethb3165c10a92b
      # vethb3165c10a92b: <BROADCAST ...> mtu .. master docker0
  $ ip -n b3165c10a92b link
    - the other is linked to the container interface  # eth0@if8:
  - the interface inside container gets assigned ip
    $ ip -n b3165c10a92b addr   # 172.17.0.3/16

- so, every time a container is created, docker:
  - creates a namespace
  - creates a pair of interfaces
  - attaches one end to the container   # eth0@if10
  - attaches the other end to the bridge network

- the interface pairs can be identified via numbers
  - odd + even = pair
  - eth0@if12 + veth6813421@if11  = pair
  - if10 + if9  = pair

- port mapping
  - if $ docker run nginx
    - nginx available only from the bridge network
  - if $ docker -p 8080:80 run
    - nginx available from the host
    $ curl http://192.168.1.10:8080   # welcome to nginx!

- port mapping is done via iptables
  $ iptables \
      - t nat \
      -A PREROUTING \
      -j DNAT \
      --dport 8080 \
      --to-destination 80
  - docker does this the same way + includes ip
  $ iptables \
      - t nat \
      -A DOCKER \
      -j DNAT \
      --dport 8080 \
      --to-destination 172.17.0.3:80

$ iptables -nvL -t nat
  - to see the rules which docker creates


9.6 container networking interface (CNI)
---
- we've seen how network namespaces work
  1] create network namespace
  2] create bridge network/interface
  3] create veth pairs (pipe, virtual cable)
  4] attach veth to namespace
  5] attach other veth to bridge
  6] assign ip addresses
  7] bring the interfaces up
  8] enable NAT - IP masquerade

- this networking stuff is solved similarly from diff vendors (CRI)
  - docker
  - rkt
  - mesos
  - kubernetes

-> they use bridge program
  $ bridge add 2e34dcf34 /var/run/netns/2e34dcf34
  # bridge add <cid> <namespace>

- CNI 
  - = container network interface
  - set of standards for networking challenges in cri (container runtime environments)
  - programs (like bridge) are plugins for cni
  - defines responsibilities of cri's and plugins
  - comes with a set of supported plugins
    - bridge, vlan, ipvlan, macvlan, windows
    - IPAN plugins: dhcp, host-local
    - other 3-rd partie: flannel, weave, cilicum
    - docker is not on the list!

- docker implements CNM (container network model)
  - cnm is another standard

- for docker there is a workaround needed
  $ docker run --network=none nginx
  $ bridgeadd 2e34dcf34 /var/run/netns/2e34dcf34

- k8s solves this the same way when running docker containers



9.7 cluster networking
---
- check the required ports in k8s documentation

- important commands
$ ip link
$ ip addr
$ ip addr add 192.168.1.10/24 dev eth0
$ ip route
$ ip route add 192.168.1.0/24 via 192.168.2.1
# ./proc/sys/net/ipv4/ip_forward    # ubuntu
$ cat /etc/sys/net/ipv4/ip_forward
  1
$ etc/nw/interfaces to persist
$ arp
$ netstat -plnt
$ route



9.8 explore networking solutions
---

- check interface, MAC, IP
  $ ifconfig
  $ ssh node01 ifconfig
  $ cat /etc/network/interfaces
  $ ip link

- check links, state of interface
  $ ip link

- which route takes ping google.com
  $ ip route
  $ ip r
    - check the default gw
  $ trecaroute www.google.com

- on which port does scheduler listen?
  $ netstat -natulp | grep kube-sche
    - 10259
  - or check the configuration of the static pod kube-scheduler

- on which port does etcd listen for the client connections?
  $ netstat -natulp | grep etcd | grep LISTEN
    - 2379  # used the most


9.9 pod networking
---
- here the networking part is done manually
- this is usually done by networking solutions for k8s, like
  - Flannel
  - ...



9.10 CNI in kubernetes
---
-CNI plugin is configured in the kubelet-service

kubelet.ervice
---
  --network-plugin=cni  \\
  --cni-bin-dir=/opt/cni/bin  \\
  --cni-conf-dir=/etc/cni/net.d \\

$ ps aux | grep kubelet
$ ls /opt/cni/bin
  - all plugins: flanne, bridge, ...
$ ls /etc/cni/net.d
  - config files 
  - e.g. bridge is used



9.11 CNI weave (weaveworks)
---
- weaveworks plugin
- installs agents on each node
- it deploys it'w own bridge network
- encapsulates k8s nw packages, does the networking part and delivers packages from node to node

- deploy weave
  - either service/deamon on k8s (manually)
  - or as a pod in k8s
    $ kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
    - this will deploy weave as DeamonSet
      - ensures one pod (weave peer) will be deployed on each node
      $ kubectl get pods -n kube-system

- to check logs
  $ kubectl logs weave-net-56cmb weave -n kube-system



9.12 explore CIN weave (solution)
---
$ ps aux | grep kubelet | grep cni
$ ls /opt/cni/bin



9.13 installing a networking addon
---
- Installing a Pod network add-on
  - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network

- list of networking addons supported by Kubernetes
  - https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy

- choices?
  - cancal / calico / flannel
  - romana



9.14 IP Address Management (weave)
---
- this doesn't concern the nodes' IP
  - IPAN solutions or on the own
  
- this concerns how 
  - how are vBridge nws assigned IP subets
  - IPs to pods
  - no duplicate ip

- CNA says
  - it's our responsibility to assign and manage IP

- we could manage and IP-list.txt
  - on every node same
  - providing and updating the list by ourself

- CNPI provides 2 buildint plugins for managing such IP-list
  - DHCP
  - host-local

- host local plugin
--
- we have to invoke getting ip's ourselves
  $ ip = get_free_ip_from_host_local()

- this plugin setup is done in net-script.conf
- cat /etc/cni/net.d/net-script.conf :
  {
    "cniVersion": "0.2.0",
    ...
    ...
    "ipam": {
      "type": "host-locl",
      "subnet": "10.244.0.0/16",
      "routes": [
        { "dst": "0.0.0.0/0" }
      ]
    }
  }

- weaveworks plugin
---
- allocates for k8s ip pool 
  - 10.32.0.0/12
    - 10.32.0.1 > 10.47.255.254
      ~ 1 million IP's  (1,048,574)
  - nodes divide this IP pool equaliy
    - node 1: 10.32.0.0
    - node 1: 10.38.0.0
    - node 1: 10.44.0.0
    - this ranges are configurable when deploying the cluster



9.15 networking weave solution
---
- what is the networking solution used by k8s cluster?
  $ ls /etc/cni/net.d/
    10-weave.conflist


- how many agents/peers deployed in the cluster?
  - they should be deamonSets, but
  $ kubectl get pods -n kube-system | grep weave

- killercoda:
  $ k get pods -n kube-system | grep canal
  $ k get pods -n kube-system | grep coredns
    - ?

- identify the name of bridge network/interface on each node
  $ ifconfig

- ip range reserved by weave?
  $ ip addr show weave


- what is defaut gateway on node X?
  - the solution
    $ kubectl run busybox --image=busybox --command sleep 1000 --dry-run=client -o yaml > busybox.yaml
      - edit yaml, add nodeName
        spec:
          nodeName: node03
          containers:
          ...
    $ kubectl exec -ti busybox -- sh
      $ ip route  / ip r    # inside of the container

  - my suggestion was WRONG !!!
    $ ssh node01 route    # read the default gw
      - WRONG !!!


9.16 service networking
---
- unlike pod networking mentioned in previous videos
- rarely pod communicate with each other directly
- a pod is hosted on a pod


- pod communication runs through services!
- service kinds:
  - clusterip      Create a ClusterIP service
  - externalname   Create an ExternalName service
  - loadbalancer   Create a LoadBalancer service
  - nodeport       Create a NodePort service

- cluster ip
  - if i want to expose pod to other pods, i would create a service
  - a service is hosted and accessible across the cluster
  - a service is accessible only within the cluster
  - eg. db service

- nodeport
  - accessible outside the cluster
  - works like clusterip
  - exposes port accross all hosts
  - eg. web application

- service is just a virtual object
  - not part of node
  - accross the whole cluster
  - kube-proxy creates service forwarding rules on each node of the cluster
    - the same rule on each node
      - IP : Port   :: 10.99.13.179
      - forward to  :: 10.244.1.2

- how kube-proxy creates rules? differently:
  - --proxy-mode (rule types)
      - iptables  (default)
      - userspace
      - ipvs
  - this can be changed by
    $ kube-proxy --proxy-mode [userspace | iptables | ipvs ] ...

- ip addresses of pods and services must not overlap!

- how to view them?
$ k get pods
$ kubectl get svc
  - read the IP
- the service IP range is specified in
  $ kube-api-server --service-cluster-ip-range ipNet 
    # default: 10.0.0.0/24
  - check your cluster
    $ ps aux | grep kube-api-server
      #  --service-cluster-ip-range
  - in the provided example
    - the service range is
      - 10.96.0.0/12
        10.96.0.0 > 10.11.255.255.255
    - pod nw range was given before:
      10.244.0.0 > 10.244.255.255

- kube-proxy creates the nat rules in iptables
  $ iptables -L -t nat | grep db-service  # search the rule for a specific service
  - this is done for nodeport and clusterIP service types

- this rules could be seen in kube-proxy node too
  $ cat /var/log/kube-proxy.log
    # Using iptables Proxier
    # Adding new service "default//db-service:3306" at 10.103.132.104:3306/TCP



9.16 service networking solution
---
- what is the IP range for nodes?
  $ kubectl get nodes -o wide
  $ ip a
    2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1460 qdisc fq_codel state UP group default qlen 1000
      link/ether f2:05:f6:3f:86:80 brd ff:ff:ff:ff:ff:ff
      inet 172.30.1.2/24 brd 172.30.1.255 scope global dynamic enp1s0
        valid_lft 86311006sec preferred_lft 86311006sec
    the answer: inet 172.30.1.2/24

- what is the IP range for pods?
  - check the configuration of the NW plugin pod (weave)
  $ kubectl -n kube-system get pods
  $ kubectl -n kube-system logs weave-net-h2x9w -c weave
    # ipalloc-range: 10.32.0.0/12

- service IP range?
  - grep service-cluster-ip-range /etc/kubernetes/manifests/kube-apiserver.yaml
    - --service-cluster-ip-range=10.96.0.0/12

- what type of proxy is kube-proxy configured to use?

- what proxy confugration is kube-proxy?
  - exinstent are:
    - iptables  (default)
    - userspace
    - ipvs
  - check the logs
    $ k get pods -n kube-system
      # "Using iptables Proxier"

- how does this k8s cluster ensure kube-proxy runs on all nodes in cluster?
  $ kubectl -n kube-system get ds   #daemonsets
  - or
  $ kubectl -n kube-system describe pods kube-proxy-2xlvb | less
    # Controlled By:  DaemonSet/kube-proxy


9.18 DNS in k8s (cluster DNS)
---
- prereq
  - what's dns
  - tools: host/ns lookup, dig
  - recorded types: A, CNAME
  - domain name hierarchy

- how to setup own dns server (next lecture)
  - coreDNS

- objectives:
  - what names are assigned to what objects
  - service dns records
  - pod dns records

- nodenames and ip's are registered in organization of the company
  - e.g.
    192.168.1.11     node1.kubeclustr.org
    192.168.1.12     node2.kubeclustr.org
    192.168.1.13     node3.kubeclustr.org
  -  this not a scope of the lessons!!

- k8s has own dns per default
  - if manual setup, then you do it by yourself

- k8s' dns doesn't care about nodes
  - pods and services are relevant!

- e.g. service dns papping
  web-service   10.107.37.188
  - within cluster and within namespace, only the service name is sufficient
    $ curl http://web-service

- organization of k8s dns
  - serviceName < namespace < type  < root
  - web-service < apps      < svc   < cluster.local
  - web-service.apps.svc.cluster.local

- the apps namespace is a namespace for
  - pods and
  - services

- records for services are created per default
- records for pods are NOT created per default
  - can be enabled
  - the podName = ip of the pod (with dash instead of dot, eg 10-244-2-5)
  - dns looks like
    -  podName < namespace  < type  < root
    - 10-244-2-5 < apps     < pod   < cluster.local
    - 10-244-2-5.apps.pod.cluster.local

- fqdn in the default namespace is
  - 10-244-1-5.default.pod.cluster.local
    - namespace = default



9.19 core DNS
---
- how k8s implements dns in the cluster

- one possibility to
 1] add entries of all pods to each /etc/hosts on each pod
  web 10.244.x.x
  - this not practical if many pods

- easier is to 
  2] move entries do DNS server
    web 10.244.x.X
    db  10.244.x.x
  - and than to add DNS server to /etc/resolv.conf of each pod
    nameserver  10.96.0.10

- k8s does similar to 2]
  - but instead of writing to pod, it uses services for resolving
  pods -> dashes

- kube DNS was used in versions < 1.12
- core DNS used in versions >= 1.12

- coreDNS
  - deploys 2 pods (because redundancy)
  - actually it is a replicaset within deployment

- ./coredns
  - /etc/coredens/Corefile
    - plugins confugured
      - health, metrics
      - plugin for kubernetes is "kubernetes culster.local in-addr.arpa {
          ...
        }
  - this file is provided as a configmap object
    $ kubectl get configmap -n kube-system
      # coredns

- when coredns deployed, a service is created
  - kube-dns
  - ip of this service is configured in resolv.conf on pods
    nameserver  10.96.0.10
    - this is done by kubelet
    - kubelet.conf containd coredns' ip & domain name

- to resolv service:
  $ host web-service
    # web-service.default.svc.cluster.local has address 10.x.x.X
    - but how fqdn?
    - resolv.conf has search defined
      search default.svc.cluster.local svc.cluster.local cluster.local
      # so you can use eiteher only service name till full FQDN
  - but!
    - pods can not be reached tonly using pod name
      $ host 10-255-2-5
        # Host x x x x not found: 3(NXDOMAIN)
      $ host 10-255-2-5.default.pod.cluster.local
        # 10-255-2-5.default.pod.cluster.local has address 10.255.2.5



9.19 core DNS solutions
---
$ k get deployments -n kube-system
  # gives coredns

$ k get svc -n kube-system 
  # name: kube-dns
  # ip: 10.96.0.10

- where is the configuration file for coredns
  $ for i in `find . -name Corefile` ; do echo $i ; more $i; done
  - gives
    - ./var/lib/kubelet/pods/0bf92ae2-11c6-4b5b-86c0-2c1ef0d2a2a8/volumes/kubernetes.io~configmap/config-volume/..2023_11_14_11_00_32.2324854149/Corefile

  - video says: "/etc/coredns/Corefile"
  $ k describe deployments -n kube-system coredns
      Args:
          -conf
          /etc/coredns/Corefile
      ...
      Mounts:
        /etc/coredns from config-volume (ro)
    Volumes:
    config-volume:
      Type:               ConfigMap (a volume populated by a ConfigMap)
      Name:               coredns

- how it is passed?
  - up it could be seen "Type: ConfigMap"
    - object

- what is a root domain configured for this k8s cluster?
  either
    1] explore "coredns" configmap
     $ k -n kube-system describe configmaps coredns
      # kubernetes cluster.local in-addr.arpa ip6.arpa { ...
      -> "cluster.local"

    2]create pod and check /etc/resolv.conf
      $ run busy --image=busybox --command sleep 1000 --dry-run=client -o yaml > busy.yaml
      $ k apply busy.yaml
      $ exec -it busy -- bash
      $/ cat /etc/resolv.conf
        # search default.svc.cluster.local svc.cluster.local cluster.local
        # nameserver 10.96.0.10
        -> "cluster.local"


- how to access services?
  $ k get svc
  $ curl http://svc-name


$ k describe svc web-service
  # Selector: name=hr 
$ k get pod hr --show-labels

$ k exec hr -- nslookup mysql.payroll > root/nslookup.out



9.20 ingress
---
- a common question:
  - the diff svc vs ingress?

- ingress 
  - takes over the load balancer configuration part
  - ssl included
  - layer 7 load balancer built-in into k8s

- ingres consist of
  - ingress controller
  - ingress resources

- ingress controller
  - not part of k8s per default


- ingress controller options
  - GCE (google)  - currently supported bz k8s
  - nginx   - currently supported bz k8s
  - Contour
  - HAProxy
  - traefik
  - istio




9.23  ingress networking 2 solution
---
todo:
~8.20

practice creating ingress
add resources (rules)



10.01 design a k8s cluster
---
- check the docu about the number of nodes vs "power" needed



10.2 choosing k8s infrastructure
---
- tunkey vs hosting solutions
  - turnkey
    - openshift
    - vmware pks 
    - vagrant
    ...
  - hosted solutions
    - GKE (google container angine)
    - openshif online
    - azure k8s service
    - EKS (amazon elastic container service for k8s)

- we will do virtualbox
  - 1 master
  - 2 worker nodes
  - this design will change in the next video :)



10.3 HA in k8s
---
- HA means 2 master nodes needed

- load balancer
  - needed n front of master nodes
    - https://load-balancer:6443 splits between (nginx, haproxy, ..)
      - https://master1:6443
      - https://master2:6443

- controller manager & scheduler
  - only one instances must be allowed
  - active vs standby
  - achived per leader-election process

- leader-election process
  - $ kube-controller-manager --leader-elect true [options]
                            --leader-elect-lease-duration 15s
                            --leader-elect-renew-deadline 10s
                            --leader-elect-retry-period 2s
  - controller manager and scheduler follow the same process

- etcd
  - two tactics
    - stacked topology = etcd is a part of master nodes
      - pro: easy setup, easy to manage, fewer nodes
      - con: risk during failures
    - external etcd topology
      - each etcd has it's own node
        - pro: less risky
        - con: harder to setup, more nodes
  - api-server is the only component talking to etcd
    $ cat /etc/systemd/system/kube-apiserver.service
      -- etcd-servers=https://10.240.0.10:2379,https://10.240.0.11:2379

- our design:
  - 2 master nodes
  - 2 workers
  - 1 node = LoadBalancer (for the API server)
  - using
    - weaveworks (let's see about that, nw plugin changed since k8s 1.13)
    - virtualbox
    - a laptop



10.4. ETCD in HA k8s setup
---
- distributed db (shall be)

- for distributed db
  - read data not a problem
  - write data needs a leader
    - the leader writes
    - the leader sends a copy of the data to other follower nodes

- how leader is selected?
  - per RAFT protocol (random timers)

- WRITE is complete if succeded on majority of nodes in the cluster
  - majority = quorum = N/2 + 1
      - quorum:
          q(1)=1
          q(2)=2
          q(3)=2
          q(4)=3
          q(5)=3
          ...
  - 3 instances are highly recommended for ha cluster
    - the fault tolerance = 1 (failed) node (in that case)

- fault tolerance = instances - quorum
    instances;  quorum; fault tolerance
    1           1       0
    2           2       0
    3           2       1
    4           3       1
    5           3       2
    6           4       2
    7           4       3

- odd number of nodes is preffered/suggested
  - in a case e.g. network segmentation, the cluster would continue to work properly
    - nw segmentation, mening the cluster is split to close a half number of Nodes
      (5 -> 3 + 2, ...)
  - more than 5 nodes are not really neccessary

- download, install and configure etcd service
  # wget -q --http-only \
    "https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz"
  
  # tar -xvf etcd-v3.3.9-linux-amd64.tar.gz

  # mv etcd-v3.3.9-linux-amd/etcd* /usr/local/bin

  # mkdir -p /etc/etcd  /var/lib/etcd

  # cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/
    - certification lessons
    - tls certificate section

  - configure
    --initial-cluster-peer ... ~10.25

  $ export ETCDCTL_API=3
  $ etcd put name john
  $ etcd get name
  $ etcd get / --prefix --keys-only # all keys


- it possible to have 3 master nodes!
- stacked topology ok

- we will use 2 master nodes for the lesson purposes
  


11.1 deployment with kubeadm
---
- steps
  - must have multiple systems
  - designate master and worker nodes
  - install docker on all nodes
  - kubeadm on all nodes
  - initialize the master server (all components are installed & configured)
  - set the pod network
  - join worker nodes on the master node



11.2 provision vms with vagrant
---
- prereq is installed:
  - virtualbox
  - vagrant

- vagrant
  - https://github.com/kodekloudhub/certified-kubernetes-administrator-course/tree/master/kubeadm-clusters/virtualbox
  - clone the repo
  - cd certified-kubernetes-administrator-course/kubeadm-clusters/virtualbox 
  - vi Vagrant      # check number of nodes, IPs etc.
    NUM_MASTER_NODE
    ..
    IP_NW = "192.168.56."

$ vagrant status
$ vagrant up
  # this will provision 3 VMs
$ vagrant status
  kubemaster                running (virtualbox)
  kubenode01                running (virtualbox)
  kubenode02                running (virtualbox)

$ vagrant ssh kubemaster
  $ ping kubenode01
$ vagrant ssh kubenode01
  $ ping kubenode02
$ vagrant ssh kubenode02



11.3 demo deployemnt with kubeadm
---
- few remarks
  - cgroup driver configuration is not applicable when using docker
  - initialize the control plane node needed only for HA cluster

- hands-on @ the local computer
---
- check k8s docu
  - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/
- this link i also checked:
  https://www.cherryservers.com/blog/install-kubernetes-on-ubuntu

- let's go!
---
  - devops/kodecloud/kubeadm-clusters/virtualbox$ 
      $ vagrant ssh kubemaster
      $ vagrant ssh kubenode01
      $ vagrant ssh kubenode02
    - todo:
      - learn vagrant / check vagrant config

  - on all nodes:
  1.  forwarding-ipv4-and-letting-iptables-see-bridged-traffic
    https://kubernetes.io/docs/setup/production-environment/container-runtimes/#forwarding-ipv4-and-letting-iptables-see-bridged-traffic

  2. install docker cri (container runtime)
    # Install using the apt repository
      https://kubernetes.io/docs/setup/production-environment/container-runtimes/#docker
      https://docs.docker.com/engine/install/ubuntu/
    # CRI ( https://github.com/Mirantis/cri-dockerd )
    - as a root!
      mkdir mirantis; cd mirantis
      wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.8/cri-dockerd_0.3.8.3-0.ubuntu-jammy_amd64.deb
      dpkg -i cri-dockerd_0.3.8.3-0.ubuntu-jammy_amd64.deb
      cd ..
      # cleanup not done!
    - THIS PART NOT CLEAR
      Path to Unix domain socket  :: unix:///var/run/cri-dockerd.sock
      - pass as the kubeadm init param?
        --cri-socket=unix:///var/run/cri-dockerd.sock

  3. default containerd config file
    https://www.cherryservers.com/blog/install-kubernetes-on-ubuntu
    $ sudo sh -c "containerd config default > /etc/containerd/config.toml"

  4. set "SystemdCgroup" to true
    $ sudo sed -i 's/ SystemdCgroup = false/ SystemdCgroup = true/' /etc/containerd/config.toml
        - This is important because Kubernetes requires all its components,
          and the container runtime uses systemd for cgroups.

  5.restart containerd and kubelet services to apply the changes you made on all nodes
    - on all nodes:
      $ sudo systemctl restart containerd.service

  6. install kubeadm (kubeadm, kubelet, kubectl), and pin the version!
    - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ 
    - copy-paste commands from the following page (Debian Based Distros)
      - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl
    - the question remains if cgroup driver has to be changed here?
      https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/

  7. master node
      $ kubeadm config images pull --cri-socket=unix:///var/run/cri-dockerd.sock

  9. all nodes
      $
        sudo systemctl restart kubelet.service
        sudo systemctl enable kubelet.service

  10. install net-tools and check the ip
    $ apt install net-tools
    $ ifconfig
      # gives
        enp0s8: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.56.11  netmask 255.255.255.0  broadcast 192.168.56.255

  11. master node
      $
        kubeadm init \
            --pod-network-cidr 10.244.0.0/16 \
            --apiserver-advertise-address=192.168.56.11 \
            --cri-socket=unix:///var/run/cri-dockerd.sock
        
        - THE IP WAS WRONG (192.168.56.11)
      ---
        Your Kubernetes control-plane has initialized successfully!

        To start using your cluster, you need to run the following as a regular user:

          mkdir -p $HOME/.kube
          sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
          sudo chown $(id -u):$(id -g) $HOME/.kube/config

        Alternatively, if you are the root user, you can run:

          export KUBECONFIG=/etc/kubernetes/admin.conf

        You should now deploy a pod network to the cluster.
        Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
          https://kubernetes.io/docs/concepts/cluster-administration/addons/

        Then you can join any number of worker nodes by running the following on each as root:

        kubeadm join 192.168.56.11:6443 --token sl6110.websmhhhe29b21l8 \
          --discovery-token-ca-cert-hash sha256:a09d034e24106d4afc6463d6bb6a586705ae49a8c609a681416b8b4cfc805350 
        ---

  12. exit root privs 
    $ exit
    - check nodes
      $ kubectl get nodes
        # kubemaster NotReady status

  13. set up the pod network solution
    - https://kubernetes.io/docs/concepts/cluster-administration/networking/
    - https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy
    -
    - this one mentioned in video
      - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network
      -
    - cidr: 10.244.0.0/16
    - weave
      - on the control-plane node or a node that has the kubeconfig credentials:
        $ kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
          # from: https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#install
    - where was the pod nw ip range setup
      - during the "kubeadm init" command

  14.- add the worker nodes to the cluster
    - on both worker nodes
    - use the saved command "kubeadm join ..." command
    $ kubeadm join 192.168.56.11:6443 --token sl6110.websmhhhe29b21l8 \
          --cri-socket=unix:///var/run/cri-dockerd.sock \
          --discovery-token-ca-cert-hash sha256:a09d034e24106d4afc6463d6bb6a586705ae49a8c609a681416b8b4cfc805350 
    
    - without "--cri-socket=unix:///var/run/cri-dockerd.sock" gives:

        Found multiple CRI endpoints on the host. Please define which one 
        do you wish to use by setting the 'criSocket' field in the 
        kubeadm configuration file: 
        unix:///var/run/containerd/containerd.sock, unix:///var/run/cri-dockerd.sock
        To see the stack trace of this error execute with --v=5 or higher

    - kubectl get nodes
      NAME         STATUS   ROLES           AGE     VERSION
      kubemaster   Ready    control-plane   48m     v1.28.4
      kubenode01   Ready    <none>          3m31s   v1.28.4
      kubenode02   Ready    <none>          4m18s   v1.28.4



11.4 install k8s cluster solution
---
- check linux version
  $ cat /etc/os-release

- recreate join kubeadm token
  $ kubeadm token create -h
  $ kubeadm token create --print-join-command

- network plugin can be installed after (commands)
  - kubeadm init
  - kubeadm join
  - !


12.1
---
- end to end cluster tests
  - not part of the exam anymore
  https://www.youtube.com/watch?v=-ovJrIIED88&list=PL2We04F3Y_41jYdadX55fdJplDvgNGENo&index=19

- k8s smoke tests
  https://www.youtube.com/watch?v=eJQ-Yla85rM&list=PL2We04F3Y_41jYdadX55fdJplDvgNGENo&index=19


13.1  troubleshooting
---
- many labs with broken cluster


13.2 application failure
---
- draw a map how app is set up (db, web, ...) / architecture
- check the service status
  curl http://web-app:node-port

- check serivce
  $ k describe service web-service
    - check selectors - if they match the pod

- check pod
  $ k get pods  # any restarts?
  $ k describe pod web
  $ k logs web
  $ k logs web -f
  $ k logs web -f --previous

- check db service
- check db pod
  - same as above

- k8s docu
  - troubleshoot applications



13.3 app failure solution
---
- check service endpoints
  $ kubectl get ep

- todo: check expose
  $ k expose pod mysql --name=mysql-service



13.4 control plane failure
---
- check k8s docs: troubleshooting control plane

- k get nodes
- k get pods

- k get pods -n kube-system                 # deployed per kubeadm
- service kube-apiserver status             # deployed as a service
- service kube-controller-manager status    # deployed as a service
- service kube-scheduler status             # deployed as a service
- service kubelet status             # deployed on the worker node as a service
- service kube-proxy status          # ddeployed on the worker node as a service

- k logs kube-apiserver-master -n kube-system   #check the logs
- sudo journalctl -n kube-apiserver   # native installation = native log checks



13.5 control plane failure solution
---
- pod in status pending
  - scheduler is responsible

- k get pods -n kube-system
- k describe kube-scheduler-master
  - static pod ("-master" in the name)

- check the hand-crafted service config in the kubelet service
    - cat /etc/systemd/system/kubelet.service.d/10-kubead.conf

- scale deployment
  - didn't work
  - controller-manager is responsible for deployemts and replicaSets

- check logs
  - kubectl -n kube-system logs kube-controller-manager-master

- ca file err in controller-manager
 - hostPath was wrong (volume mount)



13.5 worker node failure
---
- k8s docs: .../docs/tasks/debug-application-cluster/debug-cluster
- k get nodes
- k describe node worker01
  Conditions:
    OutOfDisk
    MemoryPressure
    DiskPressure
    PIDPressure
    Ready
  
  - if comm with master doesn't work -> COnditions: Unknown

- top
- df -h

- service kubelet status
- sudo journalctl -u kubelet

- check certificates
  - openssl x509 -in /var/lib/kubelet/worker01.crt -text
    - issuer, etc.



13.5 worker node failure solution
---
- ps aux | grep kubelet
- systemctl status kubelet
- systemctl status kubelet -l
- systemctl restart kubelet
- systemctl status kubelet -l
- k describe node node01
- journalctl -u kubelet
  - SHIFT+g-> goes to the bottom of the log

- connection refused kubectl -> apiserver

- kubelet service definition file location
  / etc/systemd/system/kubelet.service.d/10-kubeadm.conf
  - inside there is
  --kubeconfig=/etc/kubernetes/kubelet.conf

- in kubelet.conf are the api-server details stored

- reload and restart after changing the config
  $ systemctl daemon-reload
  $ systemctl restart kubelet
  $ systemctl status kubelet.service -l

- kubelet service definition file location
  / etc/systemd/system/kubelet.service.d/10-kubeadm.conf
  - CA config is located at
    --config=/var/lib/kubelet/config.yaml
  


14.1 advanced kubectl commands
---
- JSON path in kubectl
- learn basics
-  json path for beginners (yt)
    - www.kodekloud.com/p/json-path-quiz

- json path query
  - .items[0].spec.containers[0].image
  - $.items[0]. .. -> $ not mandatory, kubectl adds it

  $ kubectl get nodes -o=jsonpath='{ s.items[0].spec.containers[0].image }'

- jsonpath.com
  - validate and play around

- jsonpath examples
$ k get nodes -o=jsonpath='{.items[*].metadata.name}'
$ k get nodes -o=jsonpath='{.items[*].status.nodeInfo.architecture}'
$ k get nodes -o=jsonpath='{.items[*].status.capacity.cpu}'

- combining queries is possible
$ k get nodes -o=jsonpath='{.items[*].metadata.name} {.items[*].status.capacity.cpu}'
$ k get nodes -o=jsonpath='{.items[*].metadata.name} {"\n"} {.items[*].status.capacity.cpu}'


- formatters
  - {"\n"} = new line
  - {"\t"} = tab

- loops - range
  - .range = is like forEach loop

- analogy:
forEach Node
  print "nodeName \t cpuCount \n"
end

- the upper part in jsonpath:
'{range .items[*]}
  {.metadata.name}{"\t"}{.status.capacity.cpu}{"\n"}
 {end}'

- range loops
  - put this in oneliner
    - '{range .items[*]}{.metadata.name}{"\t"}{.status.capacity.cpu}{"\n"}{end}'
    $ k get nodes -o=jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.capacity.cpu}{"\n"}{end}'

- custom columns
  - k get nodes -o=custom-columns=<COLUMN NAME>:<JSON PATH>
  - .items[*] is assumed
  - examples
    $ k get nodes -o=custom-columns=NODE:.metadata.name
    $ k get nodes -o=custom-columns=NODE:.metadata.name,CPU:.status.capacity.cpu

- sort 
  - k get nodes --sort-by=
  $ k get nodes --sort-by=.metadata.name
  $ k get nodes --sort-by=.status.capacity.cpu
  $ k get nodes -o=custom-columns=NODE:.metadata.name,CPU:.status.capacity.cpu --sort-by=.status.capacity.cpu



16.1 mock exam 1
---
- exposing pod as a service is possible!!!
  $ k expose pod messaging --name messaging-service --port=6379 --target-port=6379
      # service/messaging-service exposed

- where static pods are created
  $ grep -i staticPod /var/lib/kubelet/config.yaml
    # /etc/kubernetes/manifests

- nodeport service doesn't have a switch to define (node)port
  $ k expose deployment hr-web-app \
    --name hr-web-app-service \
    --port 8080 --target-port 8080 --type='NodePort' \
    --dry-run=client -o yaml > hr-web-app-service.yaml

  - now edit the yaml and under the targetPort(8080) add 
    nodePort: 30082

- kubectl cheat sheet (jsonpath)
  - https://kubernetes.io/docs/reference/kubectl/cheatsheet/

- explain object
  $ kubectl explain pv --recursive | less
  # /hostpath
  ...
      apiVersion: v1
      kind: PersistentVolume
      metadata:
        name: pv-analytics
      spec:
        capacity:
          storage: 100Mi
        accessModes:
        - ReadWriteMany
        hostPath:
          path: /pv/data-analytics
  ...


16.2 mock exam 2
---

- take the etcd backup
--
  - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#snapshot-using-etcdctl-options

  - check the version of ETCD
    $ ETCDCTL_API=3 etcdctl version

  - chapeer 6.8
    - or, use the builtin snapshot solution
      - ETCDCTL_API=3 etcdctl \
          --CA-PARAMETERS \
          snapshot save snapshot.db

  - params needed; e.g. params and values:
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt

  $ kubectl describe -n kube-system pod etcd-kubemaster | more
  $ kubectl describe -n kube-system pod etcd-kubemaster | grep \
      -e --cert-file \
      -e --key-file \
      -e --trusted-ca-file

  $ sudo ETCDCTL_API=3 etcdctl \
    --cert="/etc/kubernetes/pki/etcd/server.crt" \
    --key="/etc/kubernetes/pki/etcd/server.key" \
    --cacert="/etc/kubernetes/pki/etcd/ca.crt" \
    snapshot save snapshot.db
    
  - enpoint option not needed -> this is run on the same server where etcd is installed
  
  - snapshot status 
    $ ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshot.db


- create a pod, assign 1 cpu & 200 MiB
--
  - https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/
  - elephant.yaml
  --
  apiVersion: v1
  kind: Pod
  metadata:
    labels:
      run: elephant
    name: elephant
  spec:
    containers:
    - image: redis
      name: elephant
      resources:
        requests:
          cpu: "0.5"
          memory: "200Mi"
    restartPolicy: Always


- create a pod which has capabilities to set system_time
--
  - Set capabilities for a Container
    - https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container

  - super-user-pod.yaml
  ---
    apiVersion: v1
    kind: Pod
    metadata:
      labels:
        run: super-user-pod
      name: super-user-pod
    spec:
      containers:
      - command: ["sleep","4800"]
        image: busybox:1.28
        name: super-user-pod
        image: gcr.io/google-samples/node-hello:1.0
        securityContext:
          capabilities:
            add: ["SYS_TIME"]
      dnsPolicy: ClusterFirst
      restartPolicy: Always
    status: {}


- create a pod which claims a pvc
--
- https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/


- create deployment + record the version + rolling update of nginx version
--
- https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment

  $ kubectl create deployment nginx-deploy --image=nginx:1.16 --replicas=2 --record
  $ kubectl rollout history deployment
  $ kubectl set image deployment/nginx-deploy nginx=nginx:1.16.1 --record
      # deployment.apps/nginx-deploy image updated
  $ kubectl rollout history deployment nginx-deploy
  $ kubectl set image deployment/nginx-deploy nginx=nginx:1.17 --record
    # Flag --record has been deprecated, --record will be removed in the future
    # deployment.apps/nginx-deploy image updated
  $ kubectl rollout history deployment nginx-deploy
  $ k rollout undo deployment nginx-deploy --to-revision=1

- Flag --record has been deprecated, --record will be removed in the future
  - use annotate
    - https://jamesdefabia.github.io/docs/user-guide/kubectl/kubectl_annotate/
    - https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/

  - the workflow:
    - create the deployment
      $ kubectl create deployment nginx-deploy --image=nginx:1.16.0 --replicas 1

    - check the history
      $ kubectl rollout history deployment nginx-deploy

    - update the image on deployment
      $ kubectl set image deployment nginx-deploy nginx=nginx:latest

    - annotate the deployment now and create the history
      $ kubectl annotate deployment nginx-deploy kubernetes.io/change-cause="version change to 1.16.0 to latest" --overwrite=true

    - check the history
      $ kubectl rollout history deployment nginx

  - check: annotate vs record!
    - "record" has been deprecated


- create a new user + grant him access to the cluster/add role
---
  - useful link:
  - https://discuss.kubernetes.io/t/how-to-create-user-in-kubernetes-cluster-and-give-it-access/9101/4
  
  - a Q:
    - are kubernetes.io discussions available during the exam?
    - or only strictly the k8s documentation?

  - the solution:
    - link
      - https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user
    
    - steps:
      - create the user Certificate Signing Request (CSR)
      - approve the CSR
      - create the role
      - create the role binding
      - set-credentials

  - create the user Certificate Signing Request (CSR)
    $ sudo su
    $ openssl genrsa -out developer-john.key 2048
    $ openssl req -new -key developer-john.key -out developer-john.csr -subj "/CN=john"
    $ cat developer-john.csr | base64 | tr -d "\n"
      # LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0diWGwxYzJWeU1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQTFaT29nRE5sVEVHYVBPS2tmeGJWU09QaXNYSzZYazFqMnNiSUQ5d3YzTWZ0CjFLa011NGtMM1h1S2o0bFd1c3FmY0xtVmdLYkUvdjFBU09oaWFXeXQ4T3lHUks0MkIyK1VQT0dGdkZzdFBhSEUKOVRKSU1YK1ZlM3JJMGVtb0J1c2tpd24zSmlzaCtZV0pGdzVFMlRMcVpPNmJ5SWJoMjZGbU5IRVVGNU1CdUlYNwphYi93ckZ5VjdudWowTXlIa21TeXBjTWxQSy81SFpHVnVyN0RacGY4NmRJMGk0akNaaXpSTHoyQmVpYXVkOFRZCklCa1BzbGFFa2hLNnVCT25vNUx0ZmN4Ty9HQTF4T2Zpd0hYc0ZvdTJjQzQ0SGZhS3Znc0V2WFVzQThQenRHVDEKbUpmZTdzWDVtcXF1eDBhR3lDelhpNCs4cS8vcitRVk1vb0JiajhOTEp3SURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBRHpZRUt3bEdHaTF1M3VGR3FSWkZnNzBiTlUyNXR4TFphRVpDeXhsV2RhNTZTL0lsbUlZCkhKclFuNnhhTldGUGFuRUd0bHBEY09OSjlHUS9TOEx3WWJkRU1ickl2OUVsSXo1QVdBRWFwaHpYV0JYdW5aRXcKbFdDcm5YQy93MXBuQkRJQ0xYdDBvdnVaR2NhbEltQ1pVMHQ5bm10K2Rpc3BZUDB1dXpxNTBRclVyV2hLc2N3Vgp3WTg1YXgyM1hLRnJiMnFCT0VoTVYyNEJNNmQ2Q0NUUmdnd0JnakdQaFFZOTlwLzYzSS93OE10b1JvYU02VzNZClgvakJaSWgyUkFsaG0rbllsUFIwUmJSeGNSNVVEVytJd1l5Q25SNk85NWVaNitiVVZaVlU4bjVLRFhqWXNFZUYKZks3R2F1cUtJdFVNTXB5L2hmOWZXaXVLdnI2a2tpajBKRms9Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=

    $ kubectl api-versions | grep certif
        # certificates.k8s.io/v1
    $
      cat <<EOF | kubectl apply -f -
      apiVersion: certificates.k8s.io/v1
      kind: CertificateSigningRequest
      metadata:
        name: john
      spec:
        request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0diWGwxYzJWeU1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQTFaT29nRE5sVEVHYVBPS2tmeGJWU09QaXNYSzZYazFqMnNiSUQ5d3YzTWZ0CjFLa011NGtMM1h1S2o0bFd1c3FmY0xtVmdLYkUvdjFBU09oaWFXeXQ4T3lHUks0MkIyK1VQT0dGdkZzdFBhSEUKOVRKSU1YK1ZlM3JJMGVtb0J1c2tpd24zSmlzaCtZV0pGdzVFMlRMcVpPNmJ5SWJoMjZGbU5IRVVGNU1CdUlYNwphYi93ckZ5VjdudWowTXlIa21TeXBjTWxQSy81SFpHVnVyN0RacGY4NmRJMGk0akNaaXpSTHoyQmVpYXVkOFRZCklCa1BzbGFFa2hLNnVCT25vNUx0ZmN4Ty9HQTF4T2Zpd0hYc0ZvdTJjQzQ0SGZhS3Znc0V2WFVzQThQenRHVDEKbUpmZTdzWDVtcXF1eDBhR3lDelhpNCs4cS8vcitRVk1vb0JiajhOTEp3SURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBRHpZRUt3bEdHaTF1M3VGR3FSWkZnNzBiTlUyNXR4TFphRVpDeXhsV2RhNTZTL0lsbUlZCkhKclFuNnhhTldGUGFuRUd0bHBEY09OSjlHUS9TOEx3WWJkRU1ickl2OUVsSXo1QVdBRWFwaHpYV0JYdW5aRXcKbFdDcm5YQy93MXBuQkRJQ0xYdDBvdnVaR2NhbEltQ1pVMHQ5bm10K2Rpc3BZUDB1dXpxNTBRclVyV2hLc2N3Vgp3WTg1YXgyM1hLRnJiMnFCT0VoTVYyNEJNNmQ2Q0NUUmdnd0JnakdQaFFZOTlwLzYzSS93OE10b1JvYU02VzNZClgvakJaSWgyUkFsaG0rbllsUFIwUmJSeGNSNVVEVytJd1l5Q25SNk85NWVaNitiVVZaVlU4bjVLRFhqWXNFZUYKZks3R2F1cUtJdFVNTXB5L2hmOWZXaXVLdnI2a2tpajBKRms9Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
        signerName: kubernetes.io/kube-apiserver-client
        expirationSeconds: 86400  # one day
        usages:
        - client auth
      EOF

  - approve the CSR
    $ kubectl get csr
    $ kubectl certificate approve john

    - get the CSR and/or export it
      $ kubectl get csr/john -o yaml
      $ kubectl get csr john -o jsonpath='{.status.certificate}'| base64 -d > john.crt

  - create the role
    - pods: create,list,get,update,delete
    $ kubectl create role developer --verb=create --verb=get --verb=list --verb=update --verb=delete --resource=pods
    $ k describe role developer
  
  - create the role-binding
    $ kubectl create rolebinding developer-binding-john --role=developer --user=john

  - test:
    $ kubectl auth can-i update pods --as=john

  - add to kubeconfig part was not needed?!?
    - this is part of the offciial k8s docu ..
      - add to kubeconfig 
        1] set-credentials ( add new credentials ):
          $ kubectl config set-credentials john --client-key=developer-john.key --client-certificate=john.crt --embed-certs=true

        2] set-context ( add the context ):
          $ kubectl config set-context john --cluster=kubernetes --user=john

    - test
      $ kubectl auth can-i create pods --as john (--namespace default)
      $ kubectl config current-context
        # kubernetes-admin@kubernetes
      $ kubectl config use-context john
        - create/delete pod

  - ERROR (AS A SIDENOTE)
  - "The connection to the server localhost:8080 was refused"
    - when running kubectl as the ROOT, the error was thrown
      # The connection to the server localhost:8080 was refused - did you specify the right host or port?
      -> the solution:
        - AS THE ROOT, RUN:
          mkdir -p $HOME/.kube
          cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
          chown $(id -u):$(id -g) $HOME/.kube/config


- create pod, expose it internally, record results into two files (.svc & .pod)
---
- CD / NoWE (ClusterIp Db / Nodeport WEb app)

- cluster ip
  - eg. db service
  - if i want to expose pod to other pods, i would create a service
  - a service is hosted and accessible across and only within the cluster
 
- nodeport
  - eg. web application
  - accessible outside the cluster
  - works like clusterip
  - exposes port accross all hosts
  - --nodePort imperative switch doesn't exist

- pod: nginx-resolver
- svs: nginx-resolver-service
  
$ k run nginx-resolver --image=nginx
$ k expose pod nginx-resolver --name=nginx-resolver-service --port=80 --target-port=80 (--type=ClusterIp)

- to test nslookup within the cluster, use busybox
- TAKE CARE ABOUT THE BUSYBOX VERSION (not all work correctly)
- use busybox:1.28

$ k run test-nslookup --image=busybox:1.28
- test the service dns
  $ k exec -it test-busybox -- bash
    $ nslookup nginx-resolver-service
  - or
  $ kubectl run --generator=run-pod/v1 test-nslookup --image-busybox:1.28 --rm -it -- nslookup nginx-resolver-service
    - or
    - the winner is
  $ k exec nslookup -- nslookup nginx-resolver-service > /root/nginx.svc
- test the pod dns
  $ k get pod nginx-resolver -o wide  # -> 10.36.0.7
  $ kubectl exec test-nslookup -- nslookup 10-36-0-7.default.pod
  $ kubectl exec test-nslookup -- nslookup 10-36-0-7.default.pod > /root/nginx.pod

- todo:
  - check DNS entries created by k8s
  - ?
    - nginx-resolver-service.default.svc.cluster.local
    - 10-36-0-7.default.pod 
    ??? 10-244-2-5.apps.pod.cluster.local



- create a static pod on node01 & configure kubelet service on node01
---

- check the kubelet config file
  - which is the location for static pods?

  $ ssh kubenode01
  $ ps aux | grep kubelet   # OR: systemctl status kubelet
    -> ... --config=/var/lib/kubelet/config.yaml
  $ grep -i static /var/lib/kubelet/config.yaml
    -> staticPodPath: /etc/kubernetes/manifests

- create the YAML pod definition on node01 in the "staticPodPath" directory (/etc/kubernetes/manifests/)

- it is normal to get the following err when running the command kubectl on the worker node(?)
  - E1216 22:42:03.315706   50651 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused

- a possible solution is to create the definition yaml on the master node, and then to copy it onto the worker node (vi file on worker)



16.3 mock exam 3
---

- task: create a service account and a pod
--
- create a service account: pvviewer
- create a cluster role: pvviewer-role : list all PersistentVolumes
- create the cluster role binding
- cerate a pod pvviewer, redis, svcacc pvviewer
  https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  - https://kubernetes.io/docs/reference/access-authn-authz/authentication/#service-account-tokens


- my solution:
  $ kubectl create serviceaccount pvviewer
  $ kubectl create role pvviewer-role --verb=list --resource=PersistentVolume
  $ kubectl create clusterrolebinding pvviewer-role-binding --serviceaccount=default:pvviewer-role --clusterrole=pvviewer-role
  $ kubectl run pvviewer --image=redis --dry-run=client -o yaml > pvviewer.yaml
    - edit the file and add under 
      spec:
        serviceAccountName: pvviewer

- test:
  $ kubectl describe pod pvviewer
  $ kubectl describe pod pvviewer | grep -i service


- task: list the InternalIP of all nodes of the cluster
--
- save the results to file /root/node_ips
- cheat sheet
  - https://kubernetes.io/docs/reference/kubectl/cheatsheet/
    # get ExternapIPs of all nodes
    # -> kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'
    
$ kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'
$ sudo su
$ kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' > /root/node_ips

- btw. since no ExternalIP type, even this command would work
$ kubectl get nodes -o jsonpath='{.items[*].status.addresses[].address}'


- task: create a multi-pod with two containers and env vars
--
- container 1: name=alpha, imege=nginx
- container 2: name=beta, image=multibox, sleep 4800
- environment var @ container 1=name: alpha
- environment var @ container 2=name: beta


- multi-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  containers:
  - image: nginx
    name: alpha
    env:
    - name: name
      value: alpha
  - image: busybox
    name: beta
    env:
     - name: name
       value: beta
    command: ["sleep","4800"]


- task: create a pod with cpu and memory limits
--
- name: lion 
- image: redis:alpine
- resource limits:
    cpu: "2"
    memory: "500MiB"

- lion.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: lion
spec:
  containers:
  - image: redis:alpine
    name: redis
    resources:
      limits:
        cpu: "2"
        memory: "500Mi"


- task: fix network for pod, fix network policy
--
- https://kubernetes.io/docs/concepts/services-networking/network-policies/
- https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/

- todo: network policies
  - improve

- video
  $ k exec test-nslookup -it -- sh
  $ nc -z -v -w 2 np-test-service 80
  $ nc -zv np-test-service 80
  $ nc -zvw 2 10.100.184.235 80
    # nc: 10.100.184.235 (10.100.184.235:80): Connection timed out

$ k get netpol

- all pods were selected
- ingress policy type defined

- spec:
  podSelector:
    matchLabels:
      run: np-test-1

- ingress rule allowing traffic to np-test-1:80
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-np-traffic
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: np-test-1
  policyTypes:
    - Ingress
  ingress:
  - ports:
    - protocol: TCP
      port: 80


- task: taint the node to be unschedulable, then tolerate for one pod
---
- taint node01
- dev-redis:alpine @ node-1 (not scheduled)
- prod-redis:alpine @ node-1 (toleration)

$ kubectl taint nodes kubenode01 nodename=kubenode01:NoSchedule

- add a toleration for the pod
---
spec:
  tolerations:
  - key: "example-key"
    operator: "Exists"
    effect: "NoSchedule"


-prod-redis.yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: prod-redis
  name: prod-redis
spec:
  containers:
  - image: redis:alpine
    name: prod-redis
  nodeSelector:
    kubernetes.io/hostname: kubenode01
  tolerations:
  - key: "nodename"
    operator: "Exists"
    effect: "NoSchedule"




home/splinter/Documents/2_EDU/kubernetes/Udemy - Certified Kubernetes Administrator (CKA) with Practice Tests







