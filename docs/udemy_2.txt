Certification Details

Note!

In the video, I said the exam is 3 hours. With the latest version of the exam, it is now only 2 hours. The contents of this course have been updated with the changes required for the latest version of the exam.


Below are some references:

Certified Kubernetes Administrator: https://www.cncf.io/certification/cka/

Exam Curriculum (Topics): https://github.com/cncf/curriculum

Candidate Handbook: https://www.cncf.io/certification/candidate-handbook

Exam Tips: http://training.linuxfoundation.org/go//Important-Tips-CKA-CKAD


    Head over to this link to enroll in the Certification Exam. Remember to keep the code - 20KODE - handy to get a 20% discount while registering for the CKA exam with Linux Foundation.



---
https://killer.sh/pricing



---
Slack Group URL:
https://kodekloud.com/pages/community



---
We have created a repository with notes, links to documentation and answers to practice questions here. Please make sure to go through these as you progress through the course:

https://github.com/kodekloudhub/certified-kubernetes-administrator-course




2.11 cluster architecture
---

- master node:
    - etcd cluster
    - kube-apiserver
    - kube controller manager
    - kube-scheduler


- worker nodes
    - kubelet
    - kube-proxy
    - container runtime engine (docker/rkt/containerd)


- k8s | CRI (I = interface!)| container runtimes
    - container runtime interface

- dockershirm
    - pseudo interface developed from k8s only for docker



2.12. docker vs containerD
---
- containerd is enough
    - docker doesn't need to be installed

- ctr tool
    - only for debugging

- nerdctl (tool)
    - supports docker compose
    - supports newest features in containerd
        - encrypted images
        - lazy pulling
        - p2p image distribution
        - image signing & verifying
        - namespaces in k8s

- nerdctl commands analogue to docker commands
    - D -> docker run --name redis redis:alpine
    - N -> nerdctl run --name redis redis:alpine

    - D -> docker run --name webserver -p 80:80 -d nginx
    - N -> nerdctl run --name webserver -p 80:80 nginx


- crictl - CLI
    - used to interaction with CRI (container runtime interface)
    - for debugging and inspection
    - not user friendly

- to communicate with containerd you got to do api calls

- before (1.13 e.g.), for troubleshooting docker commands were used
    - now crictl (cry control) tool will be used

- commands
    - crictl
    - crictl pull busybox
    - crictl images
    - crictl ps -a
    - crictl exec -i -t 3e423421dd23424a4324ac42423 ls
    - crictl logs 3e953789dd8421ae943c24
    - crictl pods

- check the difference between docker and crictl commands
    - https://kubernetes.io/docs/reference/tools/map-crictl-dockercli/

- crictl --runtime-endpoint
- export CONTAINER_RUNTIME_ENDPOINT
- unix:///var/run/cri-docker.sock


2.13 etcd for beginners
---
./etcdctl --version

- run the spec version
    $ ETCDCTL_API=3 ./etcdctl version

- or specify for the session
    $ export ETCDCTL_API=3
    $ ./etcdctl version

- we work with version 3

$ export ETCDCTL_API=3
$ ./etcdctl put key1 value1

$ etcdctl get key1



2.14 etcd in k8s
---
- etcd is a cluster datastore for:
    - nodes
    - podes
    - configs
    - secrets
    - accounts
    - roles
    - bindings
    - others

- port: 2379

- list all keys stored in etcd db
    $ kubectl exec etcd-master -n kube-system etcd get / --prefix -keys-on

- https://gist.github.com/lalyos/aef94a4c23973eaee4a17bb26b6972a2


ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt \
    ETCDCTL_CERT=/etc/kubernetes/pki/etcd/peer.crt \
    ETCDCTL_KEY=/etc/kubernetes/pki/etcd/peer.key \
    ETCDCTL_API=3  \
    etcdctl \
      get \
      --keys-only \
      --prefix=true \
      "/registry/namespaces/"


etcdctl get --keys-only --prefix=true "/registry/" | less

- you must also specify path to certificate files 
    so that ETCDCTL can authenticate to the ETCD API Server
        --cacert /etc/kubernetes/pki/etcd/ca.crt     
        --cert /etc/kubernetes/pki/etcd/server.crt     
        --key /etc/kubernetes/pki/etcd/server.key


- k8s components
---
- kube-api-scheduler
    - communication between components
- kube-controller-manager
    - the brain
- kube-scheduler
    - decides which pod on which node
- kubelet
    - connection between nodes and master
- kube-proxy
    - networking


- pod = smallest object of k8s


- accessing labs
---
https://uklabs.kodekloud.com/courses/labs-certified-kubernetes-administrator-with-practice-tests/

https://kodekloud.com/topic/practice-test-replicasets-2/

q:
- how to check the number of containers in a pod?
    $ k get pods -o wide
        # ready column


2.29 replica sets 
---
-  is often used to guarantee the availability of a specified number of identical Pods.
- use cases
    - keeps pods always running
    - loadbalancing and scaling
    - to monitor already existing pods (and re-deploy failed ones if needed)

- replication controller -> older technology
- replicaSet -> same purpose as the replication controller

- main difference between replicaSets and replication controller is the "selector" field 
- selector matches the pod, e.g.
    selector:
        matchLabels:
        type: front-end

- no daemonset/replicaset create command
    - instead "k create deployment" is used

- use "replace" command to update the replicasets
    $ kubectl replace -f replicaset-definition.yaml
        - what about "apply"???

- scale
    $ kubectl scale -replicas=6 -f replicaset-definition

- edit
    $ kubectl edit replicaset ...

commands:
    $ kubectl edit replicaset ...
    $ kubectl scale -replicas=6 -f replicaset-definition
    $ kubectl replace -f replicaset-definition.yaml
    $ kubectl explain replicaset
        # gives the kind & version



2.32 deployments
---
- supports rolling updates
    - different versions of app

- pod < replicaSet < deployment

- yaml similar to replicaSet ("kind=Deployment" is the only difference to rs)



2.36 services
---
- can span across more nodes in the cluster

- nodeport service
    - nodePort : Port : targetPort
    - NoPeWeB = NodePort is used for Web Apps/containers

- clusterip service
    - CLuPi DaBar = clusterIp is used for e.g. database (as a backEnd ip aggregator)

- loadbalancer
    - on the cloud platform that supports LoadBalancer type
    - instead of NodePort

- a difference between "expose" and "create service"
    - https://stackoverflow.com/questions/59397542/kubernetes-create-service-vs-expose-deployment
    - create service
        $ kubectl create service nodeport demo --tcp=8080:80 --node-port=31888
        - can set nodeport, port and target port
    - expose
        $ kubectl expose deployment demo --name=demo --type=NodePort --port=8080 --target-port=80
        - node port must be set afterwards (kubectl edit)
    - create service doesn't have the "set selector"
        $ kubectl set selector service demo myapp=hello
        - service to work with a deployment with pods labeled myapp: hello
            - must be set afterwards

- expose/create differences in short 
    - expose no NodePort (eno nodeport)
    - create service no label (kresxe no label)

- use "expose object"
    - when creating a service from an existing object, a suggestion is to use expose
        $ kubectl expose deployment nginx-deployment --type=NodePort --port=80 --target-port=80
            - if nodePort important, then edit the service (yaml definition file) and set the nodePort
            - nodePort: 30080

- examples of exposing and creating the service 
    - https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/15018998#questions/19114274

    - Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379
        $ kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
        ( This will automatically use the pod's labels as selectors )

    - Or
        $ kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml 
        ( This will not use the pods labels as selectors, instead it will assume selectors as app=redis. 
        You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. 
        So generate the file and modify the selectors before creating the service )

- a trick to expose a service when creating the pod, at the same time, one-liner
    $ kubectl run httpd --image=httpd:alpine --port=80 --expose=true

- there is a open task for setting labels when create service!
    - https://github.com/kubernetes/kubernetes/issues/46191    



2.41 namespaces
---
- permanently set the "dev" namespace
    $ kubectl config set-context $(kubectl config current-context) --namespace=dev



2.42 namespaces - solutions
---
- to access a service DNS accross the cluster use e.g.
    - "db-service.dev.svc.cluster.local"



2.44 imperative vs declerative
---
- commands used for manipulating objects in k8s

- imperative approach example
    - create yaml object (yaml file)
    - edit the yaml file, when needed
    - replace (update) the object
        $ kubectl replace -f nginx.yaml
        -  fails if the object doesn;t exist
    - completly delete and replace the object: --force
        $ kubectl replace --force -f nginx.yaml

- declarative example
    $ kubectl apply -f nginx.yaml
    $ kubectl apply -f path-to/dir-with/object-files

- a trick to expose a service when creating the pod, at the same time, one-liner
    $ kubectl run httpd --image=httpd:alpine --port=80 --expose=true



2.47 solutions
---
- a trick to expose a service when creating the pod, at the same time, one-liner
    $ kubectl run httpd --image=httpd:alpine --port=80 --expose=true



2.48 apply command
---
- works on 3 objects
    - local file
    - last applied config
    - live object configuration

- last applied configuration
    - to figure out what fields have been removed from the local file
    - used to check if some lines are removed from configs



3.53 manual scheduling
---
- two ways of manual scheduling
    - set the nodeName field in pod definition file
        $ kubectl replace --force -f nginx.yaml     # if the pod already existed
    - create a binding object (pod to node)

- kube-scheduler schedules the pod
    - how this is done:
        - checks pod definitions and sets the "nodeName" property
            - nodeName: node02

- without the scheduler, the easiest way to schedule a pod is to
    - set the nodeName property in the pod definition file
    - Pending state (pod)
        - if scheduler no present then pod in Pending state

- another way is to create a binding object
    - this mimics the way the scheduler works
    - if the pod already assigned to a node
        - in such case, the scheduler won't allow to change the existing object

- binding object to be sent as a POST req
    $ curl --header "Content-Type:application/json" --request POST \
        --data '{"apiVersion":"v1", "kind": "Binding", ...pod-bind-def-to-json..."}' \
        http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/

- pod-bind-definition.yaml
---
apiVersion: v1
kind: Binding
metadata:
    name: nginx
target:
    apiVersion: v1
    kind: Node
    name: node02



3.55 manual scheduling solution
---
- in order to delete/create a pod (use one command)
    $ kubectl replace --force -f nginx.yaml
- instead of these two
    $ k delete pod
    $ k create -f pod

- there is no way to move pod from one node to the other
    - a pod is a process on the system actually



3.56 labels and selector (& annotations)
---
$ kubectl get pods --selector app=App1

- annotations
    - different details; may be used e.g. for integration purposes
        - buildVersion: 1.34
        - toolVersions
    - in metadata
    - same level as labels



3.57 taints and tolerations
---
- taint the node (TaN)
    - as a repellent
    $ kubectl taint nodes node1 app=blue:NoSchedule
    # kubectl taint nodes node-name key=value:taint-effect

- toleration on pod (ToP)
    - to accept the taint

- TaN vs ToP

- master node gets a taint per default, when deployed
    - Taints: node-role.kubernetes.io/control-plane:NoSchedule



3.62 node selectors
---
- first a node is labeled
- when creating a resource, a node selector can be used

- drawback: no advance expressions
    - no NOT operation
        - label = not small
    - no "either" operation
        - label = medium or large
    - for such purpose the "node affinity" is used



3.66 taints, tolearations and node affinity
---
- these 3 must be used simultaneously to place pods on right nodes and prevent the same



3.67 resource requests and limits
---
- cpu limits will be throttled
    - if no cpu available
        - err = insufficient cpu
        - existing pods will be throttled
- memory limits can not be throttled
    - the only way to un-assign the memory is to kill the pod
    - if a pod consumes more than the limit/available
        - err = OOM (Out Of Memory)
        - existing pod will be terminated

- 1G    = 1,000,000,000 bytes (1 Gigabyte)
- 1Gi   = 1,073,741,824 bytes (1 Gibibyte)

- minimum CPU limit = 1m 
    - 1 CPU ~ 1 AWS vCPU
    - 0.1 CPU = 100m 
    - 1m = 0.001 CPU

- ideal CPU settings
    - Requests (are set)
        - for all pods!
        - otherwise a pod could be left with no resources
    - No Limits
    
- ideal Memory settings
    - Requests (are set)
        - for all pods!
        - otherwise a pod could be left with no resources
    - No Limits

- limits could be set on pod level (pod yaml spec)

- default limits
    - per default limits could be set with limit ranges
        - set at the namespace level

- limit-range-cpu.yaml
---
apiVersion: v1
kind: LimitRange
metadata:
    name: cpu-resource-constraint
spec:
    limits:
    - default:
        cpu: 500m
      defaultRequest:
        cpu: 500m
      max:
        cpu: "1"
      min:
        cpu: 100m
      type: Container


- limit-range-memory.yaml
---
apiVersion: v1
kind: LimitRange
metadata:
    name: memory-resource-constraint
spec:
    limits:
    - default:
        memory: 1Gi
      defaultRequest:
        memory: 1Gi
...


- these limits are enforced when a pod is created!

- resource quotas
    - at the namespace level (namespace level object)
    - to limit overall resources 


- resource-quota.yaml
---
apiVersion: v1
kind: ResourceQuota
metadate: 
    name: my-resource-quota
spec:
  hard:
    requests.cpu: 4
    requests.memory: 4Gi
    limits.cpu: 10
    limits.memory: 10Gi



2.68 editing pods and deployments
---
- a deployment can be edited
    $ kubectl edit deployement ABC 

- pod can not be edited always

- what can be edited in pod is
    - spec.containers[*].image
    - spec.initContainers[*].image
    - spec.activeDeadlineSeconds
    - spec.tolerations

- a tactic to re-create the pod
    $ kubectl edit pod <pod name>
        - edit the field which can not be edited
        - save: new /tmp/ file will be created
    $ kubectl delete pod webapp
    $ kubectl create -f /tmp/kubectl-edit-ccvrq.yaml



3.71 daemon sets
---
- they get deployed once per node

- use cases
    - monitoring
    - logging
    - networking (weave-net)
    - kube-proxy
    - ...

- when creating daemonSet, you could create deployent instead
    $ kubectl create deployment --image= ...
        - then change "kind:"

- creating of DeamonSert is also similar to replicaSet
    - what is a difference to RS?

- how k8s create daemonSet? k8s uses
    - NodeAffinity &
    - default scheduler

- before (version < 1.12) it set nodeName in pod definition
    - this creates pod without using scheduler



3.74 static pods
---
- created by kubelet
- so, only a worker node is needed
    - /etc/kubernetes/manifests

- static pods are recreated if they get terminated
    - without the intervention of api-server!

- this way, only the creation of pods is possible
    - not possible to create
        - daemonSets
        - replicaSets
        - deployments
        - services
    - this rest needs other k8s components like
        - replication controller
        - deployment controller, ...
        - etc.

- kubelet works on a pod level

- kubelet setting for statis pod is in "kubelet.service" file
    - --pod-manifest-path=/etc/Kubernetes/manifests \\

- the other way is to put in "kubelet.service" file
    - --config=kubeconfig.yaml \\
    - and then put in "kubeconfig.yaml" the path:
        - staticPodPath: /etc/kubernetes/manifets
    - such approach (kubeconfig.yaml) is used by kubeadm tool
        - kubeadm install

- to inspect static pods on a node use
    - one of a command
        - docker ps     # docker
        - crictl ps     # cri-O
        - nerdctl ps    # container-d
    - since, no kube api server = no kubectl installed

- the usage
    - to create controlplane components
        - install kubelet and place in manifest/ folder
            - controller-manager.yaml
            - apiserver.yaml
            - etcd.yaml
            - manager.yaml
    - services recreated if they would crash

- difference between static pods and daemonSets
    - static pods
        - created by kubelet
        - deploy control plane components as static pods
        - ignored by kube-scheduler
    - daemonSets
        - created by kube-api server (DaemonSet Controller)
        - deploy monitoring & logging agents on nodes
        - ignored by kube-scheduler

- kube-proxy component is not deployed as a static pod



2.77 multiple schedulers
---

- to install kube-scheduler
    - binary, or
    - kubeadm

- define config file
    - my-scheduler-config.yaml
    ---
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
    - schedulerName: my-scheduler
    leaderElection:     # needed if multiple copies of schedulers run in HA cluster
      leaderElect: true
      resourceNamespaces: kube-system
      resourceName: lock-object-my-scheduler


- use the config file from my-scheduler.service
    - if this were a installation from a "bin" file
        ExecStart=/usr/local/bin/kube-scheduler \\
            -- config=/etc/kubernete/config/my-scheduler.config.yaml
    - if this were the "Pod" installation (kubeadm)
        - pass the --config= option to a pod definition
            my-custom-scheduler.yaml
            ---
            apiVersion: v1:
            kind: Pod
            ...
            spec:
            containers:
            - command:
                - kube-scheduler
                - --address=127.0.0.1
                - --kubeconfig=/etc/kubernetes/scheduler.comf
                - --config=/etc/kubernetes/my-scheduler-config.yaml # the sched.-name here defined

                image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
                name: kube-scheduler

- to view schedulers
    $ kubectl get pods --namespace=kube-system

- to use the custom scheduler from a pod
    - specify "schedulerName: my-scheduler" in 
        - pod.yaml
        ---
        apiVersion: v1
        kind: Pod
        metadata:
        name: nginx
        spec:
        containers:
        - name: nginx
            image: nginx
        schedulerName: my-scheduler
    - if pod remains in Pending state
        - there is something wrong with the scheduler config then

- whoch scheduler did configure which pod 
    - view events
        $ kubectl get events -o wide
            # REASON & SOURCE
    - view logs of the scheduler
        $ kubectl logs my-scheduler -n kube-system



2.78 multiple schedulers - solution
---

- create a configmap (configmap as a volume)
    - name: my-scheduler-config
    - from-file=my-scheduler-config.yaml
    $ kubectl create configmap my-scheduler-config --from-file=/root/my-scheduler-config.yaml



2.80 configuring scheduler profiles
---
- ver >= 1.18

- pods get scheduled in this order
    - scheduling queue
        - priorityClassName: high-priority     # defined in pod.yaml
            - if a PriorityClass is defined
        - done by plugin
            - PrioritySort
    - Filtering
        - resource check: pods against nodes
        - plugins:
            - NodeResourceFit
            - NodeName      # nodeName in YMAL
            - NodeUnschedulable     # kubectl cordon $NODENAME
    - Scoring
        - free CPU space, after reserving the current pod
        - the node with more left CPUS gets a higher score
        - plugin
            - NodeResourcesFit
            - ImageLocallity     # high score if container image exists on a node
    - Binding
        - plugin
            - DefaultBinder

- extension points
    - to customize what plugins go where
    - to write our plugins and plug them in here
    - at each stage (scheduling queue, filtering, scoring, binding) there is an extension point
        - queueSort
        - filter
            - prefilter     # this extension point exists too; before filere EP
            - postFilter    # after filter
        - score
            - prescore      # before
            - reserve       # after score
            - permit        # after reserve
        - bind
            - preBind       # before bind
            - postBind      # after bind

- possible to have one scheduler (one binary) running more profiles
    - configuration done per
        - my-scheduler.yaml
        ---
        apiVersion: kubescheduler.config.k8s.io/v1
        kind: KubeSchedulerConfiguration
        profiles:
        - schedulerName: my-scheduler-2
          plugins:
            score:
              disabled:
              - name: TaintToleration
              enabled:
              - name: MyCustomPluginA
              - name: MyCustomPluginB
        - schedulerName: my-scheduler-3
          plugins:
            preScore:
              disabled:
              - name: "*"
            score:
              disabled:
              - name: "*"

- links
    - https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md
    - https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/
    - https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/
    - https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work



4.85 metrics and monitoring
---
- do not use this for prod environment !
    git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git
    cd kubernetes-metrics-server/
    k create -f .


4.86 managing application logs
---
$ docker run kodekloud/event-simulator
    - prints output / shows logs
$ docker run kodekloud/event-simulator      # detached mode
    - no logs shown
$ docker logs -f ecf    # ecf = container_id

- for k8s certification only this is needed:
    - singlecontainer pod
        $ k logs -f event-simulator-pod     # pod name = event-simulator-pod

    - multiple containers
        $ k logs -f event-simulator-pod event-simulator     # container name = "event-simulator"


4.92 rolling udpates and rollbacks (of a deployment)
---
- deployment creates a rollout

- 2 deployment strategies
    - recreate
        - destroys all & creates all
    - rollout
        - one pod by one
        - default strategy

- kubectl "apply" vs "set image"
    - apply 
        - if you change the YAML input
    - set image
        - doesn't update the YAML input!
        $ kubectl set image deployment/myapp-deployment \
            nginx-container=nginx:1.9.1

- upgrade happens per(via) changing of replicasets
    $ kubectl get replicasets
        - check this during a deployment upgrade

- rollback
    $ kubectl rollout undo deployment myapp-deployment      # rollback
        $ k get rs  # try this one again

- status
    $ kubectl rollout status deployment myapp-D

- history
    $ kubectl rollout history deployment myapp-d

- commands:
    $ k apply
    $ k set image 
    $ kubectl rollout
        - pause
        - status
        - history
        - undo    # rollback
            $ k rollout undo deployment frontend --to-revision=2

- set the deployment image
    - set image
        $ k set image deploy frontend simple-webapp=kodekloud/webapp-color:v2
        $ k set image deploy $DEPLOYMENT_NAME $CONTAINER_NAME=$IMAGE_NAME
    - edit deployment
        $ k edit deployment frontend













