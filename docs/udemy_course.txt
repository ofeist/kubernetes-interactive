course name:certified kubernetes admin course
tutor: mumshad mannambeth
location: Documents/2_EDU/kubernetes

would be good to do before:
kubernetes for absolute beginners


few links
---
- https://killer.sh/    # the best simulator
  - https://killer.sh/course/preview/e84d0e31-4fff-4c42-8afd-be1bdbc0d994
  - alias k=kubectl
  - export do="-o yaml --dry-run=client"
- the guys experience: https://georgearisty.dev/posts/ckad/
  - https://georgearisty.dev/posts/k8s-cluster-network/
---



2. cluster architecture
---
cargo and control ships analogy


master nodelogin
--
- manage
- plan
- scheduler
- monitor nodes

worker nodes
--
host applications as containers


master node consists of
--
- etcd :: key-value format db
- kube-apiserver :: orchestrator
- kube-scheduler :: how to deploy containers
- kube controller manager
  - node controller
  - replication controller


worker nodes consist of
--
- kubelet :: "the capetan"
- kube-proxy :: communication between services & containers inside of the cluster

container runtime service ( e.g. docker or rkt (rocket) )



3. & 4. etcd
---
- port: 2379
- stores / has the data about the cluster
(nodes, pods, configs, secrets, accounts, roles, bindings, others)


2 types of k8s cluster
--
- installed from scratch
  - you have to install etcd-server by yourself
- kubeadm tool
  - deploys etcd-server as a pod in kube-system namespace
$ kubectl get pods -n kube-system


$ kubectl exec etcs-master -n kube-system
$ etcdctl get / --prefix -keys-only

- data is in the root directory
/registry/
- an then constructs such as
	- minions, pods, replicasets, deployments, roles, secrets


- in HA environment
--
- there will be multiple master nodes in the cluster
- there will be multiple etcd instances spread across the master nodes
- etcd instancs shall know about each other
  - "--initial-cluster" option shall be specified



5. kube-apiserver
---
- kubectl <-reaches-the-> kube-apiserver (authorization/validation) <-retreaves-data-from-> etcd cluster
- kube-apiserver then authentizates and validates request


workflow example: create a pod (not using kubectl, but per APIs)
---
1] apiserver then creates a pod object without assigning it to a node
# curl -X POST /api/v1/namespaces/default/pods ...[other]
- the request is authenticated
- the request is validated
- the data is retrieved
- etcd is updated
- the user gets back the data


2] scheduler
- cont. monitors apiserver and realizes there is a new pod without a new node assigned
- updates kube-apiserver where to put the pod

3] kubeapi-server
- updates etcd-cluster
- passes the info to kubelet on the appropriate worker node

4] kubelet
- creates the pod on the worker node
- instructs the CRE (container runtime engine) to deploy app image
- updates status to kube-apiserver

5] kube-apiserver
updates data back into etcd cluster


kube-apiserver responsibilities are to:
--
1] authentcate user
2] validate request
3] retrieve data
4] update etcd (only component that talks to etcd is kube-apiserver)
5] scheduler
6] kubelet


kube-apiserver options
--
- eg. internesting are
  - certificates :: connectivity between different components (eg. ca.pem, kubernetes.pem, ..)
    - --etcd-catfile=..; --etcd-certfile=..; --etcd-keyfile=..;
  - location of etcd-servers
    - etcd-servers=..


- viewing kube-apiserver options
--
- depends on how you set up your cluster
  - if kubeadmin tool
    - then kube-apiserver-master deployed as a pod in -namespace kube-system @ master
    - the options could be seen in manifest file
      - /etc/kubernetes/manifests/kube-apiserver.yaml
  - if no-kubeadmin setup
    - /etc/systemd/system/kube-apiserver.service
  - or list the process
    - on the master node
      - ps -aux | grep kube-apiserver



6. kube-controller-manager
---
- the role of controller process is to:
  - watch the status
  - remediate (correct, repair) situation

- controllers examples:
  - node-controller
    - node monitor period = 5 s
      - takes the state every 5 seconds)
    - node monitor grace period - 40 s
      - after heartbeat from node notreached, node marked as unreachable
    - pod eviction timeout = 5 min
      - if node deosn't come up, it is removed, another one is provisioned
  - replication-controller
    - monitors the state of replicaSets
    - ensuring the desired nr of pods available all the times within the set
  - deployment-controller
  - namespace-controller
  - endpoint-controller
  - job-controller
  - pv-protection-controller
  - pv-binder-controller
  - replication-controller
  - cronjob
  - stateful-set
  - replicaset


- installing kube-controll-manager
--
$ wget https://storage.googleapis.com/kubernetes-release/v1.13.0/bin/linux/amd64/kube-controller-manager
# run it as a service


- options are @ kube-controller-manager.service
  - abovementioned options are here
    - --node-monitor-period=5s
    - --node-monitor-grace-period=40s
    - --pod-eviction-timeout=5m0s
  - by default all controllers are enabled
  - if some of them are not, here is a good starting point to look at
    - -- controllers stringSlice

- view kube-controller-manager server options
--
1] kubeadmin setup:
- kubeadmin deploys kube controller manager
$ kubectl get pods -n kube-system
$ cat /etc/kubernetes/manifest/kube-controller-manager.yaml

2] no-kubeadmin setup
$ cat /etc/systemd/system/kube-controller-manager.service
- or
$ ps aux | grep kube-controller-manager



7. kube-scheduler
---
- (only) decides which pod goes on which node
- kubelet :: actually places the pod on the node

- how it's done
--
1] filter nodes (e.g. that do not have enough CPUs)
2] rank nodes (ammount of resources after the pod is placed)
3] more requirements come later ..


- installing kube-scheduler
--
wget http://storage.googleapics.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-scheduler
# extract it and run it as a service


- viewing kube-scheduler options
--
$ cat /etc/kubernetes/manifests/kube-scheduler.yaml	# pod definition file
# kubeadm :: pod in kube-system namespace on the master node

$ ps -aux | grep kube-scheduler



8. kubelet
---
- is like captain of the worker node
1] registers the node with the k8s cluster
2] creates the pod (requests the CRE to pull the image and run an instance)
3] monitors node & pods


- installing kubelet
--
# kubeadm does not deploy kubelets
# kubelet has to be always manually installed on worker nodes
wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kubelet

- viewing kubelet options
--
# listing the process on the worker node
ps -aux | grep kubelet



9. kube-proxy
---
- POD Network exists across all nodes

- kube-proxy
  - process that runs on each node
  - looks for new services
    - when service created
      - using iptables
      - creates rules on each node
      - to forward traffic from services to the backend pods


- installing kube-proxy
--
# download, extract it and run it as a service
$ wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-proxy

# kubeadm deploys kube-proxy as pod on each node
# it is deployd as a deamonset
$ kubectl get demonseat -n kube-system



10. pods
---
- the smallest k8s'object
- a single instance of an application
- pod encapsulates the deployed container

- scaling application happens via single container pods

- multi-container pods
--
- having hepler containers (supporting task for e.g. web app)
- all of them share
  - same network space (@ localhost)
  - same storage space


- installing (deploying) pods
--
$ kubectl run nginx --image nginx   # create pod
# the command deploys a docker container by creating a pod

$ kubectl get pods
# list of pods in our cluster




11. & 12. pods with yaml
---
- 4 toplevel properties:
  - apiVersion, kind, metadata, spec

- pod-definition.yaml
--
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx

$ kubectl create -f pod-definition.yaml
$ kubectl get pods
$ kubectl describe pod myapp-pod
$ kubectl delete deployment myapp-pod


13. & 14. & 15. practice test introduction, accessing labs and solution
---
- kodekloud

$ sudo apt install yamllint -y
$ kubectl run redis --image=redis --dry-run=client -o yaml > redis-pod.yaml
$ kubectl apply -f redis-pod.yaml
# now fix the pod definition (the upper definition is correct, nothing to fix there ...)
$ kubectl edit pod redis



16. & 17. recap - replicaSets
---
- reasons for replication:
  - HA
  - load balancing & scaling

- replication controller spans accross multiple nodes in the cluster
- notice the difference:
  - Replication Controller vs. Replica Set
    - they have same purpose, but they are not the same
    - Replication Controller is the older tehnology
    - Replica Set is the recommended way to set up the replication
    - ReplicaSet has to have "selector:" field

1] replication controller
--
rc-definition.yaml
---
apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
    type: frontend
spec:
  template:
    metadata:
      name: nginx
      labels:
        app: myapp
    spec:
      containers:
        - name: nginx-container
          image: nginx
  
  replicas: 3

$ kubectl create -f rs-definitionl.yaml
$ kubectl get replicationcontrollers
$ kubectl get pods

2] replicaset
rs-definition.yaml
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    app: myapp
    type: front-end

spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
  
  replicas: 3
  selector:
    matchLabels:
      type: front-end

$ kubectl get replicasets
$ kubectl get pods

- labels and selectors
  - in generall used to connect dots in kubernetes

- how to scale
1]
# set replicas: 6 in yaml file
$ kubectl replace -f rs-definition.yaml

2]
# either
$ kubectl scale --replicas=6 -f rs-definition.yaml
# this doesn't change the yaml definition file
# or
$ kubectl scale --replicas replicaset myapp-replicaset

- to racap:
$ kubectl get replicaset
$ kubectl get pods
$ kubectl delete replicaset myapp-replicaset    # deletes all underlying pods
$ kubectl replace -f replicaset-definition.yaml
$ kubectl scale --replicas=6 -f rs-definition.yaml
$ kubectl scale replicaset --replicas=5 myapp-replicaset

- replicaset ensures that the desired number of pods always run


18. & 19. deployments
---
- purposes:
  - many instances of server (tech stack)
  - upgrades
  - rolling updates
  - rollback
  - pause & resume changes

- pod < replicaSet < deployment

- deployment-definition.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx
  replicas: 3
  selector:
    matchLabels:
      type: front-end


$ kubectl create -f deployment-definition.yaml
$ kubectl get replicaset
$ kubectl get pods
$ kubectl get all

- to recap the file structure:
--
(aki mesec)
apiVersion:
kind:
metadata:
spec:

--
apiVersion:
kind:
(metanola)
metadata:
  name:
  labels:
(sxpek trese)
spec:
  (temple mesec)
  template:
    metadata:
    spec:
  replicas:
  selector:


- to create deployment imperative way:
$ kubectl create deployment httpd-frontend --replicas=3 --image=httpd:2.4-alpine
$ kubectl create deployment httpd-frontend --replicas=3 --image=httpd:2.4-alpine --dry-run=client -o yaml
# --dry-run=client -o yaml



20. namespaces
---
- isolation

- existing:
  - default
  - kube-system :: services, k8s resources ...
  - kube-public :: 

- to a namespace the following could be assigned:
  - policy
  - resources

- DNS is created
  - e.g. to reach the service:
    -inside of the namespace
      - db-service
    - in another namespace
      - db-service.dev.svc.cluster.local
  - cluster.local = domain
  - svc = subdomain (for a service)
  - dev = namespace
  - db-service = service name
        
$ kubectl get pods --namespace=kube-system

$ kubectl create -f pod-definition.yaml
$ kubectl create -f pod-definition.yaml --namespace=dev

- defined under "metadata" section

- pod-definition.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  namespace: dev
  labels:
    app: myapp
    type: front-end
spec:
  containers:
  - name: nginx-controller
    image: nginx

- how to create a new namespace:

1]
- namespace-dev.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: dev

$ kubectl create -f namespace-dev.yaml

2]
$ kubectl create namespace dev

- switch the working namespace
$ kubectl config set-context $(kubectl config current-context) --namespace=dev

$ kubectl get pods --all-namespaces   # across all namespaces


- to limit resources in a namespace create a resource quota:
compute-quota.yaml
---
apiVersion: v1
kind: ReqourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi

$ kubectl create -f compute-quota.yaml

$ kubectl get namespaces --no-headers | wc -l

# reach db-service in the marketing namespace
# db-service.marketing.svc.cluster.local



22. services & NodePort
---
- k8s object (like replicaSet, deployment, ..)
- it listens to a port on the node and to forward request on that port to a port on the node running the web application
- this is NodePort service

- ServiceTypes
--
- NodePort
  - forwards request node out <> in
- ClusterIP
  - creates virtual ip
- LoadBalancer
  - balances requests

- NodePort service
--
1] targetPort
  - the pod port
  - it has the IP address
2] port
  - the service port
  - it has the IP address (the ClusterIP of the service)
3] nodePort
  - range: 30000-327676

- service-definition.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
  - nodePort: 30008
    port: 80
    targetPort: 80
  selector:
    app: myapp
    type: front-end

- pod-definition.yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: myapp
    type: front-end
  name: myapp-pod
spec:
  containers:
  - image: nginx
    name: nginx-container

$ kubectl create -f service-definition.yaml
$ kubectl create -f pod-definition.yaml

- the service acts as a load balancer if more pods are targeted
  - random algorith
  - with seesionaffinity
- if pods are distributed accross multiple nodes
  - service than spans accross all nodes in the cluster
  - target node is same on all nodes in the cluster
    - curl 192.168.1.2:30008
    - curl 192.168.1.3:30008
    - curl 192.168.1.4:30008


23. service clusterIP
---
- the clusterIP reminds me on the "facade" programming pattern
- each service get the ip which is then used to access the services "accumulated" behind this ip
- this is the default service type!

service-definition.yaml
---
apiVersion: v1
kind: Service

metadata:
  name: back-end

spec:
  type: ClusterIP
  ports:
  - targetPort: 80
    port: 80

  selector:
    app: myapp
    type: back-end



24. service LoadBalancer
---
- this works only with supported platforms (AWS, googleCloud, azure, ...)

- service-definition.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: LoadBalancer
  ports:
  - targetPort: 80
    port: 80
    nodePort: 30008

- if the platform doesn't support the LoadBalancer type, the effect is same as setting the type to NodePort



25. solution - services 
---
$ kubectl get svc                   # get the number, type of services
$ kubectl describe svc kubernetes   # the target port, Labels, Endpoints for kuberneter service -> targetPort
$ kubectl get deployments           # how many deployments
$ kubectl describe deployment nginx | grep -i image         # the image for deployment
$ kubectl create deployment web-app --dry-run=client -o yaml --image=nginx > webapp-deployment.yaml   # creates the deployment
$ kubectl expose deployment webapp --name=webapp-service --target-port=8080 --type=NodePort --port=8080   # create a service out of the deployment
$ kubectl expose deployment webapp --name=webapp-service --target-port=8080 --type=NodePort --port=8080 --dry-run=client -o yaml > webapp-service.yaml  # create a service out of the deployment
  # edit the webapp-service.yaml and add "nodePort: 30080"



26. imperative vs declarative
---
- imperative
  - create and update objects
  - kubectl run/create/expose/edit/scale/set image/create -f/replace -f/ delete -f object
  - use imperative approach for the exam, to save the time
- declarative
  - create a set of files
  - "kubectl apply -f " command
    - apply command takes a look at the existing configuration and figures out what needs to be changed
  - if the exam requires more complicated objects/more containers, this approach is better


- imperative approach
--
- create objects per object configuration files
  $ kubectl create -f nginx.yaml
- update objects:
  $ kubectl edit deployment nginx.yaml
    - this changes will be applied to the live object only, and not to the definition file
  $ kubectl replace -f nginx.yaml
    - better approach is to change the definition file and to track the changes
  $ kubectl replace -f --force nginx.yaml
    - if you want to delete and replace objects

- declarative approach
--
- use the object configuration files to create and update objects
  $ kubectl apply -f nginx.yaml
  $ kubectl apply -f /path/to/config/files

- check kubernetes documentation
  - "manage kubernetes objects"   # documentation



27. imperative vs declarative - solutions
---
$ kubectl run nginx-pod --image=nginx:alpine

$ kubectl run redis --image=redis:alpine --labels="tier=db"
$ kubectl expose pod redis --port=6379 --target-port 6379 --cluster-ip='' --selector=tier=db --name=redis-svc
  # or
  $ kubectl expose pod redis --port=6379 --selector=tier=db --name=redis-svc
    # --cluster-ip= is the default service
$ kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3
  # or
  $ kubectl create deployment webapp --image=kodekloud/webapp-color
  $ kubectl scale deployment webapp --replicas=3

$ kubectl run custom-nginx --image=nginx --port=8080

$ kubectl create ns dev-ns
$ kubectl create deployment redis-deploy --namespace=dev-ns --image=redis --replicas=2

$ kubectl run httpd --image=httpd:alpine
$ kubectl expose pod httpd --name=httpd --cluster-ip='' --port=80 --target-port=80
  # or
  $ kubectl run httpd --image=httpd:alpine --port 80 expose (--dry-run=client -o yaml)



2.28. kubectl apply command
---
- kubectl compares
  - local configuration file
  - last applied configuration
  - a live configuration of the object definition on k8s
  
- kubectl apply -f file
  - yaml -> json
  - json location: k8s cluster in live object configuration
    - annotations:
      - kubectl.kubernetes.io/last-applied-configuration
  - kubectl create/replace commands do not store this configuration!
    - that's why do not mix imperative and declerative ways when managing k8s cluster

- last applied configuration
  - contains fields that were removed from 
    - local file 
    - and from the live k8s configuration

- check k8s documentation
  - "merging changes to primitive fields"   # documentation



3. scheduling
===

3.1. introduction
---

3.2. manual scheduling
---
- nodeName propery in the pod-definition.yaml
  - if it is not there, then this is is candidate for scheduling

- nodeName can be set per binding definition

pod-bind-definition.yaml
--
apiVersion: v1
kind: Binding
metadata:
  name:nginx
target:
  apiVersion: v1
  kind: Node
  name: node02

- then the data in json format is sent to the api
curl --header "Content-Type:application/json" \
  --request POST \
  --data '{"apiVersion":"v1", "kind": "Binding" ...}' \
  http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/



3.3. solution - manual scheduling
--
$ kubectl apply -f nginx.yaml
$ kubectl -n kube-system get pods

- how to disable scheduler: https://github.com/kubernetes/website/issues/21128
- find the part "We originally followed this tutorial: "

- after the scheduler was disabled, the nodeName has to be specified
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx
  name: nginx
spec:
  nodeName: node01
  containers:
  - image: nginx
    name: nginx

- by specifying the "nodeName", the need for scheduler has been circumvented


3.4. labels and selectors
---
- grouping
$ kubectl get pods --selector app=App1  # get pods with labels

- when creating objects
- selector/matchLabels must match template/metadata label

- annotations:
--
- e.g. tool details like name, version ..



3.5. labels and selectors - solution
---
$ kubectl get pods --show-labels
$ k get pods -l env=dev --no-headers
$ k get all --show-labels --no-headers -l env=dev
$ k get pods -l env=prod,bu=finance,tier=frontend



3.6. taints and tolerations
---
- node <-> taints
- pod <->  tolerations

- taints
--
$ kubectl taint nodes node-name color=blue:taint-effect
- taint-effect
  - noSchedule
  - PreferNoSchedule
  - NoExecute
$ kubectl taint nodes node01 app=blue:noSchedule


- tolerations
--
pod-definition.yaml
---
apiVersion: v1
kind: Pod
  name:myapp-pod
spec:
  containers:
  - name: nginx-container
    image: nginx

  toleartions:
  - key:"app"
    operator:"Equal"
    value:"blue"
    effect:"NoSchedule"

$ kubectl describe node controlplane | grep Taint



3.7. solution - taints and tolerations
---
$ kubectl explain pod --recursive | less
$ kubectl explain pod --recursive | grep -A5 tolerations

$ kubectl describe nodes node01 | grep -i taint
$ kubectl taint node master app=blue:NoSchedule-  # the minus removes the taint


controlplane $ k taint node node01 app=blue:NoSchedule
controlplane $ k apply -f nginx.yaml 
controlplane $ k get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP            NODE           NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          8s    192.168.0.8   controlplane   <none>           <none>
controlplane $ k delete pod nginx 
controlplane $ k taint node node01 app=blue:NoSchedule-
controlplane $ k apply -f nginx.yaml
controlplane $ k get pods -o wide
NAME    READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
nginx   0/1     ContainerCreating   0          7s    <none>   node01   <none>           <none>

- now set the toleration in the nginx.yaml definition
nginx.yaml 
---
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx

  tolerations: 
  - effect: "NoSchedule"
    key: "app"
    operator: "Equal"
    value: "blue"
  restartPolicy: Always

$ k delete pod nginx
$ k apply -f nginx.yaml

controlplane $ k get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          3s    192.168.1.4   node01   <none>           <none>

# runs again on the node01

$ taint node node01 app=blue:NoSchedule



3.8. node selectors
---
- node selectors based on the labels assigned to the node

$ kubectl label node node-name key=value  # to label the node
$ kubectl label nodes node01 size=Large

- selectors are exclusive (u can not say e.g. nodeSelector: "Large OR Medium" )
  - for that the nodeAffinity was introduced



3.9. node affinity & solutions
---
- required OR ignored
  - during scheduling
  - during execution
  - eg:
    - requiredDuringSchedulingIgnoredDuringExecution

$ kubectl get nodes node01 --show-labels
$ kubectl label nodes node01 color=blue
$ kubectl create deployment blue --image=nginx --replicas=6
$ kubectl get pods -o wide

- check kubernetes documentation
  - "Assign Pods to Nodes using Node Affinity"   # documentation
    - "Schedule a Pod using required node affinity"   # documentation
      https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/   # link



3.11. node affinity vs taints and tolerations
---
- how to place a combination of 
  - red, green, blue and other pods
  - onto the red, green, blue and other nodes

1] taint RGB nodes
2] tolerate RGB pods
3] affinity of other pods for other nodes



3.12. resource requirements and limits
---
- configure pods and containers   # documentation
  - kubernetes.io
  - https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/



3.13. resource requirements and limits - solutions
---
$ kubectl create -f ns-mem-example.yaml
$ kubectl apply -f pod-mem-example.yaml
$ kubectl get pod memory-demo -n mem-example
$ kubectl get pod memory-demo --output=yaml --namespace=mem-example
$ kubectl top pod memory-demo --namespace=mem-example
$ kubectl top pod memory-demo --namespace=mem-example
  # metrics API not available!!!
$ kubectl delete pod mem-example -n mem-example
$ kubectl config set-context $(kubectl config current-context) --namespace=mem-example   # set (move to) default namespace
$ kubectl apply -f pod-mem-example-2.yaml



3.14. deamon sets
---
- created by kube-api server
- ignored by kube-scheduler
- https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/  # documentation
- deamonSets run one copy of a pod on each node in the cluster
- use cases:
  - install agents
    - monitoring solutions
    - logs viewer
  - kube-proxy
  - weave-net (networking agent)

- similar to ReplicaSet, but with the kind of "DaemonSet"
- to create it programatically
  1] kubectl create deploy nginx --image=nginx --dry-run -o yaml > nginx-ds.yaml
  2] edit the nginx-ds.yaml
    - delete "replicas: 1" line
    - set "kind: DaemonSet" instead of "Deployment" kind

- how does this work?
  - set the nodeName property on each node and bypass the scheduler
    - this approach was used untill k8s v1.12
  - use NodeAffinity and defaultscheduler
    - since v1.12


3.15. daemonset - solutions
---
$ kubect create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 --dry-run=client -oyaml > ds-es.yaml
  # delete replicas, strategy
  # add the namespace
  # change kind Deployment -> DaemonSet



3.16. static pods
---
- created by kubelet
- ignored by kube-scheduler
- kubelet creates a static pod if a pod-definition yaml is placed under
  - /etc/kubernetes/manifests

- this way you cannot create replicasets, daemonsets, ...
- kubelet works on the pod level, so this is possible

- the pod definition path is defined either
  1] as the option to the kubelet.service
  - --pod-manifest-path=/etc/Kubernetes/manifests \\
  - or
  2] as the option of the config file to the kubelet.service
  - --config=kubeconfig.yaml  \\
    - kubeconfig.yaml
    ---
    staticPodPath: /etc/kubernetes/manifests
  - clusters setup using kubeadmin tool use this approach

- to check static pods use the command
  $ docker ps

- kube-apiserver is aware of both
  - static pods and 
  - pods created per kube-apiserver

- kube-apiserver
    - the command "kubectl get pods" returns static pods too
    - but this is only the read property
    - any change on static pods is done per static pod definition yaml file

- the k8s cluster could be set by placing k8s komponent definition files 
  to the manifests folders on the nodes
  - this is how kubeadmin tool does the setup of the k8s cluster
  - this is why you see pods when running
    $ kubectl get pods -n kube-system



3.17. static pods - solution
---
- where are the static pod definition files?
  - $ ps -ef | grep kubelet
  - $ ps aux | grep kubelet   # this worked for me
  - $ systemctl cat kubelet   # can this be used for checking the static pod manifests location?
  - check the "kubelet" setup for the 1.28! # todo
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/  # documentation
  - https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/    # documentation

$ kubectl run static-busybox --image=busybox --dry-run=client -oyaml --command -- sleep 1000 > static-busybox.yaml



3.18. multiple schedulers
---
- how to deploy a kube-scheduler -> installing kube-scheduler

1.a] wget
$ wget http://storage.googleapics.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-scheduler
# extract it and run it as a service
- kube-scheduler.service
  - --scheduler-name=default-scheduler  # if not specified, this is default name option
  - when deploying additional scheduler, set the --scheduler-name e.g. to
    - --scheduler-name=my-custom-scheduler

1.b] kubeadm tool
- make a copy (& rename) the file
  - /etc/kubernetes/manifests/kube-scheduler.yaml
- set the custom name as part of the command
  - --scheduler-name=my-custom-scheduler

2] create pod-definition.yaml
- the next step is to set the scheduler in pod definition
  - schedulerName: my-custom-scheduler
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx

  schedulerName: my-custom-scheduler

- how to know which scheduler picked it up?
$ kubectl get events  # check which scheduler picked up the pod

- how to view the logs
$ kubectl logs my-custom-scheduler --name-space=kube-system   # check logs of the custom scheduler



3.19. multiple schedulers - solution
---
- https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/   # documentation
$ kubectl create -f my-kube-scheduler.yaml
- Failed to pull image "gcr.io/my-gcp-project/my-kube-scheduler:1.0": failed to pull and unpack image "gcr.io/my-gcp-project/my-kube-scheduler:1.0": failed to resolve reference "gcr.io/my-gcp-project/my-kube-scheduler:1.0": failed to authorize: failed to fetch anonymous token: unexpected status: 400 Bad Request

$ watch "kubectl get pods"



3.20. configuring scheduler
---
- manually (wget) vs. kubeadm tool (kubectl)
- ~0.45 shows advanced scheduling options
- https://github.com/kubernetes/community/tree/master/contributors/devel
            - the lower one didnt exist anymore
                - https:github.com/kubernetes/community/blob/master/contributors/devel/scheduler.md
- https://kubernetes.io.blog/2017/03/advanced-scheduling-in-kubernetes/
- https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/
- https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work



4.2. logging and monitoring
---
- heapster is deprecated

- metrics server (heapster's slimmed down version)
  - in-memory performance data
    - no historical performance data

- kubelet (agent) generates the metrics
  - cAdvisor component (containers advisor)
    - retrieve performance metrics from pods
    - exposes them through the kubelet api
      - makes it available to the metrics server

- to deploy metrics-server
---
- minikube environment:
$ minikube addons enable metrics-server   # if you use minikube

- all other environments:
$ git clone https://github.com/kubernetes-incubator/metrics-server.git
$ kubectl create -f deploy/1.8+/

- probably up-to-date info:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml   # install metrics server

- to check performance (cpu / memory)
$ kubectl top node
$ kubectl top pod



4.4. managing application logs
---
$ docker run kodekloud/event-simulator    # event-simulator
$ docker logs -f ecf  # ???

$ k run event-simulator-pod --image=kodekloud/event-simulator --dry-run=client -oyaml > event-simulator.yaml
$ k create -f event-simulator.yaml
$ k logs -f event-simulator-pod

- if more containers inside of pod, then the container from 
  which i'd like to read the logs must be specified

- e.g. i have event-simulator + nginx inside of the pod
  $ kubectl logs -f event-simulator-pod event-simulator   # read the logs from event-simulator



4.5. logging - solution
---
- if the pod has more containers, -c option shows containers
  $ kubectl logs webapp-2 -c simple-webapp  # gives the logs of simple-webapp container in the pod



5.1. application lifecycle management
---
- mentioned already in KCD (kubernetes certified developer course)



5.2. rolling updates and rollbacks
---
- creating a deployment triggers a rolout = deployment revision (function of app/container version)
- a next, new rolout creates new deployment revision (app version)

- deployment status
$ kubectl rollout status deployment/myapp-deployment    # deployment status

- revision and rollout history 
$ kubectl rollout history deployment/myapp-deployment   # revision and rollout history

- deployment strategies
  - recreate strategy
    - destroy & create instances(versions)
    - application downtime
    - not a default strategy
  - rolling update
    - replace instance(version) by instance
    - no downtime
    - default deployment strategy
strategy:    

- how to update deployment
  - k apply command
    $ kubectl apply -f deployment-definition.yaml
      - creates new rollout + new revision
  - k set image
    $ kubectl set image deployment/myapp-depl nginx=nginx:1.9.1
      - whis will result in different config of definition file and deployment

$ kubectl describe
  - gives deployment detailed infomation

- upgrades could be followed watching replicasets
  $ kubectl get replicasets

- rollback of deployment
  $ kubectl rollout undo deployment/myapp-deployment  # rollback deployment
  $ kubectl get replicasets   # before and after deployment

- to summarize deployment commands
$ kubectl create -f deployment-definition.yaml  # create deployment
$ kubectl get deployments
$ kubectl apply -f deployment-definition.yaml   # update deployment
$ kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1
$ kubectl rollout status deployment/myapp-deployment
$ kubectl rollout history deployment/myapp-deployment
$ kubectl rollout undo deployment/myapp

- edit the deployment
$ kubectl edit deployement web-app
  - strategy: recreate  # for example

- FYI - not default option is:
  strategy:    
    type: Recreate



5.4. application commands
---
- not part of exam

- docker image like
---
FROM Ubuntu

ENTRYPOINT ["sleep"]

CMD ["5"]

$ docker run ubuntu-sleeper 10
$ docker run --entrypoint sleep2.0 ubuntu-sleeper 10
  - sleep2.0 is the "new" (another) sleep application 



5.5. application commands and arguments
---
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      command: ["sleep2.0"]
      args: ["10"]

- similarity to docker
  - command ["sleep"] <> ENTRYPOINT ["sleep"]
  - args ["10"] <> CMD ["10"]



5.6. commands and arguments - solution
---
- interesting was flask
  - is this a web server/web app?

1] 
  
Dockerfile (kodekloud/webapp-color)
---
FROM python3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python","app.py"]

CMD ["--color", "red"]


webapp-color-pod.yaml
---
image: kodekloud/webapp-color
command: ["--color","green"]


2] web-app.yaml (from kodekloud)
video @ ~9.00
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: webapp-green
  name: webapp-green
spec:
  containers:
    args: [ "--color=green" ]
    image: kodekloud/webapp-color
    name: webapp-green
    resources: {}
  restartPolicy: Never

gives:
Error from server (BadRequest): error when creating "webapp-green.yaml": Pod in version "v1" cannot be handled as a Pod: json: cannot unmarshal object into Go struct field PodSpec.spec.containers of type []v1.Container



5.7. configure environment variables in applications
---
- use the "env:" property
- env is an array
  -> every property in the array starts with the dash "-"

- key value pair
env:
  - name:
    value:

- ConfigMap (use "valueFrom")
env:
  - name: APP_COLOR
    valueFrom:
      configMapKeyRef:

- Secrets (use "valueFrom")
env:
  - name: APP_COLOR
    valueFrom:
      secretKeyRef:



5.8. configure config map
---
- imperative
  $ kubectl create configmap 

- declerative
  $ kubectl create -f

- imperative
$ kubectl create configmap \
  app-config --from-literal=APP_COLOR=blue  \
              -- from literal=APP_MOD=prod

- from a file
kubectl create configmap \
  app-config  --from-file=app_config.properties


- declarative:
config-map.yaml
---
apiVersion: v1
data:
  APP_COLOR: blue
  APP_MODE: prod
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: app-config

$ kubectl create -f config-map.yaml

- view config map
  $ kubectl get configmaps
  $ kubectl describe configmaps config-map

- now you have to configure this in the pod
spec:
  envFrom:
  - configMapRef:
    name: app-config


- in general, 3 ways of injecting configmap
  1] env
    envFrom:
      - configMapRef:
        name: app-config

  2] single env
    env:
      - name: APP_COLOR
        valueFrom:
          configMapKeyRef:
            name: app-config
            key: APP_COLOR

  3] VOLUME
    volumes:
    - name: app-config-volume
      configMap:
        name: app-config



5.9. environment variables - solution
---
- overall, this part i need to repeat more thoroughly
- https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/

- configure microservices using configMap and secrets
  - https://kubernetes.io/docs/tutorials/configuration/configure-java-microservice/configure-java-microservice/

- katacoda shutter
  - https://kubernetes.io/blog/2023/02/14/kubernetes-katacoda-tutorials-stop-from-2023-03-31/



5.10. configure secrets in applications
---
- create a secret
  - kubectl create secret generic   # imperative
  - kubectl create -f               # declarative

- imperative
  - options
    --from-literal
    --from-file

- declerative
  - secret-data.yaml
  ---
  apiVersion: v1
  kind: Secret
  metadata:
    name: app-secret [A]
  data:
    DB_Host: mysql
    DB_User: root
    DB_Password: passwrd

- but, for the data values use
$ echo -n 'mysql' | base64
$ echo -n 'root' | base64
$ echo -n 'passwrd' | base64

$ kubectl create -f secret-data.yaml
$ kubectl get secrets
$ kubectl describe secrets
$ kubectl get secret app-secret -o yaml

- to decode the values
$ echo -n 'bXlzcWw=' | base64 --decode


- inject into pod

pod-definition
---
apiVersion: v1
kind: Pod
...
...
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
      - containerPort: 8080
    envFrom:
      - secretRef:
            name: app-secret [A]

$ kubectl create -f pod-definition.yaml


- more ways of injecting secrets into pods
1] from secrets (ENV)
2] SINGLE ENV
3] VOLUME

- volume
  -name: app-secret-volume
  - creates a file for each secret
  $ ls opt/app-secret-volumes



5.11. secrets - solutions
---
$ kubectl create secret generic db-secret \
--from-literal=DB_Host=sql01 \
--from-literal=DB_User=root \
--from-literal=DB_Password=password123 \
--dry-run=client -oyaml


db-secret.yaml
---
apiVersion: v1
data:
  DB_Host: c3FsMDE=
  DB_Password: cGFzc3dvcmQxMjM=
  DB_User: cm9vdA==
kind: Secret
metadata:
  creationTimestamp: null
  name: db-secret


$ kubectl explain pods --recursive | grep envFrom -A5
- input
  envFrom:
    - secretRef:
          name: db-secret


 webapp-pod.yaml
 ---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: webapp-pod
  name: webapp-pod
spec:
  containers:
  - image: kodekloud/simplewebapp-mysql
    name: webapp-pod
    envFrom:
    - secretRef:
        name: db-secret


5.12. multi container pods
---
- multicontainer pods share
  - lifecycle
  - network
  - storage

spec:
  containers:
  - name: simple-webapp

- the dash "-" marks that containers are an "array"

- You can use following command to delete the POD forcefully
  $ kubectl delete pod <PODNAME> --grace-period=0 --force



5.13. multi container pods - solutions
---
- sidecar container 
  -> sends logs to elasticsearch



5.14. init containers - solutions
---
- what are init containers?



6.01 cluster maintenance
---

6.02 os updates
---
- if pods part of replicaset
  - pods from a node down are deleted and recreated on another node 
    after the eviction-timeout
  - --pod-eviction-timeout=5m0s   # default

- in theory, some update could be done by taking the node down for < 5min
  - but, you can never tell if you're loose pods (not part of the replicaSet)

- how this shall be done: drain the node
  $ kubectl drain node-1      # "drain" moves pods to another nodes
    - drained node is unschedulable
  # do the maintenance (take down the node), and e.g. reboot
  $ kubectl uncordon node-1   # uncordoned node is now schedulable

- instead of drain you can mark the node unschedulable with "cordon"
  $ kubectl cordon node-1   # marks as unschedulable; doesn't drain



6.03 os updates - solution
---
- daemonSets can not be deleted (drained)
  - --ignore-daemonsets

- play around with taints, i suppose



6.04 kubernetes software versions (releases)
---
$ kubectl get nodes   # shows the version
- all releases available at the releases page of the k8s github
  - "kubernetes.tar.gz"
  - all the control plane components are in the package
    - all control plane components have same version
      - kube-apiserver
      - controller-manager
      - kube-scheduler
      - kubelet
      - kube-proxy
      - kubectl
    - there are other components within the control-plane with different version numbers
      - those are the separate projects:
        - etcd cluster
        - coreDNS

- todo
  - check swagger tutorial
  - check the versioning references (per email)



6.05  cluster upgrade process
---
- core control-plane components do not have to be on the same version

- kube-apiserver 
  - is the primary component in the control-plane
  - version of all other components not allowed to be higher of the kube-apiserver version
    - e.g. 1.10

- controller-manager and kube-scheduler 
  - can be on one version lower
  - e.g. 1.9 or 1.10

- kubelet and kube-proxy 
  - can be on two versions below
  - e.g. 1.8, 1.9 or 1.10

- kubectl version 
  - can be higher of kube-apiserver
  - e.g. 1.9, 1.10 or 1.11

- the recommended approach is to upgrade one minor version at the time

- upgrade stratiegies
  - cloud (managed upgrades - few clicks)
  - kubeadm
    - kubectl upgrade plan
    - kubectl upgrade apply
  - the hard way (cluster deployed from scratch)
    - manual deployment

- the kubadm way
---
- steps 1.10 -> 1.11
  1] upgrade the master nodes
  2] upgrade the worker nodes

1] upgrade master
--
- the master node upgraded first
- components go down briefly
  - apiserver
  - scheduler
  - controller-managers

- when master down
  - worker nodes and applications on worker nodes (and running apps) are not affected
  - but all management functions are down, e.g.
    - access via kubectl or api
    - deploy new applications
    - modify existing apps
    - controller managers do not function
      - a failed pod will not be recreated

2] upgrade the worker nodesrecommended
1] master node upgrade
  $ kubeadm upgrade plan
  $ apt-get upgrade -y kubeadm=1.12.0-00
  $ kubeadm upgrade apply v1.12.0
  $ kubectl get nodes   # VERSION column shows kubelet ver
    - gets registered version of kubelets
    - kubeadm sets kubelets on the master node
    - if installed from scratch, kubelets are placed on nodes
      - the master node will not be shown as output of "k get nodes"
  $ apt-get upgrade -y kubelet=1.12.0-00
  $ systemctl restart kubelet
  $ kubectl get nodes   # VERSION
    - the version of master upgraded

2] worker nodes upgrade
  - node-1
    $ kubectl drain node-1
      - moves the workload from the node
      - cordons the node (and marks unschedulable)
    - then upgrade kubeadm and kubelet as done on the master
      $ apt-get upgrade -y kubeadm=1.12.0-00
      $ kubeadm upgrade apply v1.12.0
      $ apt-get upgrade -y kubelet=1.12.0-00
      $ kubeadm upgrade node config --kubelet-version v1.12.0   # different from the master???
      $ systemctl restart kubelet
      $ kubectl uncordon node-1
  - node-2
    $ kubectl drain node-2
      - as per node-1
  - node-N
    $ kubectl drain node-N
      - as per node-1



6.06 k8s upgrade - solution
---
- first i tried to do it by the book (after the theory lessons)
  - s wee little difference on killercoda
    $ kubeadm upgrade node
    $ apt upgrade kubelet
    $ systemctl restart kubelet
    $ kubectl get nodes


- check available versions
  $ apt-cache policy kubeadm
  $ apt-get install kubeadm=1.28.4

- check the cluster version
  $ kubectl get nodes
  $ kubectl version

- check the latest version available for the upgrade
  $ kubeadm upgrade plan


- THE UPGRADE
---
- upgrade master (controlplane)
--
- commands
  $ kubectl drain controlplane
    - status: Ready, SchedulingDisabled
  $ apt-cache policy kubeadm | grep 1.28.
  $ apt install kubeadm=1.28.2-00     # apt-get
  $ kubeadm version
  $ kubeadm upgrade apply v1.28.2     # this upgrades the cluster
    # [upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.28.2". Enjoy!
  $ kubectl version     # server=1.28.1
  $ kubectl get nodes   # server=1.28.1 
    - kubelet needs the manual upgrade
  $ apt install kubelet=1.28.2-00
  $ kubect get nodes
  $ kubectl uncordon controlplane
    - status: Ready


- upgrade worker (node01)
--
- commands
  $ kubectl drain node01
  $ ssh node01
    # apt-cache policy kubeadm | grep 1.28.2
  $ apt install kubeadm=1.28.2-00
  $ kubeadm upgrade node
  $ kubeadm install kubelet=1.28.2-00
  $ logout  # to master
    # kubectl get nodes
  $ kubectl uncordon node01


6.08 backup and restore methods
---
- resources
  - resource configuration
  - etcd cluster
  - persistent volumes

1] resource configuration
--
- declerative way
  - configs saved at source control

- get all resource configs from the cluster
  $ kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml

- for this (get all) you could use tools
  - VELERO  # former ARK by HeptIO


2] etcd
--
- state of the cluster
- nodes and other resources

- either:
  - backup the data directory by a backup tool
    - etcd.service
    ---
    ExecSTart=/usr/local/...
      --name ${ETCD_NAME} \\
      ..
      ..
      --data-dir=/var/lib/etcd

  - or, use the builtin snapshot solution
    - ETCDCTL_API=3 etcdctl \
        snapshot save snapshot.db
    $ etcdctl snapshot save snapshot.db
    $ etcdctl snapshot status snapshot.db
    
    - to restore
      $ service kube-apiserver stop
      $ etccdctl snapshot restore snapshot.db
        - this configures a new members to a new cluster
        - to prevent a new member accidentaly to join an existing cluster
        - new data-dir will be created
          - --data-dir /var/lib/etcs-from-backup
        - the new configuration for etcd.service id needed
          etcd.service
          --name ..
          --data-dir=/var/lib/etcd-from-backup
      $ systemctl daemon-reload
      $ service etcd restart
      $ service kube-apiserver start

    - with all etcdctl commands, you must specify
      --endpoints=https://127.0.0.1:2379  \
      --cacert=/etc/etcd/ca.crt \
      --cert=/etc/etcd/etcd-server.crt \
      --key=/etc/etcd/etcd-server.key

- if managed service, you probably do not have access to the etcd
  - then resource config files option is probably a better way




















