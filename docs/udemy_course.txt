course name:certified kubernetes admin course
tutor: mumshad mannambeth
location: Documents/2_EDU/kubernetes

would be good to do before:
kubernetes for absolute beginners


2. cluster architecture
---
cargo and control ships analogy


master nodelogin
--
- manage
- plan
- scheduler
- monitor nodes

worker nodes
--
host applications as containers


master node consists of
--
- etcd :: key-value format db
- kube-apiserver :: orchestrator
- kube-scheduler :: how to deploy containers
- kube controller manager
  - node controller
  - replication controller


worker nodes consist of
--
- kubelet :: "the capetan"
- kube-proxy :: communication between services & containers inside of the cluster

container runtime service ( e.g. docker or rkt (rocket) )



3. & 4. etcd
---
- port: 2379
- stores / has the data about the cluster
(nodes, pods, configs, secrets, accounts, roles, bindings, others)


2 types of k8s cluster
--
- installed from scratch
  - you have to install etcd-server by yourself
- kubeadm tool
  - deploys etcd-server as a pod in kube-system namespace
$ kubectl get pods -n kube-system


$ kubectl exec etcs-master -n kube-system
$ etcdctl get / --prefix -keys-only

- data is in the root directory
/registry/
- an then constructs such as
	- minions, pods, replicasets, deployments, roles, secrets


- in HA environment
--
- there will be multiple master nodes in the cluster
- there will be multiple etcd instances spread across the master nodes
- etcd instancs shall know about each other
  - "--initial-cluster" option shall be specified



5. kube-apiserver
---
- kubectl <-reaches-the-> kube-apiserver (authorization/validation) <-retreaves-data-from-> etcd cluster
- kube-apiserver then authentizates and validates request


workflow example: create a pod (not using kubectl, but per APIs)
---
1] apiserver then creates a pod object without assigning it to a node
# curl -X POST /api/v1/namespaces/default/pods ...[other]
- the request is authenticated
- the request is validated
- the data is retrieved
- etcd is updated
- the user gets back the data


2] scheduler
- cont. monitors apiserver and realizes there is a new pod without a new node assigned
- updates kube-apiserver where to put the pod

3] kubeapi-server
- updates etcd-cluster
- passes the info to kubelet on the appropriate worker node

4] kubelet
- creates the pod on the worker node
- instructs the CRE (container runtime engine) to deploy app image
- updates status to kube-apiserver

5] kube-apiserver
updates data back into etcd cluster


kube-apiserver responsibilities are to:
--
1] authentcate user
2] validate request
3] retrieve data
4] update etcd (only component that talks to etcd is kube-apiserver)
5] scheduler
6] kubelet


kube-apiserver options
--
- eg. internesting are
  - certificates :: connectivity between different components (eg. ca.pem, kubernetes.pem, ..)
    - --etcd-catfile=..; --etcd-certfile=..; --etcd-keyfile=..;
  - location of etcd-servers
    - etcd-servers=..


- viewing kube-apiserver options
--
- depends on how you set up your cluster
  - if kubeadmin tool
    - then kube-apiserver-master deployed as a pod in -namespace kube-system @ master
    - the options could be seen in manifest file
      - /etc/kubernetes/manifests/kube-apiserver.yaml
  - if no-kubeadmin setup
    - /etc/systemd/system/kube-apiserver.service
  - or list the process
    - on the master node
      - ps -aux | grep kube-apiserver



6. kube-controller-manager
---
- the role of controller process is to:
  - watch the status
  - remediate (correct, repair) situation

- controllers examples:
  - node-controller
    - node monitor period = 5 s
      - takes the state every 5 seconds)
    - node monitor grace period - 40 s
      - after heartbeat from node notreached, node marked as unreachable
    - pod eviction timeout = 5 min
      - if node deosn't come up, it is removed, another one is provisioned
  - replication-controller
    - monitors the state of replicaSets
    - ensuring the desired nr of pods available all the times within the set
  - deployment-controller
  - namespace-controller
  - endpoint-controller
  - job-controller
  - pv-protection-controller
  - pv-binder-controller
  - replication-controller
  - cronjob
  - stateful-set
  - replicaset


- installing kube-controll-manager
--
$ wget https://storage.googleapis.com/kubernetes-release/v1.13.0/bin/linux/amd64/kube-controller-manager
# run it as a service


- options are @ kube-controller-manager.service
  - abovementioned options are here
    - --node-monitor-period=5s
    - --node-monitor-grace-period=40s
    - --pod-eviction-timeout=5m0s
  - by default all controllers are enabled
  - if some of them are not, here is a good starting point to look at
    - -- controllers stringSlice

- view kube-controller-manager server options
--
1] kubeadmin setup:
- kubeadmin deploys kube controller manager
$ kubectl get pods -n kube-system
$ cat /etc/kubernetes/manifest/kube-controller-manager.yaml

2] no-kubeadmin setup
$ cat /etc/systemd/system/kube-controller-manager.service
- or
$ ps aux | grep kube-controller-manager



7. kube-scheduler
---
- (only) decides which pod goes on which node
- kubelet :: actually places the pod on the node

- how it's done
--
1] filter nodes (e.g. that do not have enough CPUs)
2] rank nodes (ammount of resources after the pod is placed)
3] more requirements come later ..


- installing kube-scheduler
--
wget http://storage.googleapics.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-scheduler
# extract it and run it as a service


- viewing kube-scheduler options
--
$ cat /etc/kubernetes/manifests/kube-scheduler.yaml	# pod definition file
# kubeadm :: pod in kube-system namespace on the master node

$ ps -aux | grep kube-scheduler



8. kubelet
---
- is like captain of the worker node
1] registers the node with the k8s cluster
2] creates the pod (requests the CRE to pull the image and run an instance)
3] monitors node & pods


- installing kubelet
--
# kubeadm does not deploy kubelets
# kubelet has to be always manually installed on worker nodes
wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kubelet

- viewing kubelet options
--
# listing the process on the worker node
ps -aux | grep kubelet



9. kube-proxy
---
- POD Network exists across all nodes

- kube-proxy
  - process that runs on each node
  - looks for new services
    - when service created
      - using iptables
      - creates rules on each node
      - to forward traffic from services to the backend pods


- installing kube-proxy
--
# download, extract it and run it as a service
$ wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-proxy

# kubeadm deploys kube-proxy as pod on each node
# it is deployd as a deamonset
$ kubectl get demonseat -n kube-system



10. pods
---
- the smallest k8s'object
- a single instance of an application
- pod encapsulates the deployed container

- scaling application happens via single container pods

- multi-container pods
--
- having hepler containers (supporting task for e.g. web app)
- all of them share
  - same network space (@ localhost)
  - same storage space


- installing (deploying) pods
--
$ kubectl run nginx --image nginx   # create pod
# the command deploys a docker container by creating a pod

$ kubectl get pods
# list of pods in our cluster




11. & 12. pods with yaml
---
- 4 toplevel properties:
  - apiVersion, kind, metadata, spec

- pod-definition.yaml
--
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx

$ kubectl create -f pod-definition.yaml
$ kubectl get pods
$ kubectl describe pod myapp-pod
$ kubectl delete deployment myapp-pod


13. & 14. & 15. practice test introduction, accessing labs and solution
---
- kodekloud

$ sudo apt install yamllint -y
$ kubectl run redis --image=redis --dry-run=client -o yaml > redis-pod.yaml
$ kubectl apply -f redis-pod.yaml
# now fix the pod definition (the upper definition is correct, nothing to fix there ...)
$ kubectl edit pod redis



16. & 17. recap - replicaSets
---
- reasons for replication:
  - HA
  - load balancing & scaling

- replication controller spans accross multiple nodes in the cluster
- notice the difference:
  - Replication Controller vs. Replica Set
    - they have same purpose, but they are not the same
    - Replication Controller is the older tehnology
    - Replica Set is the recommended way to set up the replication
    - ReplicaSet has to have "selector:" field

1] replication controller
--
rc-definition.yaml
---
apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
    type: frontend
spec:
  template:
    metadata:
      name: nginx
      labels:
        app: myapp
    spec:
      containers:
        - name: nginx-container
          image: nginx
  
  replicas: 3

$ kubectl create -f rs-definitionl.yaml
$ kubectl get replicationcontrollers
$ kubectl get pods

2] replicaset
rs-definition.yaml
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    app: myapp
    type: front-end

spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
  
  replicas: 3
  selector:
    matchLabels:
      type: front-end

$ kubectl get replicasets
$ kubectl get pods

- labels and selectors
  - in generall used to connect dots in kubernetes

- how to scale
1]
# set replicas: 6 in yaml file
$ kubectl replace -f rs-definition.yaml

2]
# either
$ kubectl scale --replicas=6 -f rs-definition.yaml
# this doesn't change the yaml definition file
# or
$ kubectl scale --replicas replicaset myapp-replicaset

- to racap:
$ kubectl get replicaset
$ kubectl get pods
$ kubectl delete replicaset myapp-replicaset    # deletes all underlying pods
$ kubectl replace -f replicaset-definition.yaml
$ kubectl scale --replicas=6 -f rs-definition.yaml
$ kubectl scale replicaset --replicas=5 myapp-replicaset

- replicaset ensures that the desired number of pods always run


18. & 19. deployments
---
- purposes:
  - many instances of server (tech stack)
  - upgrades
  - rolling updates
  - rollback
  - pause & resume changes

- pod < replicaSet < deployment

- deployment-definition.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx
  replicas: 3
  selector:
    matchLabels:
      type: front-end


$ kubectl create -f deployment-definition.yaml
$ kubectl get replicaset
$ kubectl get pods
$ kubectl get all

- to recap the file structure:
--
(aki mesec)
apiVersion:
kind:
metadata:
spec:

--
apiVersion:
kind:
(metanola)
metadata:
  name:
  labels:
(sxpek trese)
spec:
  (temple mesec)
  template:
    metadata:
    spec:
  replicas:
  selector:


- to create deployment imperative way:
$ kubectl create deployment httpd-frontend --replicas=3 --image=httpd:2.4-alpine
$ kubectl create deployment httpd-frontend --replicas=3 --image=httpd:2.4-alpine --dry-run=client -o yaml
# --dry-run=client -o yaml



20. namespaces
---
- isolation

- existing:
  - default
  - kube-system :: services, k8s resources ...
  - kube-public :: 

- to a namespace the following could be assigned:
  - policy
  - resources

- DNS is created
  - e.g. to reach the service:
    -inside of the namespace
      - db-service
    - in another namespace
      - db-service.dev.svc.cluster.local
  - cluster.local = domain
  - svc = subdomain (for a service)
  - dev = namespace
  - db-service = service name
        
$ kubectl get pods --namespace=kube-system

$ kubectl create -f pod-definition.yaml
$ kubectl create -f pod-definition.yaml --namespace=dev

- defined under "metadata" section

- pod-definition.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  namespace: dev
  labels:
    app: myapp
    type: front-end
spec:
  containers:
  - name: nginx-controller
    image: nginx

- how to create a new namespace:

1]
- namespace-dev.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: dev

$ kubectl create -f namespace-dev.yaml

2]
$ kubectl create namespace dev

- switch the working namespace
$ kubectl config set-context $(kubectl config current-context) --namespace=dev

$ kubectl get pods --all-namespaces   # across all namespaces


- to limit resources in a namespace create a resource quota:
compute-quota.yaml
---
apiVersion: v1
kind: ReqourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi

$ kubectl create -f compute-quota.yaml

$ kubectl get namespaces --no-headers | wc -l

# reach db-service in the marketing namespace
# db-service.marketing.svc.cluster.local



22. services & NodePort
---
- k8s object (like replicaSet, deployment, ..)
- it listens to a port on the node and to forward request on that port to a port on the node running the web application
- this is NodePort service

- ServiceTypes
--
- NodePort
  - forwards request node out <> in
- ClusterIP
  - creates virtual ip
- LoadBalancer
  - balances requests

- NodePort service
--
1] targetPort
  - the pod port
  - it has the IP address
2] port
  - the service port
  - it has the IP address (the ClusterIP of the service)
3] nodePort
  - range: 30000-327676

- service-definition.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
  - nodePort: 30008
    port: 80
    targetPort: 80
  selector:
    app: myapp
    type: front-end

- pod-definition.yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: myapp
    type: front-end
  name: myapp-pod
spec:
  containers:
  - image: nginx
    name: nginx-container

$ kubectl create -f service-definition.yaml
$ kubectl create -f pod-definition.yaml

- the service acts as a load balancer if more pods are targeted
  - random algorith
  - with seesionaffinity
- if pods are distributed accross multiple nodes
  - service than spans accross all nodes in the cluster
  - target node is same on all nodes in the cluster
    - curl 192.168.1.2:30008
    - curl 192.168.1.3:30008
    - curl 192.168.1.4:30008


23. service clusterIP
---
- the clusterIP reminds me on the "facade" programming pattern
- each service get the ip which is then used to access the services "accumulated" behind this ip
- this is the default service type!

service-definition.yaml
---
apiVersion: v1
kind: Service

metadata:
  name: back-end

spec:
  type: ClusterIP
  ports:
  - targetPort: 80
    port: 80

  selector:
    app: myapp
    type: back-end



24. service LoadBalancer
---
- this works only with supported platforms (AWS, googleCloud, azure, ...)

- service-definition.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: LoadBalancer
  ports:
  - targetPort: 80
    port: 80
    nodePort: 30008

- if the platform doesn't support the LoadBalancer type, the effect is same as setting the type to NodePort



25. solution - services 
---
$ kubectl get svc                   # get the number, type of services
$ kubectl describe svc kubernetes   # the target port, Labels, Endpoints for kuberneter service -> targetPort
$ kubectl get deployments           # how many deployments
$ kubectl describe deployment nginx | grep -i image         # the image for deployment
$ kubectl create deployment web-app --dry-run=client -o yaml --image=nginx > webapp-deployment.yaml   # creates the deployment
$ kubectl expose deployment webapp --name=webapp-service --target-port=8080 --type=NodePort --port=8080   # create a service out of the deployment
$ kubectl expose deployment webapp --name=webapp-service --target-port=8080 --type=NodePort --port=8080 --dry-run=client -o yaml > webapp-service.yaml  # create a service out of the deployment
  # edit the webapp-service.yaml and add "nodePort: 30080"



26. imperative vs declarative
---
- imperative
  - create and update objects
  - kubectl run/create/expose/edit/scale/set image/create -f/replace -f/ delete -f object
  - use imperative approach for the exam, to save the time
- declarative
  - create a set of files
  - "kubectl apply -f " command
    - apply command takes a look at the existing configuration and figures out what needs to be changed
  - if the exam requires more complicated objects/more containers, this approach is better


- imperative approach
--
- create objects per object configuration files
  $ kubectl create -f nginx.yaml
- update objects:
  $ kubectl edit deployment nginx.yaml
    - this changes will be applied to the live object only, and not to the definition file
  $ kubectl replace -f nginx.yaml
    - better approach is to change the definition file and to track the changes
  $ kubectl replace -f --force nginx.yaml
    - if you want to delete and replace objects

- declarative approach
--
- use the object configuration files to create and update objects
  $ kubectl apply -f nginx.yaml
  $ kubectl apply -f /path/to/config/files

- check kubernetes documentation
  - "manage kubernetes objects"   # documentation



27. imperative vs declarative - solutions
---
$ kubectl run nginx-pod --image=nginx:alpine

$ kubectl run redis --image=redis:alpine --labels="tier=db"
$ kubectl expose pod redis --port=6379 --target-port 6379 --cluster-ip='' --selector=tier=db --name=redis-svc
  # or
  $ kubectl expose pod redis --port=6379 --selector=tier=db --name=redis-svc
    # --cluster-ip= is the default service
$ kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3
  # or
  $ kubectl create deployment webapp --image=kodekloud/webapp-color
  $ kubectl scale deployment webapp --replicas=3

$ kubectl run custom-nginx --image=nginx --port=8080

$ kubectl create ns dev-ns
$ kubectl create deployment redis-deploy --namespace=dev-ns --image=redis --replicas=2

$ kubectl run httpd --image=httpd:alpine
$ kubectl expose pod httpd --name=httpd --cluster-ip='' --port=80 --target-port=80
  # or
  $ kubectl run httpd --image=httpd:alpine --port 80 expose (--dry-run=client -o yaml)



2.28. kubectl apply command
---
- kubectl compares
  - local configuration file
  - last applied configuration
  - a live configuration of the object definition on k8s
  
- kubectl apply -f file
  - yaml -> json
  - json location: k8s cluster in live object configuration
    - annotations:
      - kubectl.kubernetes.io/last-applied-configuration
  - kubectl create/replace commands do not store this configuration!
    - that's why do not mix imperative and declerative ways when managing k8s cluster

- last applied configuration
  - contains fields that were removed from 
    - local file 
    - and from the live k8s configuration

- check k8s documentation
  - "merging changes to primitive fields"   # documentation



3. scheduling
===

3.1. introduction
---

3.2. manual scheduling
---
- nodeName propery in the pod-definition.yaml
  - if it is not there, then this is is candidate for scheduling

- nodeName can be set per binding definition

pod-bind-definition.yaml
--
apiVersion: v1
kind: Binding
metadata:
  name:nginx
target:
  apiVersion: v1
  kind: Node
  name: node02

- then the data in json format is sent to the api
curl --header "Content-Type:application/json" \
  --request POST \
  --data '{"apiVersion":"v1", "kind": "Binding" ...}' \
  http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/



3.3. solution - manual scheduling
--
$ kubectl apply -f nginx.yaml
$ kubectl -n kube-system get pods

- how to disable scheduler: https://github.com/kubernetes/website/issues/21128
- find the part "We originally followed this tutorial: "

- after the scheduler was disabled, the nodeName has to be specified
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx
  name: nginx
spec:
  nodeName: node01
  containers:
  - image: nginx
    name: nginx

- by specifying the "nodeName", the need for scheduler has been circumvented


3.4. labels and selectors
---
- grouping
$ kubectl get pods --selector app=App1  # get pods with labels

- when creating objects
- selector/matchLabels must match template/metadata label

- annotations:
--
- e.g. tool details like name, version ..



3.5. labels and selectors - solution
---
$ kubectl get pods --show-labels
$ k get pods -l env=dev --no-headers
$ k get all --show-labels --no-headers -l env=dev
$ k get pods -l env=prod,bu=finance,tier=frontend



3.6. taints and tolerations
---
- node <-> taints
- pod <->  tolerations

- taints
--
$ kubectl taint nodes node-name color=blue:taint-effect
- taint-effect
  - noSchedule
  - PreferNoSchedule
  - NoExecute
$ kubectl taint nodes node01 app=blue:noSchedule


- tolerations
--
pod-definition.yaml
---
apiVersion: v1
kind: Pod
  name:myapp-pod
spec:
  containers:
  - name: nginx-container
    image: nginx

  toleartions:
  - key:"app"
    operator:"Equal"
    value:"blue"
    effect:"NoSchedule"

$ kubectl describe node controlplane | grep Taint



3.7. solution - taints and tolerations
---
$ kubectl explain pod --recursive | less
$ kubectl explain pod --recursive | grep -A5 tolerations

$ kubectl describe nodes node01 | grep -i taint
$ kubectl taint node master app=blue:NoSchedule-  # the minus removes the taint


controlplane $ k taint node node01 app=blue:NoSchedule
controlplane $ k apply -f nginx.yaml 
controlplane $ k get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP            NODE           NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          8s    192.168.0.8   controlplane   <none>           <none>
controlplane $ k delete pod nginx 
controlplane $ k taint node node01 app=blue:NoSchedule-
controlplane $ k apply -f nginx.yaml
controlplane $ k get pods -o wide
NAME    READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
nginx   0/1     ContainerCreating   0          7s    <none>   node01   <none>           <none>

- now set the toleration in the nginx.yaml definition
nginx.yaml 
---
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx

  tolerations: 
  - effect: "NoSchedule"
    key: "app"
    operator: "Equal"
    value: "blue"
  restartPolicy: Always

$ k delete pod nginx
$ k apply -f nginx.yaml

controlplane $ k get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          3s    192.168.1.4   node01   <none>           <none>

# runs again on the node01

$ taint node node01 app=blue:NoSchedule



3.8. node selectors
---
- node selectors based on the labels assigned to the node

$ kubectl label node node-name key=value  # to label the node
$ kubectl label nodes node01 size=Large

- selectors are exclusive (u can not say e.g. nodeSelector: "Large OR Medium" )
  - for that the nodeAffinity was introduced



3.9. node affinity & solutions
---
- required OR ignored
  - during scheduling
  - during execution
  - eg:
    - requiredDuringSchedulingIgnoredDuringExecution

$ kubectl get nodes node01 --show-labels
$ kubectl label nodes node01 color=blue
$ kubectl create deployment blue --image=nginx --replicas=6
$ kubectl get pods -o wide

- check kubernetes documentation
  - "Assign Pods to Nodes using Node Affinity"   # documentation
    - "Schedule a Pod using required node affinity"   # documentation
      https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/   # link



3.11. node affinity vs taints and tolerations
---
- how to place a combination of 
  - red, green, blue and other pods
  - onto the red, green, blue and other nodes

1] taint RGB nodes
2] tolerate RGB pods
3] affinity of other pods for other nodes



3.12. resource requirements and limits
---
- configure pods and containers   # documentation
  - kubernetes.io
  - https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/



3.13. resource requirements and limits - solutions
---
$ kubectl create -f ns-mem-example.yaml
$ kubectl apply -f pod-mem-example.yaml
$ kubectl get pod memory-demo -n mem-example
$ kubectl get pod memory-demo --output=yaml --namespace=mem-example
$ kubectl top pod memory-demo --namespace=mem-example
$ kubectl top pod memory-demo --namespace=mem-example
  # metrics API not available!!!
$ kubectl delete pod mem-example -n mem-example
$ kubectl config set-context $(kubectl config current-context) --namespace=mem-example   # set (move to) default namespace
$ kubectl apply -f pod-mem-example-2.yaml



3.14. deamon sets
---
- created by kube-api server
- ignored by kube-scheduler
- https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/  # documentation
- deamonSets run one copy of a pod on each node in the cluster
- use cases:
  - install agents
    - monitoring solutions
    - logs viewer
  - kube-proxy
  - weave-net (networking agent)

- similar to ReplicaSet, but with the kind of "DaemonSet"
- to create it programatically
  1] kubectl create deploy nginx --image=nginx --dry-run -o yaml > nginx-ds.yaml
  2] edit the nginx-ds.yaml
    - delete "replicas: 1" line
    - set "kind: DaemonSet" instead of "Deployment" kind

- how does this work?
  - set the nodeName property on each node and bypass the scheduler
    - this approach was used untill k8s v1.12
  - use NodeAffinity and defaultscheduler
    - since v1.12


3.15. daemonset - solutions
---
$ kubect create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 --dry-run=client -oyaml > ds-es.yaml
  # delete replicas, strategy
  # add the namespace
  # change kind Deployment -> DaemonSet



3.16. static pods
---
- created by kubelet
- ignored by kube-scheduler
- kubelet creates a static pod if a pod-definition yaml is placed under
  - /etc/kubernetes/manifests

- this way you cannot create replicasets, daemonsets, ...
- kubelet works on the pod level, so this is possible

- the pod definition path is defined either
  1] as the option to the kubelet.service
  - --pod-manifest-path=/etc/Kubernetes/manifests \\
  - or
  2] as the option of the config file to the kubelet.service
  - --config=kubeconfig.yaml  \\
    - kubeconfig.yaml
    ---
    staticPodPath: /etc/kubernetes/manifests
  - clusters setup using kubeadmin tool use this approach

- to check static pods use the command
  $ docker ps

- kube-apiserver is aware of both
  - static pods and 
  - pods created per kube-apiserver

- kube-apiserver
    - the command "kubectl get pods" returns static pods too
    - but this is only the read property
    - any change on static pods is done per static pod definition yaml file

- the k8s cluster could be set by placing k8s komponent definition files 
  to the manifests folders on the nodes
  - this is how kubeadmin tool does the setup of the k8s cluster
  - this is why you see pods when running
    $ kubectl get pods -n kube-system



3.17. static pods - solution
---
- where are the static pod definition files?
  - $ ps -ef | grep kubelet
  - $ ps aux | grep kubelet   # this worked for me
  - $ systemctl cat kubelet   # can this be used for checking the static pod manifests location?
  - check the "kubelet" setup for the 1.28! # todo
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/  # documentation
  - https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/    # documentation

$ kubectl run static-busybox --image=busybox --dry-run=client -oyaml --command -- sleep 1000 > static-busybox.yaml



3.18. multiple schedulers
---
- how to deploy a kube-scheduler -> installing kube-scheduler

1.a] wget
$ wget http://storage.googleapics.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-scheduler
# extract it and run it as a service
- kube-scheduler.service
  - --scheduler-name=default-scheduler  # if not specified, this is default name option
  - when deploying additional scheduler, set the --scheduler-name e.g. to
    - --scheduler-name=my-custom-scheduler

1.b] kubeadm tool
- make a copy (& rename) the file
  - /etc/kubernetes/manifests/kube-scheduler.yaml
- set the custom name as part of the command
  - --scheduler-name=my-custom-scheduler

2] create pod-definition.yaml
- the next step is to set the scheduler in pod definition
  - schedulerName: my-custom-scheduler
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx

  schedulerName: my-custom-scheduler

- how to know which scheduler picked it up?
$ kubectl get events  # check which scheduler picked up the pod

- how to view the logs
$ kubectl logs my-custom-scheduler --name-space=kube-system   # check logs of the custom scheduler











