course name:certified kubernetes admin course
tutor: mumshad mannambeth
location: Documents/2_EDU/kubernetes

would be good to do before:
kubernetes for absolute beginners


few links
---
- https://killer.sh/    # the best simulator
  - https://killer.sh/course/preview/e84d0e31-4fff-4c42-8afd-be1bdbc0d994
  - alias k=kubectl
  - export do="-o yaml --dry-run=client"
- the guys experience: https://georgearisty.dev/posts/ckad/
  - https://georgearisty.dev/posts/k8s-cluster-network/
---



2. cluster architecture
---
cargo and control ships analogy


master nodelogin
--
- manage
- plan
- scheduler
- monitor nodes

worker nodes
--
host applications as containers


master node consists of
--
- etcd :: key-value format db
- kube-apiserver :: orchestrator
- kube-scheduler :: how to deploy containers
- kube controller manager
  - node controller
  - replication controller


worker nodes consist of
--
- kubelet :: "the capetan"
- kube-proxy :: communication between services & containers inside of the cluster

container runtime service ( e.g. docker or rkt (rocket) )



3. & 4. etcd
---
- port: 2379
- stores / has the data about the cluster
(nodes, pods, configs, secrets, accounts, roles, bindings, others)


2 types of k8s cluster
--
- installed from scratch
  - you have to install etcd-server by yourself
- kubeadm tool
  - deploys etcd-server as a pod in kube-system namespace
$ kubectl get pods -n kube-system


$ kubectl exec etcs-master -n kube-system
$ etcdctl get / --prefix -keys-only

- data is in the root directory
/registry/
- an then constructs such as
	- minions, pods, replicasets, deployments, roles, secrets


- in HA environment
--
- there will be multiple master nodes in the cluster
- there will be multiple etcd instances spread across the master nodes
- etcd instancs shall know about each other
  - "--initial-cluster" option shall be specified



5. kube-apiserver
---
- kubectl <-reaches-the-> kube-apiserver (authorization/validation) <-retreaves-data-from-> etcd cluster
- kube-apiserver then authentizates and validates request


workflow example: create a pod (not using kubectl, but per APIs)
---
1] apiserver then creates a pod object without assigning it to a node
# curl -X POST /api/v1/namespaces/default/pods ...[other]
- the request is authenticated
- the request is validated
- the data is retrieved
- etcd is updated
- the user gets back the data


2] scheduler
- cont. monitors apiserver and realizes there is a new pod without a new node assigned
- updates kube-apiserver where to put the pod

3] kubeapi-server
- updates etcd-cluster
- passes the info to kubelet on the appropriate worker node

4] kubelet
- creates the pod on the worker node
- instructs the CRE (container runtime engine) to deploy app image
- updates status to kube-apiserver

5] kube-apiserver
updates data back into etcd cluster


kube-apiserver responsibilities are to:
--
1] authentcate user
2] validate request
3] retrieve data
4] update etcd (only component that talks to etcd is kube-apiserver)
5] scheduler
6] kubelet


kube-apiserver options
--
- eg. internesting are
  - certificates :: connectivity between different components (eg. ca.pem, kubernetes.pem, ..)
    - --etcd-catfile=..; --etcd-certfile=..; --etcd-keyfile=..;
  - location of etcd-servers
    - etcd-servers=..


- viewing kube-apiserver options
--
- depends on how you set up your cluster
  - if kubeadmin tool
    - then kube-apiserver-master deployed as a pod in -namespace kube-system @ master
    - the options could be seen in manifest file
      - /etc/kubernetes/manifests/kube-apiserver.yaml
  - if no-kubeadmin setup
    - /etc/systemd/system/kube-apiserver.service
  - or list the process
    - on the master node
      - ps -aux | grep kube-apiserver



6. kube-controller-manager
---
- the role of controller process is to:
  - watch the status
  - remediate (correct, repair) situation

- controllers examples:
  - node-controller
    - node monitor period = 5 s
      - takes the state every 5 seconds)
    - node monitor grace period - 40 s
      - after heartbeat from node notreached, node marked as unreachable
    - pod eviction timeout = 5 min
      - if node deosn't come up, it is removed, another one is provisioned
  - replication-controller
    - monitors the state of replicaSets
    - ensuring the desired nr of pods available all the times within the set
  - deployment-controller
  - namespace-controller
  - endpoint-controller
  - job-controller
  - pv-protection-controller
  - pv-binder-controller
  - replication-controller
  - cronjob
  - stateful-set
  - replicaset


- installing kube-controll-manager
--
$ wget https://storage.googleapis.com/kubernetes-release/v1.13.0/bin/linux/amd64/kube-controller-manager
# run it as a service


- options are @ kube-controller-manager.service
  - abovementioned options are here
    - --node-monitor-period=5s
    - --node-monitor-grace-period=40s
    - --pod-eviction-timeout=5m0s
  - by default all controllers are enabled
  - if some of them are not, here is a good starting point to look at
    - -- controllers stringSlice

- view kube-controller-manager server options
--
1] kubeadmin setup:
- kubeadmin deploys kube controller manager
$ kubectl get pods -n kube-system
$ cat /etc/kubernetes/manifest/kube-controller-manager.yaml

2] no-kubeadmin setup
$ cat /etc/systemd/system/kube-controller-manager.service
- or
$ ps aux | grep kube-controller-manager



7. kube-scheduler
---
- (only) decides which pod goes on which node
- kubelet :: actually places the pod on the node

- how it's done
--
1] filter nodes (e.g. that do not have enough CPUs)
2] rank nodes (ammount of resources after the pod is placed)
3] more requirements come later ..


- installing kube-scheduler
--
wget http://storage.googleapics.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-scheduler
# extract it and run it as a service


- viewing kube-scheduler options
--
$ cat /etc/kubernetes/manifests/kube-scheduler.yaml	# pod definition file
# kubeadm :: pod in kube-system namespace on the master node

$ ps -aux | grep kube-scheduler



8. kubelet
---
- is like captain of the worker node
1] registers the node with the k8s cluster
2] creates the pod (requests the CRE to pull the image and run an instance)
3] monitors node & pods


- installing kubelet
--
# kubeadm does not deploy kubelets
# kubelet has to be always manually installed on worker nodes
wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kubelet

- viewing kubelet options
--
# listing the process on the worker node
ps -aux | grep kubelet



9. kube-proxy
---
- POD Network exists across all nodes

- kube-proxy
  - process that runs on each node
  - looks for new services
    - when service created
      - using iptables
      - creates rules on each node
      - to forward traffic from services to the backend pods


- installing kube-proxy
--
# download, extract it and run it as a service
$ wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-proxy

# kubeadm deploys kube-proxy as pod on each node
# it is deployd as a deamonset
$ kubectl get demonseat -n kube-system



10. pods
---
- the smallest k8s'object
- a single instance of an application
- pod encapsulates the deployed container

- scaling application happens via single container pods

- multi-container pods
--
- having hepler containers (supporting task for e.g. web app)
- all of them share
  - same network space (@ localhost)
  - same storage space


- installing (deploying) pods
--
$ kubectl run nginx --image nginx   # create pod
# the command deploys a docker container by creating a pod

$ kubectl get pods
# list of pods in our cluster




11. & 12. pods with yaml
---
- 4 toplevel properties:
  - apiVersion, kind, metadata, spec

- pod-definition.yaml
--
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx

$ kubectl create -f pod-definition.yaml
$ kubectl get pods
$ kubectl describe pod myapp-pod
$ kubectl delete deployment myapp-pod


13. & 14. & 15. practice test introduction, accessing labs and solution
---
- kodekloud

$ sudo apt install yamllint -y
$ kubectl run redis --image=redis --dry-run=client -o yaml > redis-pod.yaml
$ kubectl apply -f redis-pod.yaml
# now fix the pod definition (the upper definition is correct, nothing to fix there ...)
$ kubectl edit pod redis



16. & 17. recap - replicaSets
---
- reasons for replication:
  - HA
  - load balancing & scaling

- replication controller spans accross multiple nodes in the cluster
- notice the difference:
  - Replication Controller vs. Replica Set
    - they have same purpose, but they are not the same
    - Replication Controller is the older tehnology
    - Replica Set is the recommended way to set up the replication
    - ReplicaSet has to have "selector:" field

1] replication controller
--
rc-definition.yaml
---
apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
    type: frontend
spec:
  template:
    metadata:
      name: nginx
      labels:
        app: myapp
    spec:
      containers:
        - name: nginx-container
          image: nginx
  
  replicas: 3

$ kubectl create -f rs-definitionl.yaml
$ kubectl get replicationcontrollers
$ kubectl get pods

2] replicaset
rs-definition.yaml
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    app: myapp
    type: front-end

spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
  
  replicas: 3
  selector:
    matchLabels:
      type: front-end

$ kubectl get replicasets
$ kubectl get pods

- labels and selectors
  - in generall used to connect dots in kubernetes

- how to scale
1]
# set replicas: 6 in yaml file
$ kubectl replace -f rs-definition.yaml

2]
# either
$ kubectl scale --replicas=6 -f rs-definition.yaml
# this doesn't change the yaml definition file
# or
$ kubectl scale --replicas replicaset myapp-replicaset

- to racap:
$ kubectl get replicaset
$ kubectl get pods
$ kubectl delete replicaset myapp-replicaset    # deletes all underlying pods
$ kubectl replace -f replicaset-definition.yaml
$ kubectl scale --replicas=6 -f rs-definition.yaml
$ kubectl scale replicaset --replicas=5 myapp-replicaset

- replicaset ensures that the desired number of pods always run


18. & 19. deployments
---
- purposes:
  - many instances of server (tech stack)
  - upgrades
  - rolling updates
  - rollback
  - pause & resume changes

- pod < replicaSet < deployment

- deployment-definition.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx
  replicas: 3
  selector:
    matchLabels:
      type: front-end


$ kubectl create -f deployment-definition.yaml
$ kubectl get replicaset
$ kubectl get pods
$ kubectl get all

- to recap the file structure:
--
(aki mesec)
apiVersion:
kind:
metadata:
spec:

--
apiVersion:
kind:
(metanola)
metadata:
  name:
  labels:
(sxpek trese)
spec:
  (temple mesec)
  template:
    metadata:
    spec:
  replicas:
  selector:


- to create deployment imperative way:
$ kubectl create deployment httpd-frontend --replicas=3 --image=httpd:2.4-alpine
$ kubectl create deployment httpd-frontend --replicas=3 --image=httpd:2.4-alpine --dry-run=client -o yaml
# --dry-run=client -o yaml



20. namespaces
---
- isolation

- existing:
  - default
  - kube-system :: services, k8s resources ...
  - kube-public :: 

- to a namespace the following could be assigned:
  - policy
  - resources

- DNS is created
  - e.g. to reach the service:
    -inside of the namespace
      - db-service
    - in another namespace
      - db-service.dev.svc.cluster.local
  - cluster.local = domain
  - svc = subdomain (for a service)
  - dev = namespace
  - db-service = service name
        
$ kubectl get pods --namespace=kube-system

$ kubectl create -f pod-definition.yaml
$ kubectl create -f pod-definition.yaml --namespace=dev

- defined under "metadata" section

- pod-definition.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  namespace: dev
  labels:
    app: myapp
    type: front-end
spec:
  containers:
  - name: nginx-controller
    image: nginx

- how to create a new namespace:

1]
- namespace-dev.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: dev

$ kubectl create -f namespace-dev.yaml

2]
$ kubectl create namespace dev

- switch the working namespace
$ kubectl config set-context $(kubectl config current-context) --namespace=dev

$ kubectl get pods --all-namespaces   # across all namespaces


- to limit resources in a namespace create a resource quota:
compute-quota.yaml
---
apiVersion: v1
kind: ReqourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi

$ kubectl create -f compute-quota.yaml

$ kubectl get namespaces --no-headers | wc -l

# reach db-service in the marketing namespace
# db-service.marketing.svc.cluster.local



22. services & NodePort
---
- k8s object (like replicaSet, deployment, ..)
- it listens to a port on the node and to forward request on that port to a port on the node running the web application
- this is NodePort service

- ServiceTypes
--
- NodePort
  - forwards request node out <> in
- ClusterIP
  - creates virtual ip
- LoadBalancer
  - balances requests

- NodePort service
--
1] targetPort
  - the pod port
  - it has the IP address
2] port
  - the service port
  - it has the IP address (the ClusterIP of the service)
3] nodePort
  - range: 30000-327676

- service-definition.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
  - nodePort: 30008
    port: 80
    targetPort: 80
  selector:
    app: myapp
    type: front-end

- pod-definition.yaml
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: myapp
    type: front-end
  name: myapp-pod
spec:
  containers:
  - image: nginx
    name: nginx-container

$ kubectl create -f service-definition.yaml
$ kubectl create -f pod-definition.yaml

- the service acts as a load balancer if more pods are targeted
  - random algorith
  - with seesionaffinity
- if pods are distributed accross multiple nodes
  - service than spans accross all nodes in the cluster
  - target node is same on all nodes in the cluster
    - curl 192.168.1.2:30008
    - curl 192.168.1.3:30008
    - curl 192.168.1.4:30008


23. service clusterIP
---
- the clusterIP reminds me on the "facade" programming pattern
- each service get the ip which is then used to access the services "accumulated" behind this ip
- this is the default service type!

service-definition.yaml
---
apiVersion: v1
kind: Service

metadata:
  name: back-end

spec:
  type: ClusterIP
  ports:
  - targetPort: 80
    port: 80

  selector:
    app: myapp
    type: back-end



24. service LoadBalancer
---
- this works only with supported platforms (AWS, googleCloud, azure, ...)

- service-definition.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: LoadBalancer
  ports:
  - targetPort: 80
    port: 80
    nodePort: 30008

- if the platform doesn't support the LoadBalancer type, the effect is same as setting the type to NodePort



25. solution - services 
---
$ kubectl get svc                   # get the number, type of services
$ kubectl describe svc kubernetes   # the target port, Labels, Endpoints for kubernetes service -> targetPort
$ kubectl get deployments           # how many deployments
$ kubectl describe deployment nginx | grep -i image         # the image for deployment
$ kubectl create deployment web-app --dry-run=client -o yaml --image=nginx > webapp-deployment.yaml   # creates the deployment
$ kubectl expose deployment webapp --name=webapp-service --target-port=8080 --type=NodePort --port=8080   # create a service out of the deployment
$ kubectl expose deployment webapp --name=webapp-service --target-port=8080 --type=NodePort --port=8080 --dry-run=client -o yaml > webapp-service.yaml  # create a service out of the deployment
  # edit the webapp-service.yaml and add "nodePort: 30080"



26. imperative vs declarative
---
- imperative
  - create and update objects
  - kubectl run/create/expose/edit/scale/set image/create -f/replace -f/ delete -f object
  - use imperative approach for the exam, to save the time
- declarative
  - create a set of files
  - "kubectl apply -f " command
    - apply command takes a look at the existing configuration and figures out what needs to be changed
  - if the exam requires more complicated objects/more containers, this approach is better


- imperative approach
--
- create objects per object configuration files
  $ kubectl create -f nginx.yaml
- update objects:
  $ kubectl edit deployment nginx.yaml
    - this changes will be applied to the live object only, and not to the definition file
  $ kubectl replace -f nginx.yaml
    - better approach is to change the definition file and to track the changes
  $ kubectl replace -f --force nginx.yaml
    - if you want to delete and replace objects

- declarative approach
--
- use the object configuration files to create and update objects
  $ kubectl apply -f nginx.yaml
  $ kubectl apply -f /path/to/config/files

- check kubernetes documentation
  - "manage kubernetes objects"   # documentation



27. imperative vs declarative - solutions
---
$ kubectl run nginx-pod --image=nginx:alpine

$ kubectl run redis --image=redis:alpine --labels="tier=db"
$ kubectl expose pod redis --port=6379 --target-port 6379 --cluster-ip='' --selector=tier=db --name=redis-svc
  # or
  $ kubectl expose pod redis --port=6379 --selector=tier=db --name=redis-svc
    # --cluster-ip= is the default service
$ kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3
  # or
  $ kubectl create deployment webapp --image=kodekloud/webapp-color
  $ kubectl scale deployment webapp --replicas=3

$ kubectl run custom-nginx --image=nginx --port=8080

$ kubectl create ns dev-ns
$ kubectl create deployment redis-deploy --namespace=dev-ns --image=redis --replicas=2

$ kubectl run httpd --image=httpd:alpine
$ kubectl expose pod httpd --name=httpd --cluster-ip='' --port=80 --target-port=80
  # or
  $ kubectl run httpd --image=httpd:alpine --port 80 expose (--dry-run=client -o yaml)



2.28. kubectl apply command
---
- kubectl compares
  - local configuration file
  - last applied configuration
  - a live configuration of the object definition on k8s
  
- kubectl apply -f file
  - yaml -> json
  - json location: k8s cluster in live object configuration
    - annotations:
      - kubectl.kubernetes.io/last-applied-configuration
  - kubectl create/replace commands do not store this configuration!
    - that's why do not mix imperative and declerative ways when managing k8s cluster

- last applied configuration
  - contains fields that were removed from 
    - local file 
    - and from the live k8s configuration

- check k8s documentation
  - "merging changes to primitive fields"   # documentation



3. scheduling
===

3.1. introduction
---

3.2. manual scheduling
---
- nodeName propery in the pod-definition.yaml
  - if it is not there, then this is is candidate for scheduling

- nodeName can be set per binding definition

pod-bind-definition.yaml
--
apiVersion: v1
kind: Binding
metadata:
  name:nginx
target:
  apiVersion: v1
  kind: Node
  name: node02

- then the data in json format is sent to the api
curl --header "Content-Type:application/json" \
  --request POST \
  --data '{"apiVersion":"v1", "kind": "Binding" ...}' \
  http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/



3.3. solution - manual scheduling
--
$ kubectl apply -f nginx.yaml
$ kubectl -n kube-system get pods

- how to disable scheduler: https://github.com/kubernetes/website/issues/21128
- find the part "We originally followed this tutorial: "

- after the scheduler was disabled, the nodeName has to be specified
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: nginx
  name: nginx
spec:
  nodeName: node01
  containers:
  - image: nginx
    name: nginx

- by specifying the "nodeName", the need for scheduler has been circumvented


3.4. labels and selectors
---
- grouping
$ kubectl get pods --selector app=App1  # get pods with labels

- when creating objects
- selector/matchLabels must match template/metadata label

- annotations:
--
- e.g. tool details like name, version ..



3.5. labels and selectors - solution
---
$ kubectl get pods --show-labels
$ k get pods -l env=dev --no-headers
$ k get all --show-labels --no-headers -l env=dev
$ k get pods -l env=prod,bu=finance,tier=frontend



3.6. taints and tolerations
---
- node <-> taints
- pod <->  tolerations

- taints
--
$ kubectl taint nodes node-name color=blue:taint-effect
- taint-effect
  - noSchedule
  - PreferNoSchedule
  - NoExecute
$ kubectl taint nodes node01 app=blue:noSchedule


- tolerations
--
pod-definition.yaml
---
apiVersion: v1
kind: Pod
  name:myapp-pod
spec:
  containers:
  - name: nginx-container
    image: nginx

  toleartions:
  - key:"app"
    operator:"Equal"
    value:"blue"
    effect:"NoSchedule"

$ kubectl describe node controlplane | grep Taint



3.7. solution - taints and tolerations
---
$ kubectl explain pod --recursive | less
$ kubectl explain pod --recursive | grep -A5 tolerations

$ kubectl describe nodes node01 | grep -i taint
$ kubectl taint node master app=blue:NoSchedule-  # the minus removes the taint


controlplane $ k taint node node01 app=blue:NoSchedule
controlplane $ k apply -f nginx.yaml 
controlplane $ k get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP            NODE           NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          8s    192.168.0.8   controlplane   <none>           <none>
controlplane $ k delete pod nginx 
controlplane $ k taint node node01 app=blue:NoSchedule-
controlplane $ k apply -f nginx.yaml
controlplane $ k get pods -o wide
NAME    READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
nginx   0/1     ContainerCreating   0          7s    <none>   node01   <none>           <none>

- now set the toleration in the nginx.yaml definition
nginx.yaml 
---
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx

  tolerations: 
  - effect: "NoSchedule"
    key: "app"
    operator: "Equal"
    value: "blue"
  restartPolicy: Always

$ k delete pod nginx
$ k apply -f nginx.yaml

controlplane $ k get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
nginx   1/1     Running   0          3s    192.168.1.4   node01   <none>           <none>

# runs again on the node01

$ taint node node01 app=blue:NoSchedule



3.8. node selectors
---
- node selectors based on the labels assigned to the node

$ kubectl label node node-name key=value  # to label the node
$ kubectl label nodes node01 size=Large

- selectors are exclusive (u can not say e.g. nodeSelector: "Large OR Medium" )
  - for that the nodeAffinity was introduced



3.9. node affinity & solutions
---
- required OR ignored
  - during scheduling
  - during execution
  - eg:
    - requiredDuringSchedulingIgnoredDuringExecution

$ kubectl get nodes node01 --show-labels
$ kubectl label nodes node01 color=blue
$ kubectl create deployment blue --image=nginx --replicas=6
$ kubectl get pods -o wide

- check kubernetes documentation
  - "Assign Pods to Nodes using Node Affinity"   # documentation
    - "Schedule a Pod using required node affinity"   # documentation
      https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/   # link



3.11. node affinity vs taints and tolerations
---
- how to place a combination of 
  - red, green, blue and other pods
  - onto the red, green, blue and other nodes

1] taint RGB nodes
2] tolerate RGB pods
3] affinity of other pods for other nodes



3.12. resource requirements and limits
---
- configure pods and containers   # documentation
  - kubernetes.io
  - https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/



3.13. resource requirements and limits - solutions
---
$ kubectl create -f ns-mem-example.yaml
$ kubectl apply -f pod-mem-example.yaml
$ kubectl get pod memory-demo -n mem-example
$ kubectl get pod memory-demo --output=yaml --namespace=mem-example
$ kubectl top pod memory-demo --namespace=mem-example
$ kubectl top pod memory-demo --namespace=mem-example
  # metrics API not available!!!
$ kubectl delete pod mem-example -n mem-example
$ kubectl config set-context $(kubectl config current-context) --namespace=mem-example   # set (move to) default namespace
$ kubectl apply -f pod-mem-example-2.yaml



3.14. deamon sets
---
- created by kube-api server
- ignored by kube-scheduler
- https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/  # documentation
- deamonSets run one copy of a pod on each node in the cluster
- use cases:
  - install agents
    - monitoring solutions
    - logs viewer
  - kube-proxy
  - weave-net (networking agent)

- similar to ReplicaSet, but with the kind of "DaemonSet"
- to create it programatically
  1] kubectl create deploy nginx --image=nginx --dry-run -o yaml > nginx-ds.yaml
  2] edit the nginx-ds.yaml
    - delete "replicas: 1" line
    - set "kind: DaemonSet" instead of "Deployment" kind

- how does this work?
  - set the nodeName property on each node and bypass the scheduler
    - this approach was used untill k8s v1.12
  - use NodeAffinity and defaultscheduler
    - since v1.12


3.15. daemonset - solutions
---
$ kubect create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 --dry-run=client -oyaml > ds-es.yaml
  # delete replicas, strategy
  # add the namespace
  # change kind Deployment -> DaemonSet



3.16. static pods
---
- created by kubelet
- ignored by kube-scheduler
- kubelet creates a static pod if a pod-definition yaml is placed under
  - /etc/kubernetes/manifests

- this way you cannot create replicasets, daemonsets, ...
- kubelet works on the pod level, so this is possible

- the pod definition path is defined either
  1] as the option to the kubelet.service
  - --pod-manifest-path=/etc/Kubernetes/manifests \\
  - or
  2] as the option of the config file to the kubelet.service
  - --config=kubeconfig.yaml  \\
    - kubeconfig.yaml
    ---
    staticPodPath: /etc/kubernetes/manifests
  - clusters setup using kubeadmin tool use this approach

- to check static pods use the command
  $ docker ps

- kube-apiserver is aware of both
  - static pods and 
  - pods created per kube-apiserver

- kube-apiserver
    - the command "kubectl get pods" returns static pods too
    - but this is only the read property
    - any change on static pods is done per static pod definition yaml file

- the k8s cluster could be set by placing k8s komponent definition files 
  to the manifests folders on the nodes
  - this is how kubeadmin tool does the setup of the k8s cluster
  - this is why you see pods when running
    $ kubectl get pods -n kube-system



3.17. static pods - solution
---
- where are the static pod definition files?
  - $ ps -ef | grep kubelet
  - $ ps aux | grep kubelet   # this worked for me
  - $ systemctl cat kubelet   # can this be used for checking the static pod manifests location?
  - check the "kubelet" setup for the 1.28! # todo
  - https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/  # documentation
  - https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/    # documentation

$ kubectl run static-busybox --image=busybox --dry-run=client -oyaml --command -- sleep 1000 > static-busybox.yaml



3.18. multiple schedulers
---
- how to deploy a kube-scheduler -> installing kube-scheduler

1.a] wget
$ wget http://storage.googleapics.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-scheduler
# extract it and run it as a service
- kube-scheduler.service
  - --scheduler-name=default-scheduler  # if not specified, this is default name option
  - when deploying additional scheduler, set the --scheduler-name e.g. to
    - --scheduler-name=my-custom-scheduler

1.b] kubeadm tool
- make a copy (& rename) the file
  - /etc/kubernetes/manifests/kube-scheduler.yaml
- set the custom name as part of the command
  - --scheduler-name=my-custom-scheduler

2] create pod-definition.yaml
- the next step is to set the scheduler in pod definition
  - schedulerName: my-custom-scheduler
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx

  schedulerName: my-custom-scheduler

- how to know which scheduler picked it up?
$ kubectl get events  # check which scheduler picked up the pod

- how to view the logs
$ kubectl logs my-custom-scheduler --name-space=kube-system   # check logs of the custom scheduler



3.19. multiple schedulers - solution
---
- https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/   # documentation
$ kubectl create -f my-kube-scheduler.yaml
- Failed to pull image "gcr.io/my-gcp-project/my-kube-scheduler:1.0": failed to pull and unpack image "gcr.io/my-gcp-project/my-kube-scheduler:1.0": failed to resolve reference "gcr.io/my-gcp-project/my-kube-scheduler:1.0": failed to authorize: failed to fetch anonymous token: unexpected status: 400 Bad Request

$ watch "kubectl get pods"



3.20. configuring scheduler
---
- manually (wget) vs. kubeadm tool (kubectl)
- ~0.45 shows advanced scheduling options
- https://github.com/kubernetes/community/tree/master/contributors/devel
            - the lower one didnt exist anymore
                - https:github.com/kubernetes/community/blob/master/contributors/devel/scheduler.md
- https://kubernetes.io.blog/2017/03/advanced-scheduling-in-kubernetes/
- https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/
- https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work



4.2. logging and monitoring
---
- heapster is deprecated

- metrics server (heapster's slimmed down version)
  - in-memory performance data
    - no historical performance data

- kubelet (agent) generates the metrics
  - cAdvisor component (containers advisor)
    - retrieve performance metrics from pods
    - exposes them through the kubelet api
      - makes it available to the metrics server

- to deploy metrics-server
---
- minikube environment:
$ minikube addons enable metrics-server   # if you use minikube

- all other environments:
$ git clone https://github.com/kubernetes-incubator/metrics-server.git
$ kubectl create -f deploy/1.8+/

- probably up-to-date info:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml   # install metrics server

- to check performance (cpu / memory)
$ kubectl top node
$ kubectl top pod



4.4. managing application logs
---
$ docker run kodekloud/event-simulator    # event-simulator
$ docker logs -f ecf  # ???

$ k run event-simulator-pod --image=kodekloud/event-simulator --dry-run=client -oyaml > event-simulator.yaml
$ k create -f event-simulator.yaml
$ k logs -f event-simulator-pod

- if more containers inside of pod, then the container from 
  which i'd like to read the logs must be specified

- e.g. i have event-simulator + nginx inside of the pod
  $ kubectl logs -f event-simulator-pod event-simulator   # read the logs from event-simulator



4.5. logging - solution
---
- if the pod has more containers, -c option shows containers
  $ kubectl logs webapp-2 -c simple-webapp  # gives the logs of simple-webapp container in the pod



5.1. application lifecycle management
---
- mentioned already in KCD (kubernetes certified developer course)



5.2. rolling updates and rollbacks
---
- creating a deployment triggers a rolout = deployment revision (function of app/container version)
- a next, new rolout creates new deployment revision (app version)

- deployment status
$ kubectl rollout status deployment/myapp-deployment    # deployment status

- revision and rollout history 
$ kubectl rollout history deployment/myapp-deployment   # revision and rollout history

- deployment strategies
  - recreate strategy
    - destroy & create instances(versions)
    - application downtime
    - not a default strategy
  - rolling update
    - replace instance(version) by instance
    - no downtime
    - default deployment strategy
strategy:    

- how to update deployment
  - k apply command
    $ kubectl apply -f deployment-definition.yaml
      - creates new rollout + new revision
  - k set image
    $ kubectl set image deployment/myapp-depl nginx=nginx:1.9.1
      - whis will result in different config of definition file and deployment

$ kubectl describe
  - gives deployment detailed infomation

- upgrades could be followed watching replicasets
  $ kubectl get replicasets

- rollback of deployment
  $ kubectl rollout undo deployment/myapp-deployment  # rollback deployment
  $ kubectl get replicasets   # before and after deployment

- to summarize deployment commands
$ kubectl create -f deployment-definition.yaml  # create deployment
$ kubectl get deployments
$ kubectl apply -f deployment-definition.yaml   # update deployment
$ kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1
$ kubectl rollout status deployment/myapp-deployment
$ kubectl rollout history deployment/myapp-deployment
$ kubectl rollout undo deployment/myapp

- edit the deployment
$ kubectl edit deployement web-app
  - strategy: recreate  # for example

- FYI - not default option is:
  strategy:    
    type: Recreate



5.4. application commands
---
- not part of exam

- docker image like
---
FROM Ubuntu

ENTRYPOINT ["sleep"]

CMD ["5"]

$ docker run ubuntu-sleeper 10
$ docker run --entrypoint sleep2.0 ubuntu-sleeper 10
  - sleep2.0 is the "new" (another) sleep application 



5.5. application commands and arguments
---
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      command: ["sleep2.0"]
      args: ["10"]

- similarity to docker
  - command ["sleep"] <> ENTRYPOINT ["sleep"]
  - args ["10"] <> CMD ["10"]



5.6. commands and arguments - solution
---
- interesting was flask
  - is this a web server/web app?

1] 
  
Dockerfile (kodekloud/webapp-color)
---
FROM python3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python","app.py"]

CMD ["--color", "red"]


webapp-color-pod.yaml
---
image: kodekloud/webapp-color
command: ["--color","green"]


2] web-app.yaml (from kodekloud)
video @ ~9.00
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: webapp-green
  name: webapp-green
spec:
  containers:
    args: [ "--color=green" ]
    image: kodekloud/webapp-color
    name: webapp-green
    resources: {}
  restartPolicy: Never

gives:
Error from server (BadRequest): error when creating "webapp-green.yaml": Pod in version "v1" cannot be handled as a Pod: json: cannot unmarshal object into Go struct field PodSpec.spec.containers of type []v1.Container



5.7. configure environment variables in applications
---
- use the "env:" property
- env is an array
  -> every property in the array starts with the dash "-"

- key value pair
env:
  - name:
    value:

- ConfigMap (use "valueFrom")
env:
  - name: APP_COLOR
    valueFrom:
      configMapKeyRef:

- Secrets (use "valueFrom")
env:
  - name: APP_COLOR
    valueFrom:
      secretKeyRef:



5.8. configure config map
---
- imperative
  $ kubectl create configmap 

- declerative
  $ kubectl create -f

- imperative
$ kubectl create configmap \
  app-config --from-literal=APP_COLOR=blue  \
              -- from literal=APP_MOD=prod

- from a file
kubectl create configmap \
  app-config  --from-file=app_config.properties


- declarative:
config-map.yaml
---
apiVersion: v1
data:
  APP_COLOR: blue
  APP_MODE: prod
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: app-config

$ kubectl create -f config-map.yaml

- view config map
  $ kubectl get configmaps
  $ kubectl describe configmaps config-map

- now you have to configure this in the pod
spec:
  envFrom:
  - configMapRef:
    name: app-config


- in general, 3 ways of injecting configmap
  1] env
    envFrom:
      - configMapRef:
        name: app-config

  2] single env
    env:
      - name: APP_COLOR
        valueFrom:
          configMapKeyRef:
            name: app-config
            key: APP_COLOR

  3] VOLUME
    volumes:
    - name: app-config-volume
      configMap:
        name: app-config



5.9. environment variables - solution
---
- overall, this part i need to repeat more thoroughly
- https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/

- configure microservices using configMap and secrets
  - https://kubernetes.io/docs/tutorials/configuration/configure-java-microservice/configure-java-microservice/

- katacoda shutter
  - https://kubernetes.io/blog/2023/02/14/kubernetes-katacoda-tutorials-stop-from-2023-03-31/



5.10. configure secrets in applications
---
- create a secret
  - kubectl create secret generic   # imperative
  - kubectl create -f               # declarative

- imperative
  - options
    --from-literal
    --from-file

- declerative
  - secret-data.yaml
  ---
  apiVersion: v1
  kind: Secret
  metadata:
    name: app-secret [A]
  data:
    DB_Host: mysql
    DB_User: root
    DB_Password: passwrd

- but, for the data values use
$ echo -n 'mysql' | base64
$ echo -n 'root' | base64
$ echo -n 'passwrd' | base64

$ kubectl create -f secret-data.yaml
$ kubectl get secrets
$ kubectl describe secrets
$ kubectl get secret app-secret -o yaml

- to decode the values
$ echo -n 'bXlzcWw=' | base64 --decode


- inject into pod

pod-definition
---
apiVersion: v1
kind: Pod
...
...
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
      - containerPort: 8080
    envFrom:
      - secretRef:
            name: app-secret [A]

$ kubectl create -f pod-definition.yaml


- more ways of injecting secrets into pods
1] from secrets (ENV)
2] SINGLE ENV
3] VOLUME

- volume
  -name: app-secret-volume
  - creates a file for each secret
  $ ls opt/app-secret-volumes



5.11. secrets - solutions
---
$ kubectl create secret generic db-secret \
--from-literal=DB_Host=sql01 \
--from-literal=DB_User=root \
--from-literal=DB_Password=password123 \
--dry-run=client -oyaml


db-secret.yaml
---
apiVersion: v1
data:
  DB_Host: c3FsMDE=
  DB_Password: cGFzc3dvcmQxMjM=
  DB_User: cm9vdA==
kind: Secret
metadata:
  creationTimestamp: null
  name: db-secret


$ kubectl explain pods --recursive | grep envFrom -A5
- input
  envFrom:
    - secretRef:
          name: db-secret


 webapp-pod.yaml
 ---
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: webapp-pod
  name: webapp-pod
spec:
  containers:
  - image: kodekloud/simplewebapp-mysql
    name: webapp-pod
    envFrom:
    - secretRef:
        name: db-secret


5.12. multi container pods
---
- multicontainer pods share
  - lifecycle
  - network
  - storage

spec:
  containers:
  - name: simple-webapp

- the dash "-" marks that containers are an "array"

- You can use following command to delete the POD forcefully
  $ kubectl delete pod <PODNAME> --grace-period=0 --force



5.13. multi container pods - solutions
---
- sidecar container 
  -> sends logs to elasticsearch



5.14. init containers - solutions
---
- what are init containers?



6.01 cluster maintenance
---

6.02 os updates
---
- if pods part of replicaset
  - pods from a node down are deleted and recreated on another node 
    after the eviction-timeout
  - --pod-eviction-timeout=5m0s   # default

- in theory, some update could be done by taking the node down for < 5min
  - but, you can never tell if you're loose pods (not part of the replicaSet)

- how this shall be done: drain the node
  $ kubectl drain node-1      # "drain" moves pods to another nodes
    - drained node is unschedulable
  # do the maintenance (take down the node), and e.g. reboot
  $ kubectl uncordon node-1   # uncordoned node is now schedulable

- instead of drain you can mark the node unschedulable with "cordon"
  $ kubectl cordon node-1   # marks as unschedulable; doesn't drain



6.03 os updates - solution
---
- daemonSets can not be deleted (drained)
  - --ignore-daemonsets

- play around with taints, i suppose



6.04 kubernetes software versions (releases)
---
$ kubectl get nodes   # shows the version
- all releases available at the releases page of the k8s github
  - "kubernetes.tar.gz"
  - all the control plane components are in the package
    - all control plane components have same version
      - kube-apiserver
      - controller-manager
      - kube-scheduler
      - kubelet
      - kube-proxy
      - kubectl
    - there are other components within the control-plane with different version numbers
      - those are the separate projects:
        - etcd cluster
        - coreDNS

- todo
  - check swagger tutorial
  - check the versioning references
      https://github.com/kubernetes/community/tree/master/contributors/design-proposals
      https://github.com/kubernetes/design-proposals-archive/blob/main/release/versioning.md
      https://github.com/kubernetes/design-proposals-archive/blob/main/api-machinery/api-group.md
      https://blog.risingstack.com/the-history-of-kubernetes/
      https://kubernetes.io/releases/version-skew-policy/



6.05  cluster upgrade process
---
- core control-plane components do not have to be on the same version

- kube-apiserver 
  - is the primary component in the control-plane
  - version of all other components not allowed to be higher of the kube-apiserver version
    - e.g. 1.10

- controller-manager and kube-scheduler 
  - can be on one version lower
  - e.g. 1.9 or 1.10

- kubelet and kube-proxy 
  - can be on two versions below
  - e.g. 1.8, 1.9 or 1.10

- kubectl version 
  - can be higher of kube-apiserver
  - e.g. 1.9, 1.10 or 1.11

- the recommended approach is to upgrade one minor version at the time

- upgrade stratiegies
  - cloud (managed upgrades - few clicks)
  - kubeadm
    - kubectl upgrade plan
    - kubectl upgrade apply
  - the hard way (cluster deployed from scratch)
    - manual deployment

- the kubadm way
---
- steps 1.10 -> 1.11
  1] upgrade the master nodes
  2] upgrade the worker nodes

1] upgrade master
--
- the master node upgraded first
- components go down briefly
  - apiserver
  - scheduler
  - controller-managers

- when master down
  - worker nodes and applications on worker nodes (and running apps) are not affected
  - but all management functions are down, e.g.
    - access via kubectl or api
    - deploy new applications
    - modify existing apps
    - controller managers do not function
      - a failed pod will not be recreated

2] upgrade the worker nodesrecommended
1] master node upgrade
  $ kubeadm upgrade plan
  $ apt-get upgrade -y kubeadm=1.12.0-00
  $ kubeadm upgrade apply v1.12.0
  $ kubectl get nodes   # VERSION column shows kubelet ver
    - gets registered version of kubelets
    - kubeadm sets kubelets on the master node
    - if installed from scratch, kubelets are placed on nodes
      - the master node will not be shown as output of "k get nodes"
  $ apt-get upgrade -y kubelet=1.12.0-00
  $ systemctl restart kubelet
  $ kubectl get nodes   # VERSION
    - the version of master upgraded

2] worker nodes upgrade
  - node-1
    $ kubectl drain node-1
      - moves the workload from the node
      - cordons the node (and marks unschedulable)
    - then upgrade kubeadm and kubelet as done on the master
      $ apt-get upgrade -y kubeadm=1.12.0-00
      $ kubeadm upgrade apply v1.12.0
      $ apt-get upgrade -y kubelet=1.12.0-00
      $ kubeadm upgrade node config --kubelet-version v1.12.0   # different from the master???
      $ systemctl restart kubelet
      $ kubectl uncordon node-1
  - node-2
    $ kubectl drain node-2
      - as per node-1
  - node-N
    $ kubectl drain node-N
      - as per node-1



6.06 k8s upgrade - solution
---
- first i tried to do it by the book (after the theory lessons)
  - s wee little difference on killercoda
    $ kubeadm upgrade node
    $ apt upgrade kubelet
    $ systemctl restart kubelet
    $ kubectl get nodes


- check available versions
  $ apt-cache policy kubeadm
  $ apt-get install kubeadm=1.28.4

- check the cluster version
  $ kubectl get nodes
  $ kubectl version

- check the latest version available for the upgrade
  $ kubeadm upgrade plan


- THE UPGRADE
---
- upgrade master (controlplane)
--
- commands
  $ kubectl drain controlplane
    - status: Ready, SchedulingDisabled
  $ apt-cache policy kubeadm | grep 1.28.
  $ apt install kubeadm=1.28.2-00     # apt-get
  $ kubeadm version
  $ kubeadm upgrade apply v1.28.2     # this upgrades the cluster
    # [upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.28.2". Enjoy!
  $ kubectl version     # server=1.28.1
  $ kubectl get nodes   # server=1.28.1 
    - kubelet needs the manual upgrade
  $ apt install kubelet=1.28.2-00
  $ kubect get nodes
  $ kubectl uncordon controlplane
    - status: Ready


- upgrade worker (node01)
--
- commands
  $ kubectl drain node01
  $ ssh node01
    # apt-cache policy kubeadm | grep 1.28.2
  $ apt install kubeadm=1.28.2-00
  $ kubeadm upgrade node
  $ kubeadm install kubelet=1.28.2-00
  $ logout  # to master
    # kubectl get nodes
  $ kubectl uncordon node01


6.08 backup and restore methods
---
- resources
  - resource configuration
  - etcd cluster
  - persistent volumes

1] resource configuration
--
- declerative way
  - configs saved at source control

- get all resource configs from the cluster
  $ kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml

- for this (get all) you could use tools
  - VELERO  # former ARK by HeptIO


2] etcd
--
- state of the cluster
- nodes and other resources

- either:
  - backup the data directory by a backup tool
    - etcd.service
    ---
    ExecSTart=/usr/local/...
      --name ${ETCD_NAME} \\
      ..
      ..
      --data-dir=/var/lib/etcd

  - or, use the builtin snapshot solution
    - ETCDCTL_API=3 etcdctl \
        snapshot save snapshot.db
    $ etcdctl snapshot save snapshot.db
    $ etcdctl snapshot status snapshot.db
    
    - to restore
      $ service kube-apiserver stop
      $ etccdctl snapshot restore snapshot.db
        - this configures a new members to a new cluster
        - to prevent a new member accidentaly to join an existing cluster
        - new data-dir will be created
          - --data-dir /var/lib/etcs-from-backup
        - the new configuration for etcd.service id needed
          etcd.service
          --name ..
          --data-dir=/var/lib/etcd-from-backup
      $ systemctl daemon-reload
      $ service etcd restart
      $ service kube-apiserver start

    - with all etcdctl commands, you must specify
      --endpoints=https://127.0.0.1:2379  \
      --cacert=/etc/etcd/ca.crt \
      --cert=/etc/etcd/etcd-server.crt \
      --key=/etc/etcd/etcd-server.key

- if managed service, you probably do not have access to the etcd
  - then resource config files option is probably a better way



6.10 backup and restore - solution
---
- check the etcd version
  $ kubect describe -n kube-system pod etcd-controlplane | grep -i image

- what ports to reach etcd cluster from master node
  --listen-client-urls=https://127.0.0.1:2379,https://172.17.0.12:2379
    - 2379

- for other nodes to join the etcd cluster
  --listen-peer-urls=https://172.17.0.12:2380
    - 2380
  
- the location of the server cert file
  --cert-file=/etc/kubernetes/pki/etcd/server.crt

- the location of the etcd ca.cert
  - --truested-ca-file=/etc/kubernetes/pki/etcd/ca.cert
  - or is it the
  - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - same location

- create the backup using snapshoot tool
  - ETCDCTL_API=3 etcdctl version


$ # ETCDCTL_API=3 etcdctl --cert="" --cacert="" snapshot save
  $ kubectl describe -n kube-system pod etcd-controlplane | less
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
$ ETCDCTL_API=3 etcdctl --cert="/etc/kubernetes/pki/etcd/server.crt" --cacert="/etc/kubernetes/pki/etcd/ca.crt" --key="/etc/kubernetes/pki/etcd/server.key" snapshot save /opt/snapshot-pre-boot.db
  - enpoint option not needed -> this is run on the same server where etcd is installed

- snapshot status
  $ ETCDCTL_API=3 etcdctl --cert="/etc/kubernetes/pki/etcd/server.crt" --cacert="/etc/kubernetes/pki/etcd/ca.crt" --key="/etc/kubernetes/pki/etcd/server.key" snapshot status /opt/snapshot-pre-boot.db
  - or simply
  $ ETCDCTL_API=3 etcdctl snapshot status /opt/snapshot-pre-boot.db

- snapshot restore
  - todo: this didnt work, possibly because of the version!
  $ ETCDCTL_API=3 etcdctl snapshot restore /opt/snapshot-pre-boot.db --data-dir=/var/lib/etcd-from-backup
  - then update etcd.yaml
    - /etc/kubernetes/manifests/etcd.yaml
    - mount the "volumes / hostPath"
      - path: /var/lib/etc-from-backup
    - save and exit



7.5 TLS basics
---
~6.30 - most interesting
- PKI = public key infrastructure



7.6 tls in kubernetes
---
- server components
  - kube-apiserver
    - apiserver.crt + apiserver.key
    - possible to create a service specific certificates/keys
      - apiserver-etcd-client.crt + apiserver-etcd-client.key
  - etcd server
    - etcdserver.crt + etcdserver.key
  - kubelet server
    - kubelet.crt + kubelet.key

- clients, from the perspective of k8s, are
  - admin user
    - admin.crt + admin.key
  - scheduler
    - scheduler.crt + scheduler.key
  - controller-manager
    -  controller-manager.crt +  controller-manager.key
  - kube-proxy
    - kube-proxy.crt + kube-proxy.key

- k8s wants u to have CA (certificate authority) for the cluster
  - possible to have more than one CA
    - one for etcd specifically
      -this would sign etcd (crt+key) & api-etcd (crt+key)
    - one for all components in the cluster



7.7 certificates creation
---
- diff tools available
  - easyrsa
  - openssl
  - cfssl

1] create CA key
--
- generate keys
  $ openssl genrsa -out ca.key 2048
    - ca.key
- certificate signing request
  $ openssl req -new -key ca.key -subj 
    "/CN=KUBERNETES-CA" -out ca.csr
      - ca.csr
      - CN=...  # Common Name 
      - the name of the component the certificate is for
- sign certificates
  $ openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt
    - ca.crt
    - this ca.crt will be needed from all components in k8s!

2] client-side certificates 
(admin user)
--
- generate keys
  $ openssl genrsa -out admin.key 2048
    - admin.key
- certificate signing request
  $ openssl req -new -key admin.key -subj \
    "/CN=kube-admin/O=system:masters" -out admin.csr
      - admin.csr
      - O=system:masters  # group:SYSTEM:MASTERS
- sign certificate
  $ openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt
    - admin.crt

- same procedure for all other components
  - admin user
  - kube-scheduler
  - kube-controller-manager
  - kube-proxy

- kube scheduler and kube controller manager 
  - they are in control plane
  - their name must be prefixed with "SYSTEM:"
    - "SYSTEM:KUBE-SCHEDULER"
    - "SYSTEM:KUBE-CONTROLLER-MANAGER"

- what you do with certificates?
  - e.g. admin => use instead uname/pwd
    - $ curl https://kube-apiserver:6443/api/v1/pods  \
      --key admin.key --cert admin.crt  \
      --cacert ca.crt
    
    - or put details in kube-config.yaml
      apiVersion: v1
      clusters:
      - cluster
          certificate-authority: ca.crt
          server: https://kube-apiserver:6443
        name: kubernetes
      kind: Config
      users:
      - name: kubernetes-admin
        user:
          client-certificate: admin.crt
          client-key: admin.key

3] server-side certificates
--

- etcd servers
  - the common name CN=ETCD_SERVER
- in HA cluster more etcd servers possible
  - etcdpeer1.crt & etcdpeer1.key
  - CN=ETCD-PEER
  - all have to be specified in etcd.yaml
    - --peer-cert-file
    - ca.crt has to be specified!

- kube-apiserver
  - the most popular component
  - KUBE-API SERVER
  - all different names have to be present
    - kubernetes
    - kubernetes.default
    - kubernetes.default.svc
    - kubernetes.default.svc.cluster.local
    - 10.96.0.1
    - 172.17.0.87
  
  - generate certificates
    $ openssl genrsa -out apiserver.key 2048
      - apiserver.key
    $ openssl req -new -key apiserver.key -subj \
    "/CN=kube-apiserver" -out apiserver.csr
      - apiserver.csr
      - create openssl.cnf for alternative DNS entries
        [alt_names]
        DNS.1 = kubernetes
        DNS.2 = kubernetes.default
        DNS.3 = kubernetes.default.svc
        DNS.4 ...
    $ openssl x509 -req -in apiserver.csr \
    -CA ca.crt -CAkey ca.key -out apiserver.crt
    # sign the certificate
      - apiserver.crt

- kubelet
--
- kubectl nodes (server cert)
  - all nodes need key-cert pair
  - CN names will be: node01, node02, node03
  - use created certificates in the kubelet-config.yaml file
      kind: KubeletConfiguration
      ...
      tlsCertFile: "/var/lib/kubelet/kubelet-node01.crt"
      tlsPrivateKeyFile: "/var/lib/kubelet/kubelet-node01.key"
  - do this for all certificates

- kubectl (client cert)
--
- used for kubeapi-server authentication
- names
  - CN=system:node:node01
  - CN=system:node:node02
  - CN=system:node:node03
- group: system:nodes



7.8 view certificates
---
- it depends on the setup of cluster 
  - the hard way, or
    - /etc/systems/system/kube-apiserver.service
  - kubeadm
    - /etc/kubernetes/manifests/kube-apiserver.yaml

- create a spreadsheet for all certificates: kubeadm.certificates.csv

kubeadm.certificates.csv
---
- first row:
  - component, type, certificate path, cn name, alt names, organization, issuer, expiration

- component column, type, ... (columns):
  - kube-apiserver, server, ...
    kube-apiserver, server, ...
    kube-apiserver, server, ...
    kube-apiserver, client (kubelet), ...
    kube-apiserver, client (kubelet), ...
    kube-apiserver, client (etcd), ...
    kube-apiserver, client (etcd), ...
    kube-apiserver, client (etcd), ...

- kubeadm setup: look in the kube-apiserver definition file
  - /etc/kubernetes/manifests/kube-apiserver.yaml
   - --client-ca-file
   - --etcd-cafile
   - --etcd-certfile
   - --etcd-keyfile
   - --kubelet-client
   - --kubelet-client-key
   - --tls-cert-file
   - --tls-private-key-file

  - check inside of each certificate for the details
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
  
  - decode the details
    $ openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout
      - Subject: CN=kube-apiserver
      - X509v3 Subject Alternative Name:
      - validity: Not After : Feb 11 ... 2020
      - Issuer: CN=kubernetes

- certificate requirements listed in k8s docu
  - https://kubernetes.io/docs/setup/best-practices/certificates/

- if you run into issues, check logs
  - if you setup the cluster yourself, with native services
    - then use the OS logging functionality
      $ journalctl -u etcd.service -l
      # ClientTLS: cert
      #
  - if you setup the cluster using kubeadm
    - then use k8s logging functionality
      $ kubectl logs etcd-master
      # ClientTLS: cert
      #
    - if the k8s functionality doesn't work (e.g. pod issues)
      $ docker ps -a    # get the CONTAINER ID (e.g. 87fc6991)
      $ docker logs 87fc
      # ClientTLS: cert



7.9 manage certificates (certificate api)
---
- kubeadm creates and stores certificates on master node
  - CA Server = master node

- certificates api
  - manages certificates
  - sign requests
  - rotate certificates

- cert sign request is sent to kubernetes through api call
- how this works
  - the user creates a key
  - an admin receives a sign request
  - an admin creates CertificateSigningRequest object
  - these requests can be reviewed and approved (kubectl commands)
  - certificate can be shared with the user (after it is extracted)

- step-by-step workflow
  - the user creates a key
    $ openssl genrsa -out jane.key 2048
  - the user sends the request to the administrator
    $ openssl req -new -key jane.key -subj "/CN=jane" -out jane.csr
  - out of the key, the admin creates CertificateSigningRequest object
    - cat jane.csr | base64
    - jane-csr.yaml
    ---
    apiVersion: certificates.k8s.io/v1beta1
    kind: CertificateSigningRequest
    metadata:
      name: jane
    spec:
      groups:
      - system: authenticated
      usages:
      - digital signature
      - key encipherment
      - server auth
      request:
        LS0tLS1CRU ...
        ...
        ... base64 encoded jane.csr ... ...
        ...
        ... ... ... ... ... ... 41TXVxOTlOZbnJ
  - all cert signing requests can be seen by administrators
    $ kubectl get csr
      - NAME
      - CONDITION = Pending
  - admin can approve the request
    $ kubectl certificate approve jane
  - k8s signs the certificates & generates a certificate for the user
    - using ca key-pair
  - certificate extracted and shared with user
    $ kubectl get csr jane -o yaml
      apiVersion: certificates.k8s.io/v1beta1
      status:
        certificate:
      LS0tL ...
      ..
      .. ... ... LQo=
    $ echo "LS0 ... LQo=" | base64 --decode
      -----BEGIN CERTIFICATE -----
      MIICWDCC
      ...
      -----END CERTIFICATE -----
  - the decoded certificate can than be shared with the end user

- this everything is cone by controller-manager
  - csr-approving
  - csr-signing

- /etc/kubernetes/manifests/kube-controller-manager.yaml
 - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
 - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key



7.10 KubeConfig
---
$ curl https://my-kube:6443/api/v1/pods \
  --key admin.key
  --cert admin.crt
  --cacert ca.crt

- kubectl config file location
  - $HOME/.kube/config

- sections
    clusters:
    - name: kubernetes
    - name: development
    - name: production

    contexts:
    - name: kubernetes-admin@kubernetes
    - name: prod-user@production

    users:
    - name: kubernetes-admin
    - name: prod-user

- to change context is give the user right for the cluster

- to change the context
  $ kubectl config use-context prod-user@production
    - changes the $HOME/.kube/config file
    - "current-context: prod-user@production"
    
- $ kubectl config -h   # check

- namespaces could be set for context
  contexts:
  - name: admin@production
    context:
      cluster: production
      user: admin
      namespace: finance

- certificates in KubeConfig could be specified using
  - certificate-authority: /etc/kubernetes/pki/ca.crt
    # or
  - certificate-authority-data: LS0t .. BASE64 encoded .. PbnJ


- play around jack
  - prepare needed certs
  $ cat ~/.kube/config
  $ echo "LS0 ... LS0tCg==" | base64 --decode > admin.crt
  $ echo "LS0 ... LS0tCg==" | base64 --decode > admin.key

  $ kubectl get pods
    - $ curl https://172.30.1.2:6443/api/v1/pods \
        --key admin.key \
        --cert admin.crt  \
        --cacert /etc/kubernetes/pki/ca.crt

- rewind it and play it again
  $ kubectl version
    - $ curl https://172.30.1.2:644310.96.0.1i/ca.crt

- check the master node DNS entry
  $ openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout
    - gives:
      DNS:controlplane
      DNS:kubernetes
      DNS ...
  $ kubectl version
    - $ curl https://controlplane:6443/version \
        --key admin.key \
        --cert admin.crt  \
        --cacert /etc/kubernetes/pki/ca.crt



7.11 API groups
---
- existing
  - api:
    /metrics
    /healthz
    /version
    /api
    /apis
    /logs

- for the cluster functionality responsible are
  - /api    # core
  - /apis   # named
    $ 

- named
  /apis
    - API Groups:
      /apps
        /v1
          - Resources:  # aka nouns
            /deployments
              - Verbs:  # aka actions
                /list
                /get
                /create
                /delete
            /replicasets
            /statefulsets
      /networking.k8s.io
        /v1
          /networkpolicies

- REST looks like
  - named/API Groups/version/Resources/Verbs
    - resources = nouns
    - verbs = actions

  - Resources are the nouns of the Web 
    - they describe any object, document, or thing that 
      you may need to store or send to other services

- documentation
  - https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#pod-v1-core

- infos could be got by exec curl localhost:6433
  - prepare needed certs
    $ cat ~/.kube/config
    $ echo "LS0 ... LS0tCg==" | base64 --decode > admin.crt
    $ echo "LS0 ... LS0tCg==" | base64 --decode > admin.key

  $ curl https://controlplane:6443 \
      --key admin.key \
      --cert admin.crt  \
      --cacert /etc/kubernetes/pki/ca.crt
    #
    - returns "paths"
  
  $ curl https://controlplane:6443/api \
      --key admin.key \
      --cert admin.crt  \
      --cacert /etc/kubernetes/pki/ca.crt

- instead of authentication with keys
  - curl https:://... --key ... 
  - you can start
    $ kubectl proxy
      # Starting to serve on 127.0.0.1:8001
    $ curl http://localhost:8001

- kube-proxy != kubectl proxy
  !!!



7.12 authorization
---
!= authentication
  - authentication = access

- authorization
  - what can you do

- authorization mechanisms
  - node
  - ABAC  # attribute based authorization
  - RBAC  # role based authorization
  - Webhook
  - AlwaysAllow
  - AlwaysDeny

- node based autorization
--
- kubelet + kube API
  - system:node:node01

- node authorizer
  - authorizes any request comming from a user with 
      - a name system:node
      - and a part of the system:node group

- ABAC
--
- external access
- a user has the set of permissions
  - can view pod
  - can create pod
  - can delete pod
- this is done per a policy file with a set of policies
  - this file is passed to the API server
  - after change, the kubeAPI-server has to be restarted
    - difficult to manage

- RBAC
--
- role with a set of permissions
  - a user is then asocciated to the role
- (more) standard approach of maanging k8s

- Webhook
--
- outsourcing of all authorization mechanisms
- e.g. Open Policy Agent

- where is this setup
  - kube-apiserver.yaml
    -- authorization-mode=AlwaysAllow   # per default
  - multiple modes possible
    -- authorization-mode=Node,RBAC,Webhook
      - this is the order or authorization (1.Node, then 2.RBAC, then 3.Webhook)
        - e.g. deny, deny, allow = allow



7.13 RBAC
---

1] create the role
  developer-role.yaml
  ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      name: developer
    rules:
    - apiGroups: [""]
      resources: ["pods"]
      verbs: ["list","get","create","update","delete"]
    - apiGroups: [""]
      resources: ["ConfigMap"]
      verbs: ["create]

  - for the "core" group the "apiGroups: [""]" can stay empty
  - multiple rules (upper example) could be given for single role

  - $ kubectl create -f developer-role.yaml

2] bind the user to the role 
  devuser-developer-binding.yaml
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    name: devuser-developer-binding
  subjects:
  - kind: user
    name: dev-user
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: Role
    name: developer
    apiGroup: rbac.authorization.k8s.io

  $ kubectl create -f devuser-developer-binding.yaml

- this role + role binding is inside of the default namespace

- get the roles and bindings info
  $ kubectl get roles
  $ kubectl get rolebindings
  $ kubectl describe role developer
  $ kubectl describe rolebinding devuser-developer-binding

- get the user rights
  $ kubectl auth can-i create deployments
    - yes 
  $ kubectl auth can-i delete nodes
- check for other users (as admin)
  $ kubectl auth can-i create deployments --as dev-user
  $ kubectl auth can-i create pods --as dev-user
  $ kubectl auth can-i create pods --as dev-user --namespace test

- resource names could also be restricted
  - resourceNames: ["blue", "orange"]   # e.g. pods are resources



7.14 cluster roles and roles bindings
---
- nodes can not be grouped in the namespace
  - nodes are cluster-wide resources

- resources are either
  - namespaced
  - cluster scoped

- namespaced resources
  $ kubectl api-resources --namespaced=true 
    - pods
    - replicasets
    - jobs
    - deployments
    - ...

- cluster scoped
  $ kubectl api-resources --namespaced=false
    - nodes
    - PV
    - clusterroles
    - clusterrolebindings
    - certificatesigningrequests
    - namespaces

- cluster roles have to be created, e.g.
  - cluster-admin (create, view, delete nodes)
  - storage-admin (view PVs, create PVs, delete PVs)

- to create a cluster role
  - cluster-admin-role.yaml
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  matadata:
    name: cluster-administrator
  rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["list", "get", "create", "delete"]

  $ kubectl create -f cluster-admin-role.yaml

  - cluster-admin-role-binding.yaml
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: cluster-admin-role-binding
  subjects:
  - kind: user
    name: cluster-admin
    apiGroup: rbac.authorization.k8s.io
  roleRef:
    kind: ClusterRole
    name: cluster-administrator
    apiGroup: rbac.authorization.k8s.io
  
  $ kubectl create -f cluster-admin-role-binding.yaml

- cluster role could be created for a namespaced resource
  - the user in that case has the access across all namespaces
  - e.g. authorize the user with ClusterRole to access all pods accross the whole cluster

- k8s creates many cluster roles by default when it's set-up

# !!!
$ kubectl api-resources -o wide # get the roles and verbs



7.15 service accounts
---
$ kubectl create serviceaccount dashboard-sa
$ kubectl get serviceaccount
$ kubectl describe serviceaccount dashboard-sa
  - Tokens: dashboard-sa-token-kbbdm
    - Name (dashboard-sa) -> creates object ->token in secret -> connects to the service account
$ kubectl describe secret dashboard-sa-token-kbbdm
  - token:
    eyJhb...iwia3
- this can be used with curl to make a REST call
  $ curl https://192.168.56.70:6443/api -insecure \
    --header "Authorization: Bearer eyJhb...iwia"
      # "eyJhb ... iwia" <--TOKEN

- but what if the application is on the k8s itself?
  - then mount the secret token as a volume inside the pod where the app runs

- each namespace has the "default" service account already created and mounted
  $ kubectl get serviceaccount
    - default sa and its token are mounted as a volume to the pod
  $ kubectl describe pod my-kubernetes-dashboard
    Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-j4hkv
    SecretName: default-token-j4hkv
  $ kubectl exec -it my-kubernetes-dashboard ls /var/run/secrets/kubernetes.io/serviceaccount
    # ca.crt namespace token
  $ kubectl exec -it my-kubernetes-dashboard cat /var/run/secrets/kubernetes.io/serviceaccount

- the default sa is very restricted
  - todo: check permissions!

- to use additional service account (dashboard-sa), specify in pod-definition.yaml
    ...
    spec:
      containers:
        - name: my-kubernetes-dashboard
          image: my-kubernetes-dashboard
      serviceAccount: dashboard-sa

- pod has to be deleted and recreated (if you change the service account)
- if deployment, a new rollout will be triggered

- not to mount default service account automatically, specify:
  automountServiceAccountToken: false



7.16 image security
---
- image:    docker.io/nginx/nginx
          ( registry/user_account/image_repository )

- popular registries:
  - docker.io   # docker registry
    gcr.io      # google registry
      - gcr.io/kubernetes-e2e-test-images/dnsutils

- private repository (the docker way)
  $ docker login private-registry.io  # login first
  $ docker run private-regitry.io/apps/internal-app   # then run

- private repo (the k8s way)
  - create a secret object )(of type "docker-registry")
    $ kubectl create secret docker-registry regcred \
      --docker-server= private-registry.io  \
      --docker-username= registry-user  \
      --docker-password=  registry-password \
      --docker-eamil= user@org.com

  - use the "regcred" secret inside of pod-definition.yaml
    imagePullSecrets:
    - name: regcred


7.17 security contexts
---
- can be defined on pod or on container level
- if defined on container level, this overrides the pod level

- security-context-pod-level.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: web-pod
spec:
  securityContext:
    runAsUser: 1000

  containers:
    - name: ubuntu
      image: ubuntu
      command: ["sleep","3600"]


- security-context-container-level.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: web-pod
spec:
  containers:
    - name: ubuntu
      image: ubuntu
      command: ["sleep","3600"]
      securityContext:
        runAsUser: 1000


- to add capabilities (supported on container level only!)
---
apiVersion: v1
kind: Pod
metadata:
  name: web-pod
spec:
  containers:
    - name: ubuntu
      image: ubuntu
      command: ["sleep","3600"]
      securityContext:
        runAsUser: 1000
        capabilities:
          add:  ["MAC_ADMIN"]



7.18 network policy
---
- ingress vs. egress
  - the direction of the traffic origin is relevant
    - user -> web app -> backend app -> db

- network security in k8s network
  - per default all pods can comunicate with each other

- network policy
  - if i want to prevent communication for some pods
  - network policy is applied to a pod

- the rule:
  allow
  Ingress
  traffic
  from
  API Pod
  on
  Port 3306
---
- network-policy.yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          name: api-pod
    ports:
    - protocol: TCP
      port: 3306

$ kubectl create -f network-policy.yaml

- Flannel doesn't support network policies!

- solutions that support network policies:
  - kube-router
  - calico
  - romana
  - weave-net

- if cluster doesn't support newtwork policy, they are still createable
  - they won't work
  - no error message will be thrown

- There is no imperative command to extract yaml instead of that you can copy template from official k8s docs.
  - https://kubernetes.io/docs/concepts/services-networking/network-policies/#networkpolicy-resource  # docu
  - https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/   # docu

- Installing Romana with kubeadm 
  - https://github.com/romana/romana/tree/master/containerize   # docu



7.19 developing network policies
---
- 3 availableselectors for (ingress) traffic
  - from:
    - podSelector (matchLabels)
    - namespaceSelector (matchLabels)
    - ipBlock (cidr: 192.168.5.10/32)

- if allow ingress from API pod to DB pod
  - then no need to specify egress on DB
    - this will work automatically
      - but, if DB Pod needs to make calls to API pod then egress needs to be defined

- policy example AND / OR
---
policyTypes:
- Ingress
ingress:
- from:
  - podSelector:
      matchLabels:
        name: api-pod
    namespaceSelector:  # AND !!!
      matchLabels:
        name: prod
  - ipBlock:            # OR  !!!
      cidr: 192.168.5.10/32

- policy example OR / OR
---
ingress:
- from:
  - podSelector:        # OR !!!
      matchLabels:
        name: api-pod
  - namespaceSelector:  # OR !!!
      matchLabels:
        name: prod
  - ipBlock:            # OR  !!!
      cidr: 192.168.5.10/32


- egress:
  - when DB wnats to reach outside service
    - e.g. backup server

- policy example:
---
policyTypes:
- Ingress
- Egress
ingress:
- from:
  - podSelector:
      matchLabels:
        name: api-pod
  ports:
  - protoclo: TCP
    port: 3306
egress:
- to
  - ipBlock:
      cidr: 192.168.5.10/32



7.20 network policies
---
$ kubectl get netpol    # get network policies
$ kubectl get pods -l name=payroll
$ kubectl describe netpol payroll-policy


~5.45
  - create network policy



8.02 docker storage intro
---
- 2 types
  - storage drivers
  - volume drivers



8.03 container storage interface
---
- k8s was working on docker at the beginning
- later on, important toopen doors for other container solutions like
  - rkt
  - cri-o 
- so CRI (container runtime interface) was built

- similarly CNI (container networking interface) introduced
  - networking vendors like
    - weaveworks
    - flannel
    - cilium

- CSI = container storage interface
  - CSI is not a k8s standard
    - currently onboard are
      - k8s
      - cloud foundry
      - mesos
  - vendors like
    - portworkx
    - amazon ebs
    - dell emc
    - glusterFS


- more on the subject
  - https://github.com/container-storage-interface/spec???



8.6 volumes
---
- different storage solutions
  - should be used for multinode
  - dir mounts will assume all nodes have the same mounted dir (with the same content)
    - which is not true



8.7 persistent volumes
---

- large portion of storage defined
  - that is, persistent volume
  - this defined by administrator

- users then carve out and take what they need
  - using persisten volumes

- https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes
- Types of Persistent Volumes
  - PersistentVolume types are implemented as plugins. 
    Kubernetes currently supports the following plugins:
      - csi - Container Storage Interface (CSI)
      - fc - Fibre Channel (FC) storage
      - hostPath - HostPath volume (for single node testing only; WILL NOT WORK in a multi-node cluster; consider using local volume instead)
      - iscsi - iSCSI (SCSI over IP) storage
      - local - local storage devices mounted on nodes.
      - nfs - Network File System (NFS) storage


pv-definition.yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol1
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 1Gi 
  hostPath:
    path: /tmp/data

$ kubectl create -f pv-definition.yaml
$ kubectl get PersistentVolume

- you can replace "hostPath" with a vendor/storage type


pv-definition.yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol1
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 1Gi 
  awsElasticBlockStore:
    volumeID: <volume-id>
    fsType: ext4



8.7 persistent volume claim
---
- this defined by users
- it uses persistent volume
- persistent volume and persistent volume claims are different k8s objects
  - pv
  - pvc

- pvc are bounded to pv's based on
  - properties
    - sufficient capacity
    - access modes
    - volume modes
    - storage class
  - or labels & selectors
    selector:
      matchLabels:
        name: my-pv

- 1:1 relationship of claims & volumes
  - if pvc < pv capacity, then no other claim can take the rest of pv's capa

- pvc pending if no pv's are available


pvc-definition
---
apiVersino: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    ReadWriteOnce

  resources:
    requests:
      storage: 500Mi

$ k create -f pvc-definition.yaml
$ k get Persistentvolumeclaim
$ k delete persistenvolumeclaim myclaim   # delete persistent volume claim
  - after pvc delition, the pv stays per default
    - persistentVolumeReclaimPolicy: Retain
      - manual delition needed
    - or
      - persistentVolumeReclaimPolicy: Delete
        - automatically deleted
    - or
      - persistentVolumeReclaimPolicy: Recycle
        - the data in PV will be scrubbed after pvc delition


8.9 persistent volumes and persistent volume claims solution
---
- check the logs in the pod "webapp"
  $ kubectl exec webapp -- cat /log/app.log

- get the pod definiton file
  - delete all unnecessary lines 
    - d100d in vim editor

- nginx
  - logs per default in /var/log/nginx directory

- hostPath configuration example
  - https://kubernetes.io/docs/concepts/storage/volumes/#hostpath-configuration-example
  [
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: Directory
  ]

- container definition:
spec:
  containers:
  - env: 
    - name: LOG_HANDLERS
      value: file
    image: kodekloud/event-simulator
    name: event-simulator
    volumeMounts:
    - mountPath: /log
      name: log-volume
  volumes:
  - name: log-volume
    hostPath:
      path: /var/log/webapp

$ kubectl explain persistentvolume --recursive
  - /hostpath


- cat pv.yaml 
--
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 10Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  storageClassName: slow
  hostPath:
    path: /pv/log


- cat pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Mi

$ k get pv,pvc
$ k describe pvc
  -> waiting for first consumer to be created before binding

https://stackoverflow.com/questions/55044486/waitforfirstconsumer-persistentvolumeclaim-waiting-for-first-consumer-to-be-crea


kubectl get storageclasses.storage.k8s.io
kubectl get storageclasses.storage.k8s.io gp2 -o yaml > gp2.yaml

- todo
  - work more on the examples

