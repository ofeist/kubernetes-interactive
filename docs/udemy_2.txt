Certification Details

Note!

In the video, I said the exam is 3 hours. With the latest version of the exam, it is now only 2 hours. The contents of this course have been updated with the changes required for the latest version of the exam.


Below are some references:

Certified Kubernetes Administrator: https://www.cncf.io/certification/cka/

Exam Curriculum (Topics): https://github.com/cncf/curriculum

Candidate Handbook: https://www.cncf.io/certification/candidate-handbook

Exam Tips: http://training.linuxfoundation.org/go//Important-Tips-CKA-CKAD


    Head over to this link to enroll in the Certification Exam. Remember to keep the code - 20KODE - handy to get a 20% discount while registering for the CKA exam with Linux Foundation.



---
https://killer.sh/pricing



---
Slack Group URL:
https://kodekloud.com/pages/community



---
We have created a repository with notes, links to documentation and answers to practice questions here. Please make sure to go through these as you progress through the course:

https://github.com/kodekloudhub/certified-kubernetes-administrator-course




2.11 cluster architecture
---

- master node:
    - etcd cluster
    - kube-apiserver
    - kube controller manager
    - kube-scheduler


- worker nodes
    - kubelet
    - kube-proxy
    - container runtime engine (docker/rkt/containerd)


- k8s | CRI (I = interface!)| container runtimes
    - container runtime interface

- dockershirm
    - pseudo interface developed from k8s only for docker



2.12. docker vs containerD
---
- containerd is enough
    - docker doesn't need to be installed

- ctr tool
    - only for debugging

- nerdctl (tool)
    - supports docker compose
    - supports newest features in containerd
        - encrypted images
        - lazy pulling
        - p2p image distribution
        - image signing & verifying
        - namespaces in k8s

- nerdctl commands analogue to docker commands
    - D -> docker run --name redis redis:alpine
    - N -> nerdctl run --name redis redis:alpine

    - D -> docker run --name webserver -p 80:80 -d nginx
    - N -> nerdctl run --name webserver -p 80:80 nginx


- crictl - CLI
    - used to interaction with CRI (container runtime interface)
    - for debugging and inspection
    - not user friendly

- to communicate with containerd you got to do api calls

- before (1.13 e.g.), for troubleshooting docker commands were used
    - now crictl (cry control) tool will be used

- commands
    - crictl
    - crictl pull busybox
    - crictl images
    - crictl ps -a
    - crictl exec -i -t 3e423421dd23424a4324ac42423 ls
    - crictl logs 3e953789dd8421ae943c24
    - crictl pods

- check the difference between docker and crictl commands
    - https://kubernetes.io/docs/reference/tools/map-crictl-dockercli/

- crictl --runtime-endpoint
- export CONTAINER_RUNTIME_ENDPOINT
- unix:///var/run/cri-docker.sock


2.13 etcd for beginners
---
./etcdctl --version

- run the spec version
    $ ETCDCTL_API=3 ./etcdctl version

- or specify for the session
    $ export ETCDCTL_API=3
    $ ./etcdctl version

- we work with version 3

$ export ETCDCTL_API=3
$ ./etcdctl put key1 value1

$ etcdctl get key1



2.14 etcd in k8s
---
- etcd is a cluster datastore for:
    - nodes
    - podes
    - configs
    - secrets
    - accounts
    - roles
    - bindings
    - others

- port: 2379

- list all keys stored in etcd db
    $ kubectl exec etcd-master -n kube-system etcd get / --prefix -keys-on

- https://gist.github.com/lalyos/aef94a4c23973eaee4a17bb26b6972a2


ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt \
    ETCDCTL_CERT=/etc/kubernetes/pki/etcd/peer.crt \
    ETCDCTL_KEY=/etc/kubernetes/pki/etcd/peer.key \
    ETCDCTL_API=3  \
    etcdctl \
      get \
      --keys-only \
      --prefix=true \
      "/registry/namespaces/"


etcdctl get --keys-only --prefix=true "/registry/" | less

- you must also specify path to certificate files 
    so that ETCDCTL can authenticate to the ETCD API Server
        --cacert /etc/kubernetes/pki/etcd/ca.crt     
        --cert /etc/kubernetes/pki/etcd/server.crt     
        --key /etc/kubernetes/pki/etcd/server.key


- k8s components
---
- kube-api-scheduler
    - communication between components
- kube-controller-manager
    - the brain
- kube-scheduler
    - decides which pod on which node
- kubelet
    - connection between nodes and master
- kube-proxy
    - networking


- pod = smallest object of k8s


- accessing labs
---
https://uklabs.kodekloud.com/courses/labs-certified-kubernetes-administrator-with-practice-tests/

https://kodekloud.com/topic/practice-test-replicasets-2/

q:
- how to check the number of containers in a pod?
    $ k get pods -o wide
        # ready column


2.29 replica sets 
---
-  is often used to guarantee the availability of a specified number of identical Pods.
- use cases
    - keeps pods always running
    - loadbalancing and scaling
    - to monitor already existing pods (and re-deploy failed ones if needed)

- replication controller -> older technology
- replicaSet -> same purpose as the replication controller

- main difference between replicaSets and replication controller is the "selector" field 
- selector matches the pod, e.g.
    selector:
        matchLabels:
        type: front-end

- no daemonset/replicaset create command
    - instead "k create deployment" is used

- use "replace" command to update the replicasets
    $ kubectl replace -f replicaset-definition.yaml
        - what about "apply"???

- scale
    $ kubectl scale -replicas=6 -f replicaset-definition

- edit
    $ kubectl edit replicaset ...

commands:
    $ kubectl edit replicaset ...
    $ kubectl scale -replicas=6 -f replicaset-definition
    $ kubectl replace -f replicaset-definition.yaml
    $ kubectl explain replicaset
        # gives the kind & version



2.32 deployments
---
- supports rolling updates
    - different versions of app

- pod < replicaSet < deployment

- yaml similar to replicaSet ("kind=Deployment" is the only difference to rs)



2.36 services
---
- can span across more nodes in the cluster

- nodeport service
    - nodePort : Port : targetPort
    - NoPeWeB = NodePort is used for Web Apps/containers

- clusterip service
    - CLuPi DaBar = clusterIp is used for e.g. database (as a backEnd ip aggregator)

- loadbalancer
    - on the cloud platform that supports LoadBalancer type
    - instead of NodePort

- a difference between "expose" and "create service"
    - https://stackoverflow.com/questions/59397542/kubernetes-create-service-vs-expose-deployment
    - create service
        $ kubectl create service nodeport demo --tcp=8080:80 --node-port=31888
        - can set nodeport, port and target port
    - expose
        $ kubectl expose deployment demo --name=demo --type=NodePort --port=8080 --target-port=80
        - node port must be set afterwards (kubectl edit)
    - create service doesn't have the "set selector"
        $ kubectl set selector service demo myapp=hello
        - service to work with a deployment with pods labeled myapp: hello
            - must be set afterwards

- expose/create differences in short 
    - expose no NodePort (eno nodeport)
    - create service no label (kresxe no label)

- use "expose object"
    - when creating a service from an existing object, a suggestion is to use expose
        $ kubectl expose deployment nginx-deployment --type=NodePort --port=80 --target-port=80
            - if nodePort important, then edit the service (yaml definition file) and set the nodePort
            - nodePort: 30080

- examples of exposing and creating the service 
    - https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/15018998#questions/19114274

    - Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379
        $ kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
        ( This will automatically use the pod's labels as selectors )

    - Or
        $ kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml 
        ( This will not use the pods labels as selectors, instead it will assume selectors as app=redis. 
        You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. 
        So generate the file and modify the selectors before creating the service )

- a trick to expose a service when creating the pod, at the same time, one-liner
    $ kubectl run httpd --image=httpd:alpine --port=80 --expose=true

- there is a open task for setting labels when create service!
    - https://github.com/kubernetes/kubernetes/issues/46191    



2.41 namespaces
---
- permanently set the "dev" namespace
    $ kubectl config set-context $(kubectl config current-context) --namespace=dev



2.42 namespaces - solutions
---
- to access a service DNS accross the cluster use e.g.
    - "db-service.dev.svc.cluster.local"



2.44 imperative vs declerative
---
- commands used for manipulating objects in k8s

- imperative approach example
    - create yaml object (yaml file)
    - edit the yaml file, when needed
    - replace (update) the object
        $ kubectl replace -f nginx.yaml
        -  fails if the object doesn;t exist
    - completly delete and replace the object: --force
        $ kubectl replace --force -f nginx.yaml

- declarative example
    $ kubectl apply -f nginx.yaml
    $ kubectl apply -f path-to/dir-with/object-files

- a trick to expose a service when creating the pod, at the same time, one-liner
    $ kubectl run httpd --image=httpd:alpine --port=80 --expose=true



2.47 solutions
---
- a trick to expose a service when creating the pod, at the same time, one-liner
    $ kubectl run httpd --image=httpd:alpine --port=80 --expose=true



2.48 apply command
---
- works on 3 objects
    - local file
    - last applied config
    - live object configuration

- last applied configuration
    - to figure out what fields have been removed from the local file
    - used to check if some lines are removed from configs



3.53 manual scheduling
---
- two ways of manual scheduling
    - set the nodeName field in pod definition file
        $ kubectl replace --force -f nginx.yaml     # if the pod already existed
    - create a binding object (pod to node)

- kube-scheduler schedules the pod
    - how this is done:
        - checks pod definitions and sets the "nodeName" property
            - nodeName: node02

- without the scheduler, the easiest way to schedule a pod is to
    - set the nodeName property in the pod definition file
    - Pending state (pod)
        - if scheduler no present then pod in Pending state

- another way is to create a binding object
    - this mimics the way the scheduler works
    - if the pod already assigned to a node
        - in such case, the scheduler won't allow to change the existing object

- binding object to be sent as a POST req
    $ curl --header "Content-Type:application/json" --request POST \
        --data '{"apiVersion":"v1", "kind": "Binding", ...pod-bind-def-to-json..."}' \
        http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/

- pod-bind-definition.yaml
---
apiVersion: v1
kind: Binding
metadata:
    name: nginx
target:
    apiVersion: v1
    kind: Node
    name: node02



3.55 manual scheduling solution
---
- in order to delete/create a pod (use one command)
    $ kubectl replace --force -f nginx.yaml
- instead of these two
    $ k delete pod
    $ k create -f pod

- there is no way to move pod from one node to the other
    - a pod is a process on the system actually



3.56 labels and selector (& annotations)
---
$ kubectl get pods --selector app=App1

- annotations
    - different details; may be used e.g. for integration purposes
        - buildVersion: 1.34
        - toolVersions
    - in metadata
    - same level as labels



3.57 taints and tolerations
---
- taint the node (TaN)
    - as a repellent
    $ kubectl taint nodes node1 app=blue:NoSchedule
    # kubectl taint nodes node-name key=value:taint-effect

- toleration on pod (ToP)
    - to accept the taint

- TaN vs ToP

- master node gets a taint per default, when deployed
    - Taints: node-role.kubernetes.io/control-plane:NoSchedule



3.62 node selectors
---
- first a node is labeled
- when creating a resource, a node selector can be used

- drawback: no advance expressions
    - no NOT operation
        - label = not small
    - no "either" operation
        - label = medium or large
    - for such purpose the "node affinity" is used



3.66 taints, tolearations and node affinity
---
- these 3 must be used simultaneously to place pods on right nodes and prevent the same



3.67 resource requests and limits
---
- cpu limits will be throttled
    - if no cpu available
        - err = insufficient cpu
        - existing pods will be throttled
- memory limits can not be throttled
    - the only way to un-assign the memory is to kill the pod
    - if a pod consumes more than the limit/available
        - err = OOM (Out Of Memory)
        - existing pod will be terminated

- 1G    = 1,000,000,000 bytes (1 Gigabyte)
- 1Gi   = 1,073,741,824 bytes (1 Gibibyte)

- minimum CPU limit = 1m 
    - 1 CPU ~ 1 AWS vCPU
    - 0.1 CPU = 100m 
    - 1m = 0.001 CPU

- ideal CPU settings
    - Requests (are set)
        - for all pods!
        - otherwise a pod could be left with no resources
    - No Limits
    
- ideal Memory settings
    - Requests (are set)
        - for all pods!
        - otherwise a pod could be left with no resources
    - No Limits

- limits could be set on pod level (pod yaml spec)

- default limits
    - per default limits could be set with limit ranges
        - set at the namespace level

- limit-range-cpu.yaml
---
apiVersion: v1
kind: LimitRange
metadata:
    name: cpu-resource-constraint
spec:
    limits:
    - default:
        cpu: 500m
      defaultRequest:
        cpu: 500m
      max:
        cpu: "1"
      min:
        cpu: 100m
      type: Container


- limit-range-memory.yaml
---
apiVersion: v1
kind: LimitRange
metadata:
    name: memory-resource-constraint
spec:
    limits:
    - default:
        memory: 1Gi
      defaultRequest:
        memory: 1Gi
...


- these limits are enforced when a pod is created!

- resource quotas
    - at the namespace level (namespace level object)
    - to limit overall resources 


- resource-quota.yaml
---
apiVersion: v1
kind: ResourceQuota
metadate: 
    name: my-resource-quota
spec:
  hard:
    requests.cpu: 4
    requests.memory: 4Gi
    limits.cpu: 10
    limits.memory: 10Gi



2.68 editing pods and deployments
---
- a deployment can be edited
    $ kubectl edit deployement ABC 

- pod can not be edited always

- what can be edited in pod is
    - spec.containers[*].image
    - spec.initContainers[*].image
    - spec.activeDeadlineSeconds
    - spec.tolerations

- a tactic to re-create the pod
    $ kubectl edit pod <pod name>
        - edit the field which can not be edited
        - save: new /tmp/ file will be created
    $ kubectl delete pod webapp
    $ kubectl create -f /tmp/kubectl-edit-ccvrq.yaml



3.71 daemon sets
---
- they get deployed once per node

- use cases
    - monitoring
    - logging
    - networking (weave-net)
    - kube-proxy
    - ...

- when creating daemonSet, you could create deployent instead
    $ kubectl create deployment --image= ...
        - then change "kind:"

- creating of DeamonSert is also similar to replicaSet
    - what is a difference to RS?

- how k8s create daemonSet? k8s uses
    - NodeAffinity &
    - default scheduler

- before (version < 1.12) it set nodeName in pod definition
    - this creates pod without using scheduler



3.74 static pods
---
- created by kubelet
- so, only a worker node is needed
    - /etc/kubernetes/manifests

- static pods are recreated if they get terminated
    - without the intervention of api-server!

- this way, only the creation of pods is possible
    - not possible to create
        - daemonSets
        - replicaSets
        - deployments
        - services
    - this rest needs other k8s components like
        - replication controller
        - deployment controller, ...
        - etc.

- kubelet works on a pod level

- kubelet setting for statis pod is in "kubelet.service" file
    - --pod-manifest-path=/etc/Kubernetes/manifests \\

- the other way is to put in "kubelet.service" file
    - --config=kubeconfig.yaml \\
    - and then put in "kubeconfig.yaml" the path:
        - staticPodPath: /etc/kubernetes/manifets
    - such approach (kubeconfig.yaml) is used by kubeadm tool
        - kubeadm install

- to inspect static pods on a node use
    - one of a command
        - docker ps     # docker
        - crictl ps     # cri-O
        - nerdctl ps    # container-d
    - since, no kube api server = no kubectl installed

- the usage
    - to create controlplane components
        - install kubelet and place in manifest/ folder
            - controller-manager.yaml
            - apiserver.yaml
            - etcd.yaml
            - manager.yaml
    - services recreated if they would crash

- difference between static pods and daemonSets
    - static pods
        - created by kubelet
        - deploy control plane components as static pods
        - ignored by kube-scheduler
    - daemonSets
        - created by kube-api server (DaemonSet Controller)
        - deploy monitoring & logging agents on nodes
        - ignored by kube-scheduler

- kube-proxy component is not deployed as a static pod



2.77 multiple schedulers
---

- to install kube-scheduler
    - binary, or
    - kubeadm

- define config file
    - my-scheduler-config.yaml
    ---
    apiVersion: kubescheduler.config.k8s.io/v1
    kind: KubeSchedulerConfiguration
    profiles:
    - schedulerName: my-scheduler
    leaderElection:     # needed if multiple copies of schedulers run in HA cluster
      leaderElect: true
      resourceNamespaces: kube-system
      resourceName: lock-object-my-scheduler


- use the config file from my-scheduler.service
    - if this were a installation from a "bin" file
        ExecStart=/usr/local/bin/kube-scheduler \\
            -- config=/etc/kubernete/config/my-scheduler.config.yaml
    - if this were the "Pod" installation (kubeadm)
        - pass the --config= option to a pod definition
            my-custom-scheduler.yaml
            ---
            apiVersion: v1:
            kind: Pod
            ...
            spec:
            containers:
            - command:
                - kube-scheduler
                - --address=127.0.0.1
                - --kubeconfig=/etc/kubernetes/scheduler.comf
                - --config=/etc/kubernetes/my-scheduler-config.yaml # the sched.-name here defined

                image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
                name: kube-scheduler

- to view schedulers
    $ kubectl get pods --namespace=kube-system

- to use the custom scheduler from a pod
    - specify "schedulerName: my-scheduler" in 
        - pod.yaml
        ---
        apiVersion: v1
        kind: Pod
        metadata:
        name: nginx
        spec:
        containers:
        - name: nginx
            image: nginx
        schedulerName: my-scheduler
    - if pod remains in Pending state
        - there is something wrong with the scheduler config then

- whoch scheduler did configure which pod 
    - view events
        $ kubectl get events -o wide
            # REASON & SOURCE
    - view logs of the scheduler
        $ kubectl logs my-scheduler -n kube-system



2.78 multiple schedulers - solution
---

- create a configmap (configmap as a volume)
    - name: my-scheduler-config
    - from-file=my-scheduler-config.yaml
    $ kubectl create configmap my-scheduler-config --from-file=/root/my-scheduler-config.yaml



2.80 configuring scheduler profiles
---
- ver >= 1.18

- pods get scheduled in this order
    - scheduling queue
        - priorityClassName: high-priority     # defined in pod.yaml
            - if a PriorityClass is defined
        - done by plugin
            - PrioritySort
    - Filtering
        - resource check: pods against nodes
        - plugins:
            - NodeResourceFit
            - NodeName      # nodeName in YMAL
            - NodeUnschedulable     # kubectl cordon $NODENAME
    - Scoring
        - free CPU space, after reserving the current pod
        - the node with more left CPUS gets a higher score
        - plugin
            - NodeResourcesFit
            - ImageLocallity     # high score if container image exists on a node
    - Binding
        - plugin
            - DefaultBinder

- extension points
    - to customize what plugins go where
    - to write our plugins and plug them in here
    - at each stage (scheduling queue, filtering, scoring, binding) there is an extension point
        - queueSort
        - filter
            - prefilter     # this extension point exists too; before filere EP
            - postFilter    # after filter
        - score
            - prescore      # before
            - reserve       # after score
            - permit        # after reserve
        - bind
            - preBind       # before bind
            - postBind      # after bind

- possible to have one scheduler (one binary) running more profiles
    - configuration done per
        - my-scheduler.yaml
        ---
        apiVersion: kubescheduler.config.k8s.io/v1
        kind: KubeSchedulerConfiguration
        profiles:
        - schedulerName: my-scheduler-2
          plugins:
            score:
              disabled:
              - name: TaintToleration
              enabled:
              - name: MyCustomPluginA
              - name: MyCustomPluginB
        - schedulerName: my-scheduler-3
          plugins:
            preScore:
              disabled:
              - name: "*"
            score:
              disabled:
              - name: "*"

- links
    - https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md
    - https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/
    - https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/
    - https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work



4.85 metrics and monitoring
---
- do not use this for prod environment !
    git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git
    cd kubernetes-metrics-server/
    k create -f .


4.86 managing application logs
---
$ docker run kodekloud/event-simulator
    - prints output / shows logs
$ docker run kodekloud/event-simulator      # detached mode
    - no logs shown
$ docker logs -f ecf    # ecf = container_id

- for k8s certification only this is needed:
    - singlecontainer pod
        $ k logs -f event-simulator-pod     # pod name = event-simulator-pod

    - multiple containers
        $ k logs -f event-simulator-pod event-simulator     # container name = "event-simulator"


5.92 rolling udpates and rollbacks (of a deployment)
---
- deployment creates a rollout

- 2 deployment strategies
    - recreate
        - destroys all & creates all
    - rollout
        - one pod by one
        - default strategy

- kubectl "apply" vs "set image"
    - apply 
        - if you change the YAML input
    - set image
        - doesn't update the YAML input!
        $ kubectl set image deployment/myapp-deployment \
            nginx-container=nginx:1.9.1

- upgrade happens per(via) changing of replicasets
    $ kubectl get replicasets
        - check this during a deployment upgrade

- rollback
    $ kubectl rollout undo deployment myapp-deployment      # rollback
        $ k get rs  # try this one again

- status
    $ kubectl rollout status deployment myapp-D

- history
    $ kubectl rollout history deployment myapp-d

- commands:
    $ k apply
    $ k set image 
    $ kubectl rollout
        - pause
        - status
        - history
        - undo    # rollback
            $ k rollout undo deployment frontend --to-revision=2

- set the deployment image
    - set image
        $ k set image deploy frontend simple-webapp=kodekloud/webapp-color:v2
        $ k set image deploy $DEPLOYMENT_NAME $CONTAINER_NAME=$IMAGE_NAME
    - edit deployment
        $ k edit deployment frontend



5.95 Configure Applications
---
- Configuring applications comprises the following configurations
    - Command and Arguments on applications
    - Environment Variables
    - Secrets



5.96 commands
---
- a task for me
    - run busybox with bash instead of sleep / without sleep
        - check if process dies
            -> it recreates the container after 'sleep 10"!
            - after(?) that it ends in "CrashLoopBackOff



5.97 commands and arguments
---
- docker style
Form Ubuntu
ENTRYPOINT["sleep"]
CMD["5"]

- k8s stype
command:["sleep2.0"]    <- overrides ENTRYPOINT[] from docker image
args:["10"]             <- overrides CMD[]  from docker image



5.99 commands and args - solution
---
- pass command and args to the pod
    $ kubectl run webapp-green --image=kodekloud/webapp-color --command -- $cmd -- $arg1 $arg2

- pass only args to the pod
    $ kubectl run webapp-green --image=kodekloud/webapp-color -- $arg1 $arg2



5.100 ENV variables
---
- -env => array
    - has 2 properties
        - name:
          value:

- 3 types:
    - Plain Key Value
    - configMap
    - Secrets

- Plain Key Value
    - env:
      - name: APP_COLOR
        value: pink

- configMap
    - env:
      - name: APP_COLOR
        valueFrom:
            configMapKeyRef:

- Secrets
    - env:
      - name: APP_COLOR
        valueFrom:
            secretKeyRef:



5.101 configure configMaps in apps
---
- two steps involved:
    - 1: create config map
    - 2: inject into the pod

- step 1: create
---
    - (imperative) :: to create configMap
        - from command line
            $ k create configmap \
                app-config --from-litaral=APP_COLOR=blue \
                            --from-litaral=APP_MOD=prod
        - from a file
            $ k create configmap \
                app-config --from-file=app.properties

    - (declerative) :: to create configMap
        $ kubectl create -f

    - to view configMaps
        $ k get configmaps
        $ k describe configmaps

- step 2: inject
---
    - as the environment variable (from config-map.yaml)
        - pod-definition.yaml
        ---
        ...
        spec:
        containers:
          - name:
            image:
            envFrom:
            - configMapRef:
                    name: app-config    
                        # app-config = the name of config-map.yaml metadata / name: app-config
    - as single ENV
        env:
          - name: APP_COLOR
            valueFrom:
              configMapKeyRef:
                name: app-config
                key: APP_COLOR
    - as a volume
        volumes:
        - name: app-config-volume
          configMap:
            name: app-config



5.104 secrets in applications
---
- as per configMaps
    - create a secret
    - inject into a pod

- to create
    - imperative
        $ kubectl create secret generic \\
            sec-name --from-literal


    - declerative
        $ kubectl create -f secret-data.yaml
        - secret data in base64
            $ echo -n "mysql" | base64

    - to get the values
        $ kubectl get secrets app-secret -o yaml
        $ echo -n "bXlzcWw=" | base64 --decode

- to inject
    - from secrets-def.yaml
        pod-def.yaml
        ---
        ...
        spec:
        containers:
        - name: ..
            image: ..
            envFrom
            - secretRef:
                    name: app-secret
    - as single ENV
        env:
          - name: DB_PASS
            valueFrom:
              secretKeyRef:
                name: app-config
                key: APP_COLOR
    - as a volume
        - to use
            volumes:
            - name: app-secret-volume
            secret:
                name: app-secret
        - each value of the secret gets mounted as a volume on the pod
            - /opt/app-secret-volumes/DB_Password
            - /opt/app-secret-volumes/DB_User
            - ...

- not on secrets
    - secrets are not encrypted (only base64 encoded)
    - secretds in etcd not encrypted
        - consider encrypting data at rest
            https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
    - anyone able to create pods/deployments in namespace can access secrets
        - configure least-privilege access to secrets - RBAC
    - consider third-party secrets store providers
        - AWS, Azure, GCP, Vault provider

- some best practices around using secrets make it safer:
    - Not checking-in secret object definition files to source code repositories
    - Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD
        - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

- there are other better ways of handling sensitive data like passwords in Kubernetes
    - such as using tools like 
        - Helm Secrets
        - HashiCorp Vault
            - https://www.vaultproject.io/



5.108 demo: encrypting secret data at rest
---
- https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

$ ETCDCTL_API=3 etcdctl \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt   \
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key  \
   get /registry/secrets/default/secret1 | hexdump -C



5.113 Multi-container Pods Design Patterns
---
- 3 common patterns
    - sidecar
    - adapter
    - ambassador



5.114 InitContainers
---
- multicontainter pods expect all containers to run throughout the whole lifecycle of the pod
    - when one container stops, the pod restarts

- initContainers
    - do stop after execution
    - run a task when pod is created
    - e.g. a process that pulls a code or binary from a repository
         that will be used by the main web application.

- Init containers always run to completion.

- Each init container must complete successfully before the next one starts.

- when multiple initContainers
    - each init container is run one at a time in sequential order

- if any of the initContainers fail to complete
    - Pod restarted repeatedly until the Init Container succeeds



5.117 Self Healing Applications
---

- Kubernetes supports self-healing applications through 
    -ReplicaSets and Replication Controllers.
    
- The replication controller helps in ensuring that a POD is re-created automatically 
    - when the application within the POD crashes.



6.121 os updates
---
$ kubectl drain node01
    - terminates and moves pods to another node
    - cordons the node (marks as unschedulable)

$ kubectl uncordon node01
    - after the maintenance mark the node as schedulable again

$ kubectl cordon node01
    - you can only mark a node as unschedulable (if needed)



6.125 k8s sw versions
---
- not needed for the exam

- https://kubernetes.io/docs/concepts/overview/kubernetes-api/
- https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md
- https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md



6.126 cluster upgrade process
---
- upgrading kubeadm clusters @ k8s docu page
    - https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

- allowed components' versions
    - x (e.g. x=1.10)
        - kube-apiserver
    - x-1
        - controller-manager
        - kube-scheduler
    - x-2
        - kubelet
        - kube-proxy
    - x+1, x, x-1
        - kubectl

- k8s supports 3 minor versions
- recommended upgrade upproach
    - one minor version at a time
        - 1.10 -> 1.11 -> 1.12 -> 1.13

- first master nodes are upgraded
- after that worker nodes are upgraded

$ kubeadm upgrade plan
    - shows available versions

- how to upgrade
    - master node
        1. upgrade kubeadm
            $ apt-get upgrade -y kubeadm=1.12.0-00
            $ kubeadm upgrade apply v1.12.0
                $ kubeadm version   # check the kubeadm ver
            $ kubectl get nodes
                - master still showing old ver
        2. upgrade kubelet (& kubectl)
            (- fyi kubelet doesn't have to run on master node)
            $ apt-get upgrade -y kubelet=1.12.0-00
            $ systemctl daemon-reload
            $ systemctl restart kubelet
            $ kubectl get nodes
                - master showing new ver
    - worker nodes, one at a time
        1. move the workloads from a node
            $ kubectl drain node01
        2. upgrade kubeadm and kubelet
            $ apt-get upgrade -y kubeadm=1.12.0-00
                $ kubeadm upgrade apply v1.12.0
            $ apt-get upgrade -y kubelet=1.12.0-00
            $ kubeadm upgrade node config --kubelet-version v1.12.0
            $ systemctl restart kubelet
        3. uncordon the node
            $ kubectl uncordon node01
        4. continue with the next node



6.128 cluster upgrade - solution
---
- check the OS release
    $ cat /etc/*release*

- 1.26 -> 1.27
    - https://v1-27.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/



6.130 backup and restore methods
---
- backup candidates are
    - resource configuration
        - namespace, secret, configmap, ... (imperative and declerative)
    - etcd cluster
        - keeps the state of cluster
        - instead of saving resources, you can save etcd cluster
    - persistent volumes

- backup
    - resource configuration
        $ kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml
        - check VELERO  (formerly called ARK by HeptiO)
    - etcd cluster
        $ ETCDCTL_API=3 etcdctl snapshot save snapshot.db



6.131 Working with ETCDCTL
---
- etcdctl is a command line client for etcd
- make sure that you set the ETCDCTL_API to 3
    $ export ETCDCTL_API=3
        # on the master node!

- first stop the kube-apiserver
    $ service kube-apiserver stop

$ etcdctl snapshot save -h
$ etcdctl snapshot restore -h

- the following options are mandatory
    - --cacert
        - verify certificates of TLS-enabled secure servers using this CA bundle
    - --cert
        - identify secure client using this TLS certificate file
    - --endpoints=[127.0.0.1:2379]
        - This is the default as ETCD is running on master node and exposed on localhost 2379
    - --key
        - identify secure client using this TLS key file

- backup etcd
    $ export ETCDCTL_API=3
    $ etcdctl snapshot save snapshot.db

- restore etcd
    $ service kube-apiserver stop       # stop the api server
    $ etcdctl snapshot restore snapshot.db \
        -- data-dir /var/lib/etcd-from-backup   # backup is restored as a new cluster
    - now configure the etcd config file to use new data-dir
        - etcd.service
        ---
        --data-dir=/var/lib/etcd-from-backup
    $ systemctl daemon-reload           # reload the service daemon
    $ service etcd restart              # restart the etcd service
    $ service kube-apiserver start      # start the api server

- with all etcd commands you have to specify
    $ ETCDCTL_API=3 etcdctl \
        snapshot save snapshot.db   \
        -- endpoints=https://127.0.0.1:2379 \
        --cacert=/etc/kubernetes/pki/etcd/ca.crt    \
        --cert=/etc/kubernetes/pki/etcd/server.crt  \
        --key=/etc/kubernetes/pki/etcd/server.key

- references
    - https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster
    - https://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md
    - https://www.youtube.com/watch?v=qRPNuT080Hk



6.132 Practice Test - Backup and Restore Methods
---
$ export ETCDCTL_API=3
$ ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  snapshot save /opt/snapshot-pre-boot.db

$ etcdctl snapshot restore \
    --data-dir /var/lib/etcd-from-backup \
    /opt/snapshot-pre-boot.db

- change the mount math on the host to point to /var/lib/etcd-from-backup

- restart the etcd-controlplane pod
    - delete the pod



6.134 practice test - backup and restore/multiple clusters
---
$ kubectl config view
$ kubectl config use-context cluster1

- apiserver will reveal the etcd-server address
    $ describe apiserver-cluster1-controlplane -n kube-system

- how many nodes are part of etcd cluster?
    $ etcdctl member list
    $ etcdctl member list \
        --cacert=/etc/etcd/pki/ca.pem \
        --cert=/etc/etcd/pki/etcd.pem \
        --key=/etc/etcd/pki/etcd-key.pem \
        --endpoints=https://192.37.38.3:2379


- after restoring on EXTERNAL ETCD server
    - change the owner of the etcd files
        $ chown -r etcd:etcd etcd-data-new/
    - change --data-dir folder
        $ vi /etc/systemd/system/etcd.service
    - restart the service
        $ systemctl daemon-reload
        $ systemctl restart etcd
        $ systemctl status etcd
    - restart all controlplane components
        - delete pods (controller-manager, scheduler)
        - systemctl restart kubelet



7.140 k8s security primitives
---
- whO can access the cluster
    - authEntication
    - whO - authE
    - O-E
    - to aute

- whAt can they do
    - authOrization
    - what - authO
    - A-O 
    - ta auto


7.141 authentication
---
- WHO
- O-E

- all user access is managed by kube-apiserver

- kube-apiserver authenticates requests via
    - static pwd file
    - static token file
    - certificates
    - identity services (LDAP, kerberos, ..)

- basic auth
    - create static pwd file
        - user-details.csv
            ---
            pwd1,user1,u0001(,group1)       # group1 is optional
            pwd2,user2,u0002
        - pass to kubeapi-server
            - --basic-auth-file=user-details.csv
    - static token file
        - static-token-file.csv
            ---
            Kpfsdfaef,user10,u0010,group1   # group1 is optional
            ...
        - pass to kubeap-server
            - --token-auth-file=user-token-details.csv



7.144 TLS certificates
---
- PKI 
    - public key infrastructure
    - pki is the whole process with people and certificates and signing etc.

- public key
    - .pem | .crt
    - a convention

- private key
    - have key in their extension
    - .key | .pem.key
    - a convention



7.146 certificate creation
---
- generate key
    $ openssl genrsa -out ca.key 2048
    - ca.key

- certificate signing request
    $ openssl req -new -key ca.key -sub "/CN=KUBERNETES-CA" -out ca.csr
    - ca.csr

- sign certificate
    $ openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt
    - ca.crt

- check/decode the certificate's details
    $ openssl x509 -in file-path.crt -text -noout
    $ openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout
      - Subject: CN=kube-apiserver
      - X509v3 Subject Alternative Name:
      - validity: Not After : Feb 11 ... 2020
      - Issuer: CN=kubernetes



7.150 view certification details - solution
---
- view pod logs (when kubectl doesn't work)
    $ docker ps -a | grep kube-apiserver
    $ crictl ps -a | grep kube-apiserver

    $ docker logs 25c825c2152a3
    $ crictl logs 25c825c2152a3


- kube-apiserver config for --etcd-cafile
    - is in the etcd/ folder!
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt



7.151 Certificates API
---
- CA server
    - actually ca files & server where these files are kept

- cert API used to
    - sign
    - rotate
    - manage certificates

- all certificate operations performed by
    - Controller manager

- workflow
    - create CSR object (certificateSigningRequest)
    - review request
    - approve req
    - share cert to users

- user creates CSR object (certificateSigningRequest)
    $ openssl genrsa -out jane.key 2048
    $ openssl req -new -key jane.key -subj "/CN=jane" -out jane.csr

- admin generates CSR
    - yaml file
    - jane-csr.yaml
    ---
        kind: CertificateSigningRequest
        request: {`cat jane.csr | base64 | tr -d "\n"`}
            $ cat jane.csr | base64 -w 0
                - this works too, instead of tr -d "\n"

- review request
    $ kubectl get csr

- approve req
    $ kubectl certificate approve jane

- view the certificate in yaml
    $ kubectl get csr jane -o yaml
    $ echo 'FASFASVD4242314' | base64 --decode



7.154 KubeConfig
---
$ curl https://my-kube-playground:6433/api/v1/pods  \
    --key admin.key 
    --cert admin.crt
    --cacert ca.crt

$ kubectl get pods
    --server my-kube-playground:6433
    --client-key admin.key 
    --client-certificate admin.crt
    --cacercertificate-authority ca.crt

$ kubectl get pods
    --kubeconfig config

- by default KubeCOnfig file is in
    -$HOME/.kube/config

- KubeConfig file
    - clusters
    - contexts
    - users

$ kubectl config view
$ kubectl config use-context prod-user@production


7.154 kubeconfig solutions
---
- https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#explore-the-home-kube-directory

$ kubeconfig config use-context research --kubeconfig /root/my-kube-config



7.158 api groups
---
- all resources in k8s are organized in api groups

- our focus in this lecture are these apis
    - the version
        - /version
    - the API
        - /api  (core group)
            / v1
                /namespaces
                /pods
                /rc
        - /apis (named group)
            - going forward all apis will be organized in the named groups
            - api group (apps, extensions, networking.k8s.io, ...)
            /apps
                /v1
                    - resources:
                    /deployments
                    /replicasets
                    /statefulsets
                        - set of actions = verbs (each resource has one!)
                        - list 
                        - get 
                        - create 
                        - delete 
                        - update 
                        - watch
                    ...
            /extensions
            /networking.k8s.io
                /v1
                    /networkpolicies
            /storage.k8s.io
            /authentication.k8s.io
            /certificates.k8s.io
                /v1
                    /certificatesigningrequests

- accessing api groups per curl (3.35)
    - https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/14296108#overview
    - kubectl proxy (!= kube proxy !!!)



7.159 authOrization
---
- WHAT can somebody
- O-A

- mechanisms / modes (--authorization-mode)
    - node
    - ABAC (attribute based authorization)
        - not easy to manage (k8s restart needed after new policy)
    - RBAC (role based)
    - webhook
    - AlwaysAllow
    - AlwaysDeny



7.160 RBAC
---
- developer-role.yaml
    $ k create -f yaml
    $ k get roles
    $ k describe role developer

- devuser-developer-binding.yaml
    $ k create -f yaml
    $ k get rolebindings
    $ k describe rolebinding developer

$ kubetl auth can-i create deployments
    ... delete nodes
    ...

$ kubetl auth can-i create deployments --as dev-user

- resource level RBAC could be defined
    - resources such as pods (blue, green, etc ..)



7.162 RBAC solution
---
$ k create role --help
$ k create rolebinding --help



7.163 cluster roles and role bindings
---
- nodes can not be associated with namespaces
- nodes are cluster-scoped resources

- resources can be
    - namespaced    
        - e.g. pods, rs, jobs, deployments, ...
        $ kubectl api-resources --namespaced=true
    - cluster-scoped
        - nodes, 
        - PV, 
        - clusterroles, 
        - clusterrolebindings, 
        - certificateSigningRequest
        - namespaces
        $ kubectl api-resources --namespaced=false

- to authorize users there are (similar to roles and rolebindings)
    - clusterroles
    - clusterrolebindings

- e.g. clusterroles:
    - cluster admin
        - CRUD nodes
    - storage admin
        - CRUD PVs / PVCs
    - cluster-admin-role.yaml
    - cluster-admin-role-binding.yaml

- a cluster role for a namespaced resource possible
    - in that case, the user has access to the resource accross the cluster (all namespaces???)



7.164 cluster roles and binidings - solution
---
$ kubectl api-resources | grep -e pv -e storageclass
$ k create clusterrole -h
$ k create clusterrolebindings -h

$ k get nodes --as michelle



7.166 Service Accounts & tokenRequestAPI
---
$ k create serviceaccount dashboard-sa
$ k create token dashboard-sa   # since ver 1.24
    - expiry date!
$ k get sa
$ k describe sa dashboard-sa


- when SA created, per default
    - a token was created too
    - "Tokens: dashboard-sa-token-kbbdm"
    $ kubectl describe secret dashboard-sa-token-kbbdm
        -> "eyJhbGciTRALALA ..."

- token could be used for authentication when making a request
    $ curl https://192.168.56.70:6443/api -insecure
        -- header "Authorization: Bearer eyJhbGciTRALALA ..."

- this token could be mounted inside of the pod on k8s cluster

- each namespace has it's own service account created
    - default       # k get serviceaccount

- when pod created, a token "default-token-j4hkv" is mounted to the pod
    - location: /var/run/secrets/kubernetes.io/serviceaccount
    $ k exec -it my-pod -- ls /var/run/secrets/kubernetes.io/serviceaccount
    $ k exec -it my-pod -- cat /var/run/secrets/kubernetes.io/serviceaccount

- default token is limited in permissions

- if i want to use another token
    - mount it to the pod
    ---
    spec:
        containers:
          - name: ..
            image: ..
        serviceAccountName: dashboard-sa

- you cannot edit the service account of an existing pod.
- However, in case of a deployment,
    you will be able to edit the service account
    as any changes to the pod definition file
    will automatically trigger a new rollout for the deployment.
        
- since version 1.22
    - tokenRequestAPI introduced
    - pod doesn't rely on SA token (default one) anymore
    - sets the time boundary/limits on the created token

$ k create token dashboard-sa   # since ver 1.24
    - expiry date!

- you still can create non-expiry token (like it was auto-created before)
    - secret-definition.yaml
    ---
    annotations: 
        ...
    - this should be don only if you can not use tokenRequest API to obtain token


7.167 sa token - solution
---
$ kubectl create token dashboard-sa



7.169 image security
---
- https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-pod-that-uses-your-secret
- image: nginx
- actually
    - image: docker.io/library/nginx
             registry / user account / image repository

- popular registries
    - docker.io
    - gcr.io

- for the private repo:
    $ kubectl create secret docker-registry regcred \
        --docker-server=
        --docker-username=
        --docker-password=
        --docker-email=
    - docker-registry
        - builtin type

- private repo
    $ docker login private-registry.io
    $ docker run private-registry.io/apps/internal-app
    - k8s:
        - nginx-pod.yaml
        ---
        spec:
        containers:
        - name: nginx
          image: private-registry.io/apps/internal-app
        imagePullSecrets:
        - name: regcred



7.172 security in docker
---
- process isolation
    - diff proc id's in diff namespaces
        - PID=123 @ container
        - PID=456 @ host

- user security
    - by default docker runs processes as the root user
        $ docker run ubuntu sleep 3600  # as root
    - user security could be enforced by changing the user which runs proc
        - with the --user command
            $ docker run --user=1000 ubuntu sleep 3600
        - to define the user in the image
            Dockerfile
            ---
            FROM ubuntu
            USER 1000
    - additionally, docker sets the limit on the container's root user
        - this happens using linux capabilities
        - e.g. no reboot allowed, cannot disrupt host or other container, ...

- linux capabilities
    - /usr/include/linux/capability.h
    - to add the capability
        $ docker run --cap-add MAC_ADMIN ubuntu
    - to drop the capability
        $ docker run --cap-drop MAC_ADMIN ubuntu
    - or, with all privileges enabled
        $ docker run --privileged drop MAC_ADMIN ubuntu



7.173 security contexts (security in k8s)
---
- similar to docker
- security contexts / capabilities could be enforced on
    - container level
    - pod level

- settings on container > settings on the pod
    - > means: are stronger/override

- pod level securityContext
    - pod definition:
        - pod.yaml
        ---
        apiVersion ...
        ..
        spec:
        securityContext:
            runAsUser: 1000
        containers:
        - name: ubuntu
            image: ubuntu

- container level securityContext
    - pod definition + capabilities
        - pod.yaml
        ---
        apiVersion ...
        ..
        spec:
            securityContext:
                runAsUser: 1000
            containers:
            - name: ubuntu
            image: ubuntu
            securityContext:
                runAsUser: 1000
                capabilities:
                add: ["MAC_ADMIN"]
    - capabilities only supported ath the container level



7.175 security contexts - solutions
---
$ kubectl exec ubuntu-sleeper -- whoami



7.176 network policies
---
- by default all pod communication is allowed

- nw-policy.yaml
---
...
policyTypes:
- Ingress
- ingres:
  - from:
    - podSelector:
      matchLabels:
        name: api-pod
    ports:
    - protocol: TCP
      port: 3306


- 3 types of selectors
    - podSelector
    - namespaceSelector
    - ipBlock

- list (-) = OR selector
    - from:
      - podSelector
      - ipBlock

- item = AND selector
    - from:
      - podSelector:
        ...
        namespaceSelector:



7.179  Kubectx and Kubens – Command line Utilities
---
- in a real “live” kubernetes cluster implemented for production,
     there could be a possibility of often switching between a 
     large number of namespaces and clusters

- This is where command line tools such as kubectx and kubens come in to picture.
    - Reference: https://github.com/ahmetb/kubectx

- Kubectx:
    - With this tool, you don't have to make use of lengthy “kubectl config” commands to switch between contexts. This tool is particularly useful to switch context between clusters in a multi-cluster environment.

    - Installation:
        sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
        sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx

    - To list all contexts:
        kubectx

    - To switch to a new context:
        kubectx <context_name>

    - To switch back to previous context:
        kubectx -

    - To see current context:
        kubectx -c

- Kubens:
    - This tool allows users to switch between namespaces quickly with a simple command.

- Installation:
    sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
    sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens

    - To switch to a new namespace:
        kubens <new_namespace>

    To switch back to previous namespace:
        kubens -



8.182 docker storage
---
- storage drivers
- volume drivers

- storage
    - storage
        - /var/lib/docker/volumes

    - 2 types of mounts:
        - volume mount
            - mounts volume from volumes dir
        - bind mount
            - mounts dir from in-location on the docker host

    $ docker run \
        --mount type=bind, source=/data/mysql,target=/var/lib/mysql mysql

    - docker uses storage drivers
        - it chooses the driver automatically, based on OS 

    - storage drivers
        - aufs
            - available on ubuntu
        - zfs
        - btrfs
        - device mapper
            - available on centos, fedora
        - overlay
        - overlay2

- volume drivers
    - default one = Local
    - rexray, portworx, flocker, ..



187. Volumes
---
- host path not recomended on the multinode cluster
    - pods would always moundt the same folder
    - unless you have some cluster-storage solution, e.g.
        - nfs, gluster, ceph, scaleio, flocker, aws, ...
        - aws: awsElasticBlockStore:



188. Persistent Volumes
---
- A PersistentVolume (PV) is a piece of storage in the cluster 
    that has been provisioned by an administrator or 
    dynamically provisioned using Storage Classes.
- a large amount of storage "carved" out by an admin

- users takes peaces from the large storage amount

- PV = large amount of storage defined by an admin

- PVC = using pvc's users now can use the storage (use out the PV)

- pve-definition.yaml
    ---
    apiVersion: v1
    kind: PersistentVolume
    metadata:
        nameL pv-vol1
    ...

$ k create -f ..
$ kubectl get persitentvolume



189. Persistent Volume Claims
---
- PVC 
    - using pvc's users now can use the storage (use out the PV)

- PVC is taken out of PV
    - by k8s dependant on
        - sufficient capacity
        - access modes
        - volume modes
        - storage class
    - by user
        - labels and selectors

- the relationship between PV:PVC = 1:1

- PVC could be defined before PV
    - it stays in the pending state until PV is defined

- pvc-definition.yaml
    ---
    apiVersin: v1
    kind: PersistentVolumeClaim
    ...

$ k create -f ...
$ k get pvc
$ k delete pvc
    - persistentVolumeReclaimPolicy: Retain
        - defines what happens with PV after PVC woudl be deleted
            - Retain 
                -> until manually deleted by an admin (not available for the re-use)
            - Delete
                -> deleted with the PVC
            - Recycle
                -> the data will be scrubbed



190. Using PVCs in Pods
---
- Once you create a PVC use it in a POD definition file by 
    specifying the PVC Claim name under persistentVolumeClaim
     section in the volumes section like this:


        apiVersion: v1
        kind: Pod
        metadata:
        name: mypod
        spec:
        containers:
            - name: myfrontend
            image: nginx
            volumeMounts:
            - mountPath: "/var/www/html"
                name: mypd
        volumes:
            - name: mypd
            persistentVolumeClaim:
                claimName: myclaim

- The same is true for ReplicaSets or Deployments. Add this
     to the pod template section of a Deployment on ReplicaSet.

- Reference URL: 
    - https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes



8.194 additional topics
---
- Additional topics such as StatefulSets are out 
    of scope for the exam. However, if you wish to learn them,
    they are covered in the 
    Certified Kubernetes Application Developer (CKAD) course.



195. Storage Class
---
- dynamic provisioning
    - when PVC is created, the storage class associated with it
      uses the defined provisioner to provision a new disk with 
      the required size on GCP, and then creates a persistent volume,
      and then binds the PVC to that volume.

- you can define diff storage classes
    - silver : pd-standard
    - gold: pd-sends
    - platinum: pd-ssd

- Volume binding mode
    The volumeBindingMode field controls when volume binding
     and dynamic provisioning should occur. When unset, Immediate
      mode is used by default.
    - https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode

- WaitForFirstConsumer (e.g.)
    - The Storage Class called local-storage makes use of VolumeBindingMode set 
        to WaitForFirstConsumer. This will delay the binding and provisioning of 
        a PersistentVolume until a Pod using the PersistentVolumeClaim is created.



200.  Prerequisite - Switching Routing
---
- switch -> same nw
    $ ip link         # see the interface on the host
    $ ip addr add     # assign the address

- router -> connects diff networks
    $ route                                         # existing route cfg on the system
    $ ip route add 192.168.2.0/24 via 192.168.1     # configure a gateway on the nw
    $ ip route add default via 192.168.2.1          # default (0.0.0.0) gw to any other nw


$ ping 192.168.2.5
    - connect: network is unreachable
    - routing table needs to be fixed
        $ ip route add 192.168.2.0/24 via 192.168.1.6
    $ ping 192.168.2.5
        - blank:    no nw unreachable message
        - packets are not forwarded
        - cat /proc/sys/net/ipv4/ip_forward     
            - echo 1 > /proc/sys/net/ipv4/ip_forward    # temp solution, before restart
            - /etc/sysctl.conf                          # net.ipv4.ip_forward = 1



201. Prerequisite - DNS
---
$ ping db
    - ping: unknown host db
    $ cat /etc/hosts

- dns server 
    -> manages all hostnames & addresses
    $ cat /etc/resolv.conf
        # nameserver    192.168.1.100

- which one first?
    - etc/hosts
        - or
    - /etc/resolv.conf
        - defined by cat/etc/nsswitch.conf

- subdomain.domain.to-level-domain

- record types
    - A     = web-server
    - AAAA  = web-server
    - CNAME = food.web-server   # name to name mapping

$ nslookup www.google.com



202. Prerequisite - CoreDNS
---
- https://github.com/kubernetes/dns/blob/master/docs/specification.md
- https://coredns.io/plugins/kubernetes/



203. Prerequisite - Network Namespaces
---
$ ip netns add red
$ ip netns add blue
    - create new network namespaces: read and blue

$ ip netns
    - list nw namespaces

$ ip link
    - list nw interfaces on the host

$ ip netns exec red ip link
    - list nw interfaces inside the red namespace
$ ip -n red link
    - list nw interfaces inside the red namespace

$ arp
    - check arp table on the host
$ ip netns exec red arp
    - check arp in namespace

$ route
$ ip netns exec red route


- to connect two namespaces
    - use pipe (aka virtual cable)
    $ ip link add veth-red type veth peer name veth-blue
        - create the pipe
    $ ip link set veth-red netns red
        - attach red interface to the red namespace
    $ ip link set veth-blue netns blue
        - attach blue interface to the blue namespace
    $ ip -n red addr add 192.168.15.1 dev veth-red          # $ ip -n red addr add 192.168.1.10/24 dev veth-red
    $ ip -n blue addr add 192.168.15.2 dev veth-blue        # $ ip -n red addr add 192.168.1.10/24 dev veth-red
        - assign the ip's to namespaces
    $ ip -n red link set veth-red up
    $ ip -n blue link set veth-blue up
        - bring up the interfaces up
    - test with ping and check arp
        $ ip netns exec red ping 192.168.15.2
        $ ip netns exec red arp

- next, switch setup will be made (to connect more namespaces via switch)
    - delete the created links first
    $ ip -n red link del veth-red

- for multiple namespaces to communicate you create A
    - virtual switch
        - linux bridge (we use this one), open vSwitch (Ovs)
    $ ip link add v-net-0 type bridge
        - add the interface of the bridge type
    $ ip link set dev v-net-0 up
        - bring the interface up
    $ ip link add veth-red type veth peer name veth-red-br
        - create the cables/pipes to link between namespace and the bridge network
    $ ip link set veth-red netns red
        - attach one end of the pipe to ns
    $ ip link set veth-red-br master v-net-0
        - attach the other pipe end to the switch
    $ ip -n red addr add 192.168.15.1 dev veth-red
    $ ip -n blue addr add 192.168.15.2 dev veth-blue
        - set the ip addr for the links
    $ ip -n red link set veth-red up
    $ ip -n blue link set veth-blue up
        - turn the links up
    - now blue and red ns can cmmunicate through the switch

- if i want to comm & reach from host through virtual switch to ns
    $ ping 192.168.15.1     # ping red ns
        - "Not reachable!"
        - host on one network, ns on another
    - what is missing is the ip addr assignet to the swithc-interface
    $ ip addr add 192.168.15.5/24 dev v-net-0
        $ ip netns exec red ping 192.168.15.2
        - ping 192.168.15.1 works now


- this NW is still private (no connection external node 192.168.1.3 <> NS possible)
    $ ip netns exec blue ping 192.168.1.3   # ping from blue some node outside the host nw
        - "Connect: Network is unreachable"
        - blue check route tables
            $ ip netns exec blue route
            - no gw/door to the outside world
            - this GW is the host with all namespaces
    $ ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5
        - add the GW in blue ns to route all traffic from blue to 192.168.1.0/24 
    $ ip netns exec blue ping 192.168.1.3
        - "PING 192.168.1.3 56(84) bytes of data."
        - u still don't get any response back from the ping
        - the destination (internet) doesn't know about the blue private addresses
        - they can not reach back
        - we need a NAT enabled on our host, acting as a GW
    $ iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -jMASQUERADE
        - add NAT to the host to masquerade IPs
        $ ip netns exec blue ping 192.168.1.3
            - "64 bytes from 192.168.1.3: icmp_seq=1 ttl=63 time=0.587 ms"
            - works now

- to connect to internet
    $ ip netns exec blue ping 8.8.8.8
        "Connect: Network is unreachable"
    $ ip netns exec blue route
        - no route fir this case
    $ ip netns exec blue ip route add default via 192.168.15.5
        - specify the default route through the host
        $ ip netns exec blue ping 8.8.8.8

- what about the connectivity from outside (node @ 192.168.1.3) to our NS?
    - 192.168.15.2:80   -> web server e.g.
    $ iptables -t nat -A PREROUTING --dport 80 --to-destination 192.168.15.2:80 -j DNAT


- FAQ
    - if you can't ping one namespace from the other
        - make sure you set the NETMASK while setting IP Address. 
            - ie: 192.168.1.10/24
            $ ip -n red addr add 192.168.1.10/24 dev veth-red

        - Another thing to check is FirewallD/IP Table rules. 
            - Either add rules to IP Tables to allow traffic from 
                one namespace to another. 
            - Or disable IP Tables all together 
                (Only in a learning environment).



205. Prerequisite - Docker Networking
---
- network
    - none
    - host
    - bridge
        - this one is ok for us

$ ip link
    -> gives "docker0"
    - docker0 = actually bridge network

$ ip addr
    - inet 172.17.0.1/24

- whenever container created, docker creates a network for it

$ ip netns
    - to list the namespace
    - container = network namespace in a networking sense

- connect container nw to the host nw
    $ docker run -p 8080:80 nginx
    $ curl http://192.168.1.10:8080

- this port fw happens per nat routing (iptables), same as above
    - docker creates the rule
        $ iptables \
            - t nat \
            -A DOCKER   \
            -j DNAT \
            --dport 8080    \
            --to-destination 173.17.0.3:80
    - check the rule
        $ iptables -nvL -t nat



206. Prerequisite - CNI
---
- same workflow for solving networking challenges/namespaces came from different container solutions
    - docker, rkt, mesos, k8s

- this steps could be then generalized 
    - this could be my networking solution
    - let's call it "bridge"
    - solves all steps besides 1] create network namespace

- bridge solution, in order to work with docker, rkt, mesos, k8s, etc. namespaces
    - has to follow the standard
    -> CNI

- CNI
    - caontainer network interface
    - a standard how the program should look like

- bridge solution = called "plugin"

- CNI commes with a set of support for plugins
    - bridge
    - vlan
    - ipvlan
    - macvlan
    - windows
    - dhcp
    - host-local
    ---
    - weave (3rd party)
    - flannel
    - cilium
    - vmware
    - calico

- docker 
    - doesn't implement CNI
    - implements CNM (container network model)



207. Cluster Networking
---
- commands
    ip link
    ip addr
    ip addr show type bridge
    ip addr show eth0
    ip addr add 192.168.1.10/24 dev eth0
    ip route
    ip route add 192.168.1.0/24 via 192.168.2.1
    route
    cat /proc/sys/net/ipv4/ip_forward
    arp
    netstat -npl
    netstat -npa

- etcd ports
    - 2379 is the port of ETCD to which all control plane components connect to. 
        2380 is only for etcd peer-to-peer connectivity. When you have multiple 
        controlplane nodes



208. Important Note about CNI and CKA Exam
---
- Network Addons
- you can use any of the plugins which are described here:
    - https://kubernetes.io/docs/concepts/cluster-administration/addons/
    - https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model

- In the CKA exam, for a question that requires you to deploy 
    a network addon, unless specifically directed, you may use any of the 
    solutions described in the link above

- NOTE: In the official exam, all essential CNI deployment details will be provided.



211. Pod Networking
---
- k8s expects the inter-pod communication to be working
- k8s doesn't solve this problem

- nw model
    - every pod has an ip
    - pods can communicate between e.a. no matter on which node they are
    - no NAT for pod communication


$ ping -> Network is unreachable
    - route is missing
        $ ip route add 10.244.2.2. via 192.168.1.12

- etc = config
    - /etc/cni/net.d/net.conflist
- bin = binary
    - /opt/cni/bin/net.sh



212. CNI in kubernetes
---
- defined in kubelet.service
    - ...
    - --network-plugin=cni
    - --cni-bin-dir=/opt/cni/bin
    - --cni-conf-gir=/etc/cni/net.d

$ ps -aux | grep kubelet



213. Note CNI Weave
---
- weave announced the end of service for Weave Cloud.
- To know more about this, read the blog from the link below: -
    https://www.weave.works/blog/weave-cloud-end-of-service


- use the below latest link to install the weave net: 
    - kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

- Reference links: 
    - https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#-installation
    - https://github.com/weaveworks/weave/releases



214. CNI weave
---
- weave agents deployed on each node
    - svc or daemonset

$ k exec busybox ip route

$ k get pods -n kube-system
    - waeve-net-2jfg

$ k logs waeve-net-2jfg weave -n kube-system



217. Practice Test - Deploy Network Solution
---
- https://kubernetes.io/docs/concepts/cluster-administration/addons/
- https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#-installation

$ kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

- but, some modifications will be needed
    - Things to watch out for:
        - https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#-things-to-watch-out-for
    - e.g. --cluster-cidr 
        - https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#-changing-configuration-options
        - set:
                  containers:
                    - name: weave
                    env:
                        - name: IPALLOC_RANGE
                          value: 10.0.0.0/16



222. Service Networking
---
- although pods can communicate with each-other
    - rarely would you have direct pod2pod communication
        - but you woudl set up services

- clusterIP
    - e.g. database
    - a service is accessible on the whole cluster
        - all pods
        - all nodes

- nodePort
    - eg web service
    - a service is accessible from 
        - the whole cluster (pods), and in addition
        - exposes a service on all nodes on a spec port 

- services are a cluster-wide concept
    - they are a virtual object!

- kube-proxy creates rules (ip:port) to reach the services, using either
    - iptables (default),
    - userspace, or
    - ipvs

- setup of the abovementioned is done per
    - --proxy-mode [userspace | iptables | ipvs] ...

- assigned IP(-range) specified in 
    - kube-api-server --service-cluster-ip-range ipNet
        - default:  
    $ ps aux | grep kube-api-server

- the service and the pod ip ranges shouldn't overlap!
    - e.g. service ip range = 10.96.0.0/12
        - 10.96.0.0 - 10.111.255.255
    - e.g. pod nw cin range = 10.244.0.0/16
        - 10.244.0.0 - 10.244.255.255

- check iptables for roules set by kube-proxy
    $ iptables -L -t nat | grep db-service  # k get service -> NAME=db-service

- cat /var/log/kube-proxy.log



223. solution to network services
---
$ ip add
    - to check interfaces, ip addresses, subnets, ...

- what range does pods get the ip from
    - check the networking addon logs
    - weave e.g.
    $ k logs -n kube-system weave-netrbx4p
        # ipalloc-range: 10.244.0.0/16

- ip allocation range defined at (found by "grep 10.244.0.0/16")
    - weave log file
        - var/log/pods/kube-system_weave-net-q9llz_6c15eb3b-c431-4f3f-8dd9-03a46adf1061/weave/1.log
    - configmap/kube-proxy
    - etc/kubernetes/manifests/kube-controller-manager.yaml
    - proc
        - ps aux | grep weave
        - ps aux | grep kube-controller-manager

- a services alloaction
    $ cat kube-apiserver.yaml   # e.g.

- what type of proxy is the kube-proxy configured to use
    $ k logs -n kube-system kube-proxy-l25qk



225. DNS in kubernetes
---
- tools
    - host/ns lookup
    - dig

- focus is pnly on pods and services
    - not nodes!

- for each service DNS creates a subdomain
    - e.g. .apps
- all services grouped under subdomain "svc"
    - .svc
- all services and pods grouped under root's "cluster.local"
    - .cluster.local

- how to reach the service
    $ ping web-service          # inside the namespace
    $ ping web-service.apps     # apps = namespace (from other NS)
    $ ping web-service.apps.svc
    $ ping web-service.apps.svc.cluster.local   # FQDN 4 the service

- for pods, dns records are not enabled per default
    - this could be done
    - 10.244.2.5    -> 10-244-2-5
    $ ping 10-244-2-5.apps.pod.cluster.local
        - type = pod
        - root = .cluster.local



226. CoreDNS in Kubernetes
---
- after v1.12 coreDNS replaced kube-dns

$ /cat/etc/coredns/Corefile
    - passed in to coreDns pod as a configmap object
    $ kubectl get configmap -n kube-system

- next, pods have to point to the dns server
    $ cat /etc/resolv.conf
    - coreDns deployed as a service: kube-dns
        - IP-address of this svc is configured as the "nameserver   10.96.0.10" e.g. on pods
            - this configuration is done by kubelet (automatically)
            - cat /var/lib/kubelet/config.yaml
                ...
                clusterDNS:
                - 10.96.0.10
                clusterDomain: cluster.local

$ host web-service
    -> returns the FQDN of the service
    - web-service.default.svc.cluster.local

- host web-service can return FQDN because there is this entry in
    - /etc/resolv.conf
        -> search   default.svc.cluster.local svc.cluster.local cluster.local
    - this allows to find the service using any of
        $ host web-service
        $ host web-service.default
        $ host web-service.default.svc

- $ host WORKS ONLY FOR SERVICES
    - for PODS you have to specify FQDN
    $ host 10-244-2-5.default.pod.cluster.local



229. Ingress
---
- ingress is
    - layer 7 loadbalancer implemented in k8s
    - implementation instead of the (many) load-balancers possibly needed
    - implementing SSL security too

- k8s implementation of ingress consists of
    1] ingress controller
        - e.g.
            - GCE load balancer (supported by k8s), or
            - nginx (supported by k8s), or
            - haproxy, or
            - traefik
        - doesn't come per default with k8s
            - you ust deploy one
    2] ingress resources
        - configurations, like ssl ETCDCTL_CACERT

- 1] ingress controller
    - nginx setup
        - pass 
            args 
                - /nginx-ingress-controller
                - configmap     (-> configmap at the beginning will 
                                make it easy to change the config only
                                per configmap in the future)
            env:
                - name: POD_NAME
                ...
            ports:
                - name:
                ...

    - we need a service to expose the ingress controller to inet
        - nodeport

    - we need a service account
        - roles
        - clusterroles
        - rolebindings

- 2] ingress resources
    - implement rules like
        - forward traffic to one pod/svc
        - forward dependant on url to diff pods/svcs
        - route based on domain name



230. Article: Ingress
---
- in k8s version 1.20+ we can create an Ingress resource from 
    the imperative way like this:
        $ kubectl create ingress <ingress-name> --rule="host/path=service:port"
        $ kubectl create ingress ingress-test --rule="wear.my-online-store.com/wear*=wear-service:80"

- Find more information and examples in the below reference link:
    - https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-ingress-em-

- References:
    - https://kubernetes.io/docs/concepts/services-networking/ingress
    - https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types




































https://www.udemy.com/course/certified-kubernetes-administrator-with-practice-tests/learn/lecture/14296014#questions/19114274




